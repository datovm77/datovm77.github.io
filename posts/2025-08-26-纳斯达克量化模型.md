## 主程序
> 文件名：纳斯达克量化模型（主程序）.py

> 所需库：pip install pandas numpy tqdm akshare yfinance scipy scikit-learn yagmail pyarrow fastparquet

> 纳斯达克脚本默认用 QQ 邮箱，需开通 IMAP/SMTP 服务，生成 授权码（非登录密码），填入EMAIL_USER处

> 功能：目标：基于多因子模型、机器学习和风险控制，对 纳斯达克指数 进行趋势预测并生成买卖信号，并且发送报告到邮箱 

```python
# -*- coding: utf-8 -*-
"""
 Livermore纳斯达克量化预测系统 v7.4
- 保证基础功能（数据获取、邮件收发、日志、异常处理）
- 最大化扩展因子体系、信号生成、风险控制、模型融合与精度提升
- 兼容akshare, yfinance等主流数据/分析库
- [V7.4] 最终审查与精修：参数中央化、数据融合逻辑修复、情景分析完备性、决策透明度提升
-本项目使用了 AKShare 数据接口: https://github.com/akfamily/akshare
"""

import pandas as pd
import numpy as np
import akshare as ak
import yfinance as yf
import time
import logging
from datetime import datetime, timedelta
import yagmail
import os
import warnings
from scipy import stats
from sklearn.preprocessing import RobustScaler, StandardScaler, QuantileTransformer
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error
import traceback
import json
from functools import lru_cache, wraps
from typing import Dict, List, Tuple, Optional, Any, Callable
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore")
os.environ['TZ'] = 'Asia/Shanghai'

# ===================================
# 基础参数配置区（可环境变量覆盖）
# ===================================
EMAIL_USER = os.getenv('LIVERMORE_EMAIL_USER', 'your Emali')
EMAIL_PASS = os.getenv('LIVERMORE_EMAIL_PASS', 'your email password')
RECIPIENTS = [os.getenv('LIVERMORE_EMAIL_RCPT', 'your Emali')]

PARAMS = {
    # --- 核心交易与风控参数 ---
    'BASE_BUY_THRESHOLD': 0.30,  # 基础买入门槛
    'EXIT_THRESHOLD': -0.40,  # 基础离场门槛
    'TREND_ADJUST_FACTOR': 0.15,  # 趋势对门槛的调整敏感度
    'LIVERMORE_DRAWDOWN_PERCENT': 0.07,  # 利弗莫尔动态止损的回撤比例
    'ATR_STOP_LOSS_MULTIPLIER': 1.5,  # 初始ATR止损的乘数
    'POSITION_DISAGREEMENT_MIN_SCALE': 0.3,  # 模型分歧时头寸缩放的最小比例
    'POSITION_DISAGREEMENT_POWER': 1.5,  # 模型分歧对头寸缩放的指数强度

    # --- 情景分析参数 ---
    'REGIME_BULL_THRESHOLD': 0.15,  # 定义牛市的统一风险指数下限
    'REGIME_BEAR_THRESHOLD': -0.15,  # 定义熊市的统一风险指数上限
    'REGIME_SCORE_WEIGHTS': {'trend': 0.5, 'macro': 0.3, 'volatility': 0.2},  # 统一风险指数的成分权重
    'CLUSTER_CONF_MIN': 0.1,  # 策略簇置信度的下限
    'CLUSTER_CONF_MAX': 0.9,  # 策略簇置信度的上限

    # --- 因子与模型参数 ---
    'MAX_IR_WEIGHT': 5.0,  # 簇内加权时单个因子的最大IR权重
    'MIN_DATA_POINTS': 100,  # 数据清洗后要求的最少数据点
    'VOLATILITY_WINDOW': 20,  # 波动率因子的计算窗口
    'CACHE_EXPIRE_MINUTES': 5,  # 数据缓存的有效分钟数
    'MAX_THREADS': 6,  # 并行计算的最大线程数

    # --- (以下为原参数，保持不变) ---
    'BREAKOUT_DAYS': 55, 'CONFIRMATION_DAYS': 3, 'VOLUME_THRESHOLD': 1.3, 'RSI_PERIOD': 14,
    'MARKET_SMA': 200, 'LOOKBACK_PERIOD': 252, 'IC_THRESHOLD': 0.025, 'MOMENTUM_WINDOW': 10,
    'REVERSION_WINDOW': 5, 'SIGNAL_DECAY': 0.95, 'MAX_FACTOR_WEIGHT': 0.25, 'MIN_SAMPLES_FOR_CALC': 30,
    'OUTLIER_STD_THRESHOLD': 3.5, 'MODEL_WINDOW': 256, 'ML_N_ESTIMATORS': 90, 'ML_ALPHA': 0.3,
    'ML_MAXMODELS': 4, 'IC_CONF_SCALE': 5.0, 'IC_CONF_BASE': 0.05, 'ML_CONF_MIN': 0.05,
}

FACTOR_BASE_WEIGHTS = {
    'momentum': 0.13, 'volatility': 0.12, 'volume': 0.11,
    'price_pattern': 0.12, 'trend_strength': 0.10, 'mean_reversion': 0.08,
    'breakout': 0.12, 'macd': 0.07, 'bollinger': 0.07, 'vix_proxy': 0.08
}

# 将所有因子按其经济学逻辑归类，形成策略组合的基础。
FACTOR_CLUSTERS = {
    'trend': [
        'momentum', 'trend_strength', 'breakout', 'macd',
        'kama_trend', 'slope_trend', 'trend_persist', 'livermore_pivotal',
        'calm_trend'
    ],
    'reversion': ['mean_reversion', 'price_pattern', 'bollinger'],
    'volatility': ['volatility', 'vix_proxy', 'lag_ac1'],
    'macro': [
        'pce', 'nfp', 'rtl', 'ism_non_mfg', 'job', 'epu',
        'cpi_yoy', 'ppi_yoy', 'ism_mfg', 'durable', 'confidence', 'unemployment'
    ]
}

trading_calendar_cache = None


# ===================================
# 日志系统增强
# ===================================
def setup_enhanced_logging():
    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()

    class ColoredFormatter(logging.Formatter):
        COLORS = {
            'DEBUG': '\033[36m',
            'INFO': '\033[32m',
            'WARNING': '\033[33m',
            'ERROR': '\033[31m',
            'CRITICAL': '\033[35m',
            'ENDC': '\033[0m'
        }

        def format(self, record):
            log_color = self.COLORS.get(record.levelname, self.COLORS['ENDC'])
            levelname_colored = f"{log_color}{record.levelname}{self.COLORS['ENDC']}"
            message = super().format(record)
            return message.replace(record.levelname, levelname_colored, 1)

    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level))
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    console_handler = logging.StreamHandler()
    console_formatter = ColoredFormatter(
        '%(asctime)s | %(levelname)-8s | %(filename)s:%(lineno)d | %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    try:
        if not os.path.exists('cache'):
            os.makedirs('cache')
        file_handler = logging.FileHandler('cache/livermore_enhanced.log', encoding='utf-8')
        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(funcName)s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        logger.warning(f"文件日志配置失败: {e}")
    return logger


logger = setup_enhanced_logging()


# ===================================
# 缓存系统
# ===================================
class EnhancedCache:
    def __init__(self, expire_minutes: int = 5):
        self._cache = {}
        self._timestamps = {}
        self._expire_seconds = expire_minutes * 60
        self._lock = threading.RLock()

    def get(self, key: str) -> Any:
        with self._lock:
            if key not in self._cache: return None
            if time.time() - self._timestamps[key] > self._expire_seconds:
                del self._cache[key];
                del self._timestamps[key];
                return None
            return self._cache[key]

    def set(self, key: str, value: Any) -> None:
        with self._lock:
            self._cache[key] = value
            self._timestamps[key] = time.time()

    def clear(self) -> None:
        with self._lock:
            self._cache.clear();
            self._timestamps.clear()


cache_manager = EnhancedCache(PARAMS['CACHE_EXPIRE_MINUTES'])

# === 持仓状态持久化工具 ===
STATE_FILE = os.path.join("cache", "position_state.json")


def load_position_state() -> Dict[str, Any]:
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.warning(f"读取持仓状态失败: {e}")
    return {'status': 'flat', 'entry_price': None}


def save_position_state(state: Dict[str, Any]) -> None:
    try:
        os.makedirs("cache", exist_ok=True)
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"写入持仓状态失败: {e}")


def retry_on_exception(max_retries: int = 3, delay: float = 1.0, backoff: float = 2.0):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt == max_retries - 1: break
                    logger.warning(f"{func.__name__} 第{attempt + 1}次尝试失败: {e}, {current_delay:.1f}秒后重试")
                    time.sleep(current_delay);
                    current_delay *= backoff
            logger.error(f"{func.__name__} 所有重试均失败: {last_exception}")
            raise last_exception

        return wrapper

    return decorator


def performance_monitor(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            logger.debug(f"{func.__name__} 执行时间: {execution_time:.3f}秒")
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"{func.__name__} 执行失败 ({execution_time:.3f}秒): {e}")
            raise

    return wrapper


# ===================================
# 数据获取（多源融合增强）
# ===================================
@performance_monitor
@retry_on_exception(max_retries=3, delay=2.0)
def fetch_nasdaq_data_eastmoney() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("🔄 东方财富获取数据...")
        cache_key = f"nasdaq_em_{datetime.now().strftime('%Y%m%d_%H%M')}"
        cached_data = cache_manager.get(cache_key)
        if cached_data: logger.info("✅ 使用缓存数据"); return cached_data
        spot_df = ak.index_global_spot_em()
        nasdaq_row = spot_df[spot_df["名称"].str.contains("纳斯达克", na=False)]
        if nasdaq_row.empty: raise ValueError("未找到纳斯达克指数")
        symbol = nasdaq_row.iloc[0]["名称"]
        hist_df = ak.index_global_hist_em(symbol=symbol)
        column_mapping = {
            "日期": "Date", "今开": "Open", "最高": "High",
            "最低": "Low", "最新价": "Close", "收盘": "Close",
            "成交量": "Volume"
        }
        hist_df.rename(columns=column_mapping, inplace=True)
        hist_df['Date'] = pd.to_datetime(hist_df['Date'])
        hist_df.set_index('Date', inplace=True)
        hist_df.sort_index(inplace=True)
        current_price = float(nasdaq_row.iloc[0]["最新价"])

        hist_df = quality_check_and_clean(hist_df)
        hist_df = align_to_trading_calendar(hist_df)
        # (安全修复) 只选择数据帧中实际存在的列
        final_cols = [col for col in ['Open', 'High', 'Low', 'Close', 'Volume'] if col in hist_df.columns]
        result = (hist_df[final_cols], current_price, "东方财富")
        cache_manager.set(cache_key, result)
        logger.info(f"✅ 东方财富数据获取: {len(hist_df)}条, 当前价: {current_price:.2f}")
        return result
    except Exception as e:
        logger.warning(f"❌ 东方财富获取失败: {e}")
        raise


@performance_monitor
@retry_on_exception(max_retries=2, delay=2.0)
def fetch_nasdaq_data_sina() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("🔄 新浪财经获取数据...")
        cache_key = f"nasdaq_sina_{datetime.now().strftime('%Y%m%d_%H%M')}"
        cached_data = cache_manager.get(cache_key)
        if cached_data: logger.info("✅ 使用缓存数据"); return cached_data

        df = ak.index_us_stock_sina(".IXIC")
        if df.empty or len(df) < PARAMS['MIN_DATA_POINTS']:
            raise ValueError("新浪财经数据不足")

        current_price = float(df['close'].iloc[-1])
        df = standardize_dataframe_columns(df)
        df = quality_check_and_clean(df)

        # 【关键修复】: 重新启用交易日历对齐，保证数据处理一致性
        df = align_to_trading_calendar(df)

        result = (df, current_price, "新浪财经")
        cache_manager.set(cache_key, result)
        logger.info(f"✅ 新浪财经数据获取与对齐完成: {len(df)}条, 当前价: {current_price:.2f}")
        return result
    except Exception as e:
        logger.warning(f"❌ 新浪财经获取失败: {e}")
        raise


@performance_monitor
@retry_on_exception(max_retries=2, delay=2.0)
def fetch_nasdaq_data_yahoo() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("🔄 Yahoo Finance获取数据...")
        cache_key = f"nasdaq_yahoo_{datetime.now().strftime('%Y%m%d_%H%M')}"
        cached_data = cache_manager.get(cache_key)
        if cached_data: logger.info("✅ 使用缓存数据"); return cached_data
        ticker = yf.Ticker("^IXIC")
        end_date = datetime.now()
        start_date = end_date - timedelta(days=400)
        df = ticker.history(start=start_date, end=end_date, interval="1d")
        if df.empty or len(df) < PARAMS['MIN_DATA_POINTS']:
            raise ValueError("Yahoo Finance数据不足")
        df = df.reset_index()
        df = standardize_dataframe_columns(df)
        df = quality_check_and_clean(df)
        df = align_to_trading_calendar(df)
        current_price = float(df['Close'].iloc[-1])
        result = (df, current_price, "Yahoo Finance")
        cache_manager.set(cache_key, result)
        logger.info(f"✅ Yahoo Finance数据获取: {len(df)}条, 当前价: {current_price:.2f}")
        return result
    except Exception as e:
        logger.warning(f"❌ Yahoo Finance获取失败: {e}")
        raise


@performance_monitor
@retry_on_exception(max_retries=2, delay=2.0)
def fetch_nasdaq_data_investing() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("🔄 investing.com获取数据（备用）...")
        # 复用新浪财经
        return fetch_nasdaq_data_sina()
    except Exception as e:
        logger.warning(f"❌ investing.com获取失败: {e}")
        raise


def align_to_trading_calendar(df: pd.DataFrame) -> pd.DataFrame:
    """
    【V7.4】: 本函数不再进行任何网络请求。
    它只从本地缓存文件 'cache/trading_calendar_cache.pkl' 加载交易日历。
    这个缓存文件由主数据获取函数 fetch_nasdaq_data() 负责创建。
    """
    global trading_calendar_cache

    cal_idx = None
    cache_file = os.path.join("cache", "trading_calendar_cache.pkl")

    if 'trading_calendar_cache' in globals() and trading_calendar_cache is not None:
        cal_idx = trading_calendar_cache
    elif os.path.exists(cache_file):
        try:
            cal_idx = pd.read_pickle(cache_file)
            trading_calendar_cache = cal_idx
        except Exception as e:
            logger.error(f"❌ 读取交易日历缓存失败: {e}")
            return df

    if cal_idx is None:
        logger.warning("交易日历缓存尚未创建，暂时不对齐数据。")
        return df

    df_aligned = df.copy()
    if 'Date' in df_aligned.columns:
        df_aligned['Date'] = pd.to_datetime(df_aligned['Date'], errors='coerce')
        df_aligned = df_aligned.dropna(subset=['Date']).set_index('Date')

    if not isinstance(df_aligned.index, pd.DatetimeIndex):
        logger.warning("DataFrame 索引不是日期类型，无法对齐，返回原始df。")
        return df

    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        if col not in df_aligned.columns:
            df_aligned[col] = 0.0

    df_aligned = df_aligned[['Open', 'High', 'Low', 'Close', 'Volume']].reindex(cal_idx)
    df_aligned[['Open', 'High', 'Low', 'Close']] = df_aligned[['Open', 'High', 'Low', 'Close']].fillna(method='ffill')
    df_aligned['Volume'] = df_aligned['Volume'].fillna(0)
    df_aligned = df_aligned.dropna(subset=['Close'])

    logger.debug(f"数据已成功对齐到自建的交易日历，数据点: {len(df_aligned)}。")
    return df_aligned


def reconcile_data(primary_res: Tuple, backup_res: Tuple, tol: float = 0.005) -> Tuple:
    """
    【V7.4 新增】对两个成功获取的数据源结果进行交叉验证和修正。
    """
    df_p, price_p, src_p = primary_res
    df_b, price_b, src_b = backup_res

    try:
        # 1. 对比实时价格
        price_diff = abs(price_p - price_b) / max(price_p, 1e-6)
        if price_diff > tol:
            logger.warning(
                f"多源实时价格差异较大: {src_p} ({price_p:.2f}) vs {src_b} ({price_b:.2f}) - 差异 {price_diff:.2%}")

        # 2. 对比历史收盘价
        if not isinstance(df_p.index, pd.DatetimeIndex) and 'Date' in df_p.columns:
            df_p = df_p.set_index('Date')
        if not isinstance(df_b.index, pd.DatetimeIndex) and 'Date' in df_b.columns:
            df_b = df_b.set_index('Date')

        idx = df_p.index.intersection(df_b.index)
        if not idx.empty:
            p = df_p.loc[idx, 'Close'].astype(float)
            b = df_b.loc[idx, 'Close'].astype(float)
            diff = (p - b).abs() / p.clip(1e-8)
            bad_days = diff > tol
            if bad_days.any():
                logger.warning(f"主备历史Close差异超阈 {bad_days.sum()} 天，按备源({src_b})修正。")
                df_p.loc[idx[bad_days], 'Close'] = b[bad_days]
                df_p.loc[idx[bad_days], 'High'] = np.maximum(df_p.loc[idx[bad_days], 'High'],
                                                             df_p.loc[idx[bad_days], 'Close'])
                df_p.loc[idx[bad_days], 'Low'] = np.minimum(df_p.loc[idx[bad_days], 'Low'],
                                                            df_p.loc[idx[bad_days], 'Close'])

    except Exception as e:
        logger.warning(f"数据源交叉验证失败: {e}")

    return (df_p, price_p, f"{src_p} (已与{src_b}交叉验证)")


def fetch_nasdaq_data() -> Tuple[pd.DataFrame, float, str]:
    """【V7.4 】获取纳斯达克数据，并修复了多源融合逻辑"""
    data_sources = [fetch_nasdaq_data_eastmoney, fetch_nasdaq_data_sina]
    results = []
    last_error = None

    for i, func in enumerate(data_sources, 1):
        try:
            logger.info(f"📊 尝试数据源 {i}/{len(data_sources)}: {func.__name__}")
            df, price, src = func()
            if not df.empty:
                results.append((df, price, src))
        except Exception as e:
            last_error = e
            logger.warning(f"数据源 {func.__name__} 失败: {e}，尝试下一个...")

    if not results:
        logger.error(f"💥 所有数据源均失败，最后错误: {last_error}")
        return pd.DataFrame(), 0.0, "无数据源"

    final_result = results[0]
    if len(results) > 1:
        logger.info(f"成功获取 {len(results)} 个数据源，开始交叉验证...")
        final_result = reconcile_data(results[0], results[1])

    # --- 创建/刷新交易日历缓存（基于最终确定的数据源）---
    try:
        df_final, price_final, src_final = final_result
        if not isinstance(df_final.index, pd.DatetimeIndex):
            df_final = df_final.copy()
            if 'Date' in df_final.columns:
                df_final['Date'] = pd.to_datetime(df_final['Date'], errors='coerce')
                df_final = df_final.dropna(subset=['Date']).set_index('Date')

        cal_idx = pd.DatetimeIndex(df_final.index).normalize().drop_duplicates().sort_values()
        global trading_calendar_cache
        trading_calendar_cache = cal_idx
        os.makedirs("cache", exist_ok=True)
        pd.to_pickle(cal_idx, os.path.join("cache", "trading_calendar_cache.pkl"))
        logger.info(f"✅ 交易日历缓存已创建/更新，长度: {len(cal_idx)}")

        return df_final, price_final, src_final

    except Exception as e:
        logger.warning(f"交易日历缓存创建失败: {e}，直接返回原结果。")
        return final_result


def standardize_dataframe_columns(df: pd.DataFrame) -> pd.DataFrame:
    try:
        column_mapping = {
            'date': 'Date', 'Date': 'Date',
            'open': 'Open', 'Open': 'Open', 'opening': 'Open',
            'high': 'High', 'High': 'High', 'highest': 'High',
            'low': 'Low', 'Low': 'Low', 'lowest': 'Low',
            'close': 'Close', 'Close': 'Close', 'closing': 'Close', 'adj close': 'Close', 'Adj Close': 'Close',
            'adj_close': 'Close',
            'volume': 'Volume', 'Volume': 'Volume', 'vol': 'Volume'
        }
        df = df.copy()
        df.rename(columns=column_mapping, inplace=True)

        if 'Date' not in df.columns and (df.index.name in ['Date', 'date']):
            df = df.reset_index()

        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

        def clean_numeric(s):
            if s.dtype == 'O':
                s = s.astype(str).str.replace(',', '', regex=False).str.replace('%', '', regex=False)
            s = pd.to_numeric(s, errors='coerce')
            return s

        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
            if col in df.columns:
                df[col] = clean_numeric(df[col])

        if 'Volume' not in df.columns:
            df['Volume'] = np.nan

        if 'Date' in df.columns:
            df = df.dropna(subset=['Date']).sort_values('Date')
            df = df[~df['Date'].duplicated(keep='last')].reset_index(drop=True)

        logger.debug(f"列名标准化完成，最终列: {list(df.columns)}")
        return df
    except Exception as e:
        logger.error(f"列名标准化失败: {e}")
        return df


@performance_monitor
def quality_check_and_clean(df: pd.DataFrame) -> pd.DataFrame:
    try:
        if df.empty:
            return df

        logger.debug(f"开始数据质量检查，原始数据: {len(df)} 条")
        required_columns = ['Open', 'High', 'Low', 'Close']
        for col in required_columns:
            if col not in df.columns:
                logger.error(f"缺失必要列: {col}")
                return pd.DataFrame()

        df = df.copy()
        keep_cols = [c for c in ['Date', 'Open', 'High', 'Low', 'Close', 'Volume'] if c in df.columns]
        df = df[keep_cols]

        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df = df.dropna(subset=['Date']).set_index('Date')
        df = df.sort_index()
        if df.index.has_duplicates:
            df = df[~df.index.duplicated(keep='last')]

        for col in ['Open', 'High', 'Low', 'Close']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=['Open', 'High', 'Low', 'Close'])
        df = df[(df[['Open', 'High', 'Low', 'Close']] > 0).all(axis=1)]

        df['High'] = df[['High', 'Open', 'Close']].max(axis=1)
        df['Low'] = df[['Low', 'Open', 'Close']].min(axis=1)

        if 'Volume' in df.columns:
            df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce').fillna(0)
            df['Volume'] = df['Volume'].clip(lower=0)

        if len(df) < PARAMS['MIN_DATA_POINTS']:
            logger.error(f"清洗后数据不足: {len(df)} < {PARAMS['MIN_DATA_POINTS']}")
            return pd.DataFrame()

        logger.debug(f"数据质量检查完成，有效数据: {len(df)} 条")
        return df
    except Exception as e:
        logger.error(f"数据质量检查失败: {e}")
        return pd.DataFrame()


# ===================================
# 邮件通知
# ===================================
def send_notification(subject: str, content: str, attachments: Optional[List[str]] = None):
    try:
        yag = yagmail.SMTP(user=EMAIL_USER, password=EMAIL_PASS, host='smtp.qq.com', port=465, smtp_ssl=True)
        if attachments is None: attachments = []
        yag.send(to=RECIPIENTS, subject=subject, contents=content, attachments=attachments)
        logger.info(f"📧 邮件发送成功: {subject}")
    except Exception as e:
        logger.error(f"📧 邮件发送失败: {e}")


from functools import lru_cache


@lru_cache(maxsize=1)
def fetch_dxy_data() -> pd.Series:
    """
    获取并缓存美元指数(DXY)历史数据。
    """
    try:
        logger.info("🔄 (新) 尝试获取美元指数(DXY)历史数据...")
        dxy_df = ak.index_global_hist_em(symbol="美元指数")

        column_mapping = {"日期": "Date", "最新价": "Close"}
        dxy_df.rename(columns=column_mapping, inplace=True)

        dxy_df['Date'] = pd.to_datetime(dxy_df['Date'])
        dxy_series = pd.Series(dxy_df['Close'].values, index=dxy_df['Date'].values)
        dxy_series = dxy_series[~dxy_series.index.duplicated(keep='last')].sort_index()

        if dxy_series.empty:
            raise ValueError("获取的美元指数数据为空")

        logger.info(f"✅ (新) 美元指数数据获取成功，共 {len(dxy_series)} 条。")
        return dxy_series
    except Exception as e:
        logger.error(f"❌ (新) 获取美元指数数据失败: {e}")
        return pd.Series(dtype=float)


# ===================================
# 增强版因子引擎（多因子并行+融合+机器学习）
# ===================================
class EnhancedFactorEngine:
    """
    核心多因子引擎，扩展因子族、支持ML融合
    """

    def __init__(self, data: pd.DataFrame):
        logger.info(f"因子引擎初始化中，传入数据点数: {len(data)}...")
        self.data = data.copy()
        self.factors = {}
        self.factor_weights = {}
        self.ic_history = {}
        self.factor_performance = {}
        self.scaler = RobustScaler()
        self._calculation_cache = {}
        self.base_weights = FACTOR_BASE_WEIGHTS.copy()

        logger.info("...正在初始化并对齐美元指数(DXY)数据...")
        dxy_raw = fetch_dxy_data()
        if not dxy_raw.empty:
            self.dxy_series = dxy_raw.reindex(self.data.index, method='ffill').bfill()
        else:
            self.dxy_series = pd.Series(0.0, index=self.data.index)
        logger.info("...美元指数数据初始化完成。")

        logger.info(f"✅ 因子引擎初始化完成。")

    def safe_calculate(self, func: Callable, default_value: Any = 0, factor_name: str = "") -> Any:
        try:
            cache_key = f"{factor_name}_{len(self.data)}_{hash(str(self.data.index[-1]) if len(self.data) > 0 else 'empty')}"
            if cache_key in self._calculation_cache:
                return self._calculation_cache[cache_key]
            result = func()
            if isinstance(result, (pd.Series, pd.DataFrame)):
                result = result.fillna(default_value)
                if isinstance(result, pd.Series) and len(result) > 0:
                    result = self._winsorize_series(result)
            elif isinstance(result, (np.ndarray, list)):
                result = np.array(result)
                result = np.nan_to_num(result, nan=default_value, posinf=default_value, neginf=default_value)
            elif pd.isna(result) or np.isinf(result):
                result = default_value
            self._calculation_cache[cache_key] = result
            return result
        except Exception as e:
            logger.warning(f"因子计算失败 ({factor_name}): {e}")
            if isinstance(default_value, (pd.Series, pd.DataFrame)):
                return default_value.fillna(0) if hasattr(default_value, 'fillna') else default_value
            return default_value

    def _winsorize_series(self, series: pd.Series, limits: Tuple[float, float] = (0.05, 0.05)) -> pd.Series:
        try:
            if len(series) < 10: return series
            lower_limit = series.quantile(limits[0])
            upper_limit = series.quantile(1 - limits[1])
            return series.clip(lower=lower_limit, upper=upper_limit)
        except:
            return series

    @performance_monitor
    def calculate_momentum_factor(self) -> pd.Series:
        def _calc():
            short_window, medium_window, long_window = 20, 60, 120
            if len(self.data) < long_window + 10: return pd.Series(0.0, index=self.data.index)
            close = self.data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).ffill().bfill()
            lr = np.log(close).diff()
            mom_short = lr.rolling(short_window, min_periods=3).sum()
            mom_medium = lr.rolling(medium_window, min_periods=10).sum()
            mom_long = lr.rolling(long_window, min_periods=20).sum()
            weighted = 0.5 * mom_short + 0.3 * mom_medium + 0.2 * mom_long
            lower, upper = weighted.quantile(0.01), weighted.quantile(0.99)
            weighted = weighted.clip(lower, upper)
            roll_w = max(60, int(min(len(weighted), 252)))
            med = weighted.rolling(roll_w, min_periods=10).median()
            mad = (weighted - med).abs().rolling(roll_w, min_periods=10).median()
            robust_z = (weighted - med) / (mad * 1.4826 + 1e-8)
            z_smoothed = robust_z.ewm(span=5, adjust=False).mean().fillna(0.0)
            return z_smoothed.clip(-4, 4).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index), factor_name="momentum")

    @performance_monitor
    def calculate_volatility_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < PARAMS['VOLATILITY_WINDOW'] + 5: return pd.Series([0] * len(self.data),
                                                                                  index=self.data.index)
            returns = self.data['Close'].pct_change()
            hist_vol = returns.rolling(PARAMS['VOLATILITY_WINDOW'], min_periods=5).std()
            log_ho, log_lo = np.log(self.data['High'] / self.data['Open']), np.log(self.data['Low'] / self.data['Open'])
            log_co, log_oc = np.log(self.data['Close'] / self.data['Open']), np.log(
                self.data['Open'] / self.data['Close'].shift(1))
            log_cc = np.log(self.data['Close'] / self.data['Close'].shift(1))
            yang_zhang = log_oc ** 2 + 0.5 * log_ho * log_co - 0.386 * log_cc ** 2 + log_lo * log_co
            yz_vol = yang_zhang.rolling(PARAMS['VOLATILITY_WINDOW'], min_periods=5).mean().apply(np.sqrt)
            combined_vol = 0.6 * hist_vol + 0.4 * yz_vol
            long_term_vol = combined_vol.rolling(120, min_periods=20).mean()
            relative_vol = combined_vol / (long_term_vol + 1e-8)
            return relative_vol - 1

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="volatility")

    @performance_monitor
    def calculate_volume_factor(self) -> pd.Series:
        def _calc():
            if 'Volume' not in self.data.columns:
                volume = pd.Series(0.0, index=self.data.index)
            else:
                volume = self.data['Volume'].fillna(0)
            if volume.nunique() <= 2: return pd.Series(0.0, index=self.data.index)
            if len(self.data) < 20: return pd.Series([0] * len(self.data), index=self.data.index)
            volume = self.data['Volume'].replace(0, 1)
            vol_ratio_5 = volume / volume.rolling(5, min_periods=2).mean()
            vol_ratio_20 = volume / volume.rolling(20, min_periods=5).mean()
            vol_ratio_60 = volume / volume.rolling(60, min_periods=10).mean()
            price_change = self.data['Close'].pct_change()
            volume_change = volume.pct_change()
            rolling_corr = price_change.rolling(20, min_periods=5).corr(volume_change)
            volume_factor = (0.4 * vol_ratio_5 + 0.3 * vol_ratio_20 + 0.2 * vol_ratio_60 + 0.1 * rolling_corr)
            return (volume_factor - volume_factor.rolling(60, min_periods=10).mean()) / (
                        volume_factor.rolling(60, min_periods=10).std() + 1e-8)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="volume")

    # ... (此处省略其他因子计算函数，内容保持不变，以节约篇幅) ...
    # calculate_price_pattern_factor, calculate_trend_strength_factor, etc.
    @performance_monitor
    def calculate_price_pattern_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 50:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            high = self.data['High']
            low = self.data['Low']
            open_ = self.data['Open']
            support_levels = []
            resistance_levels = []
            for i in range(20, len(close)):
                window_low = low.iloc[i - 20:i + 1]
                window_high = high.iloc[i - 20:i + 1]
                local_min_idx = window_low.idxmin()
                if local_min_idx == window_low.index[-1]:
                    support_levels.append(low.iloc[i])
                local_max_idx = window_high.idxmax()
                if local_max_idx == window_high.index[-1]:
                    resistance_levels.append(high.iloc[i])
            price_range = high.rolling(20, min_periods=5).max() - low.rolling(20, min_periods=5).min()
            price_position = (close - low.rolling(20, min_periods=5).min()) / (price_range + 1e-8)
            body = abs(close - open_)
            upper_shadow = high - np.maximum(close, open_)
            lower_shadow = np.minimum(close, open_) - low
            hammer = ((lower_shadow > 2 * body) & (upper_shadow < 0.3 * body)).astype(int)
            doji = (body < 0.1 * (high - low)).astype(int)
            pattern_factor = (
                    0.5 * price_position +
                    0.3 * hammer.rolling(3, min_periods=1).sum() +
                    0.2 * doji.rolling(3, min_periods=1).sum()
            )
            return (pattern_factor - 0.5) * 2

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="price_pattern")

    @performance_monitor
    def calculate_trend_strength_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 50:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            high = self.data['High']
            low = self.data['Low']
            tr1 = high - low
            tr2 = abs(high - close.shift(1))
            tr3 = abs(low - close.shift(1))
            tr = pd.DataFrame({'tr1': tr1, 'tr2': tr2, 'tr3': tr3}).max(axis=1)
            plus_dm = high.diff()
            minus_dm = -low.diff()
            plus_dm[(plus_dm < 0) | (plus_dm < minus_dm)] = 0
            minus_dm[(minus_dm < 0) | (minus_dm < plus_dm)] = 0
            atr = tr.rolling(14, min_periods=5).mean()
            plus_di = 100 * plus_dm.rolling(14, min_periods=5).mean() / atr
            minus_di = 100 * minus_dm.rolling(14, min_periods=5).mean() / atr
            dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-8)
            adx = dx.rolling(14, min_periods=5).mean()

            def rolling_trend(x):
                if len(x) < 10: return 0
                x_vals = np.arange(len(x))
                slope, _, r_value, _, _ = stats.linregress(x_vals, x)
                return abs(slope) * r_value ** 2

            trend_strength = close.rolling(20, min_periods=10).apply(rolling_trend)
            trend_strength = (trend_strength - trend_strength.rolling(60, min_periods=10).mean()) / \
                             (trend_strength.rolling(60, min_periods=10).std() + 1e-8)
            combined_strength = 0.6 * (adx / 100 - 0.5) * 2 + 0.4 * trend_strength
            return combined_strength.clip(-1, 1)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="trend_strength")

    @performance_monitor
    def calculate_mean_reversion_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 50:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            sma_20 = close.rolling(20, min_periods=10).mean()
            sma_50 = close.rolling(50, min_periods=20).mean()
            dev_20 = (close - sma_20) / sma_20
            dev_50 = (close - sma_50) / sma_50
            bb_upper = sma_20 + 2 * close.rolling(20, min_periods=10).std()
            bb_lower = sma_20 - 2 * close.rolling(20, min_periods=10).std()
            bb_position = (close - bb_lower) / (bb_upper - bb_lower + 1e-8)
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(14, min_periods=5).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14, min_periods=5).mean()
            rs = gain / (loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
            mean_reversion = (
                    0.4 * (dev_20 + dev_50).clip(-0.1, 0.1) * 10 +
                    0.3 * (bb_position - 0.5) * 2 +
                    0.3 * (rsi - 50) / 50
            )
            return mean_reversion.clip(-1, 1)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="mean_reversion")

    @performance_monitor
    def calculate_breakout_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 60:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            high = self.data['High']
            low = self.data['Low']
            period = 20
            upper_channel = high.rolling(period, min_periods=10).max()
            lower_channel = low.rolling(period, min_periods=10).min()
            channel_width = upper_channel - lower_channel
            breakout_strength = (close - lower_channel) / (channel_width + 1e-8) - 0.5
            if 'Volume' in self.data.columns and self.data['Volume'].nunique() > 1:
                volume = self.data['Volume']
                avg_volume = volume.rolling(20, min_periods=5).mean()
                volume_ratio = volume / (avg_volume + 1e-8)
                breakout_signal = ((close > upper_channel.shift(1)) | (close < lower_channel.shift(1))).astype(int)
                confirmed_breakout = breakout_signal * (volume_ratio > 1.2).astype(int)
                breakout_factor = (0.5 * breakout_strength * 2 + 0.5 * confirmed_breakout)
            else:
                logger.debug("calculate_breakout_factor: 未找到有效的Volume列，仅使用价格信号。")
                breakout_factor = breakout_strength * 2
            return breakout_factor.clip(-1, 1)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="breakout")

    @performance_monitor
    def calculate_macd_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            ema_fast = close.ewm(span=12, min_periods=6).mean()
            ema_slow = close.ewm(span=26, min_periods=13).mean()
            macd_line = ema_fast - ema_slow
            macd_signal = macd_line.ewm(span=9, min_periods=5).mean()
            return macd_line - macd_signal

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="macd")

    @performance_monitor
    def calculate_bollinger_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            sma = close.rolling(20, min_periods=10).mean()
            std = close.rolling(20, min_periods=10).std()
            upper = sma + 2 * std
            lower = sma - 2 * std
            return (close - lower) / (upper - lower + 1e-8) - 0.5

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="bollinger")

    @performance_monitor
    def calculate_vix_proxy_factor(self) -> pd.Series:
        def _calc():
            returns = self.data['Close'].pct_change()
            vol_20d = returns.rolling(20, min_periods=10).std()
            vol_60d = returns.rolling(60, min_periods=30).std()
            return vol_20d / (vol_60d + 1e-8)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="vix_proxy")

    @performance_monitor
    def calculate_kama_trend_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            n_er, n_fast, n_slow = 10, 2, 30
            if len(close) < max(2 * n_slow, 60): return pd.Series(0.0, index=self.data.index)
            change = close.diff(n_er).abs()
            vol = close.diff().abs().rolling(n_er).sum()
            er = (change / (vol + 1e-8)).clip(0, 1)
            fast_sc, slow_sc = 2 / (n_fast + 1), 2 / (n_slow + 1)
            sc = (er * (fast_sc - slow_sc) + slow_sc) ** 2
            kama = pd.Series(index=close.index, dtype=float)
            kama.iloc[n_er] = close.iloc[:n_er + 1].mean()
            for i in range(n_er + 1, len(close)):
                kama.iloc[i] = kama.iloc[i - 1] + sc.iloc[i] * (close.iloc[i] - kama.iloc[i - 1])
            slope = kama.diff(5) / (close.rolling(20).std() + 1e-8)
            slope = slope.fillna(0.0)
            z = (slope - slope.rolling(60, min_periods=20).mean()) / (slope.rolling(60, min_periods=20).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index), factor_name="kama_trend")

    @performance_monitor
    def calculate_slope_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            window = 50
            if len(close) < window + 10: return pd.Series(0.0, index=self.data.index)
            idx = np.arange(window)

            def _slope(x):
                y = np.array(x)
                slope, _, r, _, _ = stats.linregress(idx, y)
                return (slope / (np.mean(y) + 1e-8)) * (r ** 2)

            s = close.rolling(window, min_periods=window).apply(_slope, raw=False)
            z = (s - s.rolling(120, min_periods=30).mean()) / (s.rolling(120, min_periods=30).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="slope_trend")

    @performance_monitor
    def calculate_trend_persistence_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            ret = close.pct_change().fillna(0.0)
            window = 20
            if len(close) < window + 5: return pd.Series(0.0, index=self.data.index)
            up_ratio = (ret > 0).rolling(window, min_periods=int(window * 0.6)).mean() - 0.5
            z = (up_ratio - up_ratio.rolling(120, min_periods=30).mean()) / (
                        up_ratio.rolling(120, min_periods=30).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="trend_persist")

    @performance_monitor
    def calculate_lag_autocorr_factor(self) -> pd.Series:
        def _calc():
            ret = self.data['Close'].pct_change().fillna(0.0)
            window = 40
            if len(ret) < window + 5: return pd.Series(0.0, index=self.data.index)
            r0, r1 = ret, ret.shift(1)
            roll_cov = (r0 * r1).rolling(window, min_periods=int(window * 0.6)).mean() - r0.rolling(
                window).mean() * r1.rolling(window).mean()
            roll_var = r0.rolling(window, min_periods=int(window * 0.6)).var()
            ac1 = (roll_cov / (roll_var + 1e-8)).clip(-1, 1)
            z = (ac1 - ac1.rolling(120, min_periods=30).mean()) / (ac1.rolling(120, min_periods=30).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index), factor_name="lag_ac1")

    @performance_monitor
    def calculate_dxy_momentum_factor(self) -> pd.Series:
        def _calc():
            if self.dxy_series.nunique() < 20: return pd.Series(0.0, index=self.data.index)
            log_returns = np.log(self.dxy_series).diff()
            momentum = log_returns.rolling(20, min_periods=15).sum().fillna(0.0)
            rolling_mean = momentum.rolling(120, min_periods=60).mean()
            rolling_std = momentum.rolling(120, min_periods=60).std()
            z_score = ((momentum - rolling_mean) / (rolling_std + 1e-8)).fillna(0.0)
            return z_score.clip(-3.5, 3.5)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="dxy_momentum")

    @performance_monitor
    def calculate_calm_trend_factor(self) -> pd.Series:
        """
        【新增交互特征】: 冷静趋势因子 (Calm Trend Factor)
        逻辑: 趋势强度 / (波动率代理 + ε)，寻找在市场平稳（非恐慌）状态下的强劲趋势。
        这是对趋势质量的更高阶判断。
        """

        def _calc():
            # 安全地获取依赖的因子，如果不存在则使用中性值
            trend_strength = self.factors.get('trend_strength', pd.Series(0.0, index=self.data.index))
            vix_proxy = self.factors.get('vix_proxy', pd.Series(1.0, index=self.data.index))  # 波动率代理，默认为1避免除零

            # 核心逻辑：趋势强度 / (VIX代理 + 一个小常数以防除零)
            # VIX代理值越大（市场越恐慌），冷静趋势因子的值就越被压制
            calm_trend = trend_strength / (vix_proxy + 1e-8)

            # 对结果进行标准化，使其与其他因子量纲可比
            rolling_mean = calm_trend.rolling(120, min_periods=60).mean()
            rolling_std = calm_trend.rolling(120, min_periods=60).std()
            z_score = ((calm_trend - rolling_mean) / (rolling_std + 1e-8)).fillna(0.0)

            return z_score.clip(-3.5, 3.5)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="calm_trend")


    @performance_monitor
    def calculate_livermore_pivotal_factor(self) -> pd.Series:
        def _calc():
            BREAKOUT_WINDOW, VOLUME_SPIKE_RATIO, TREND_FILTER_WINDOW = 55, 1.5, 50
            if len(self.data) < BREAKOUT_WINDOW + 10: return pd.Series(0, index=self.data.index)
            close, high, volume = self.data['Close'], self.data['High'], self.data['Volume']
            is_in_uptrend = close > close.rolling(TREND_FILTER_WINDOW, min_periods=20).mean()
            highest_in_window = high.shift(1).rolling(BREAKOUT_WINDOW, min_periods=30).max()
            is_breakout = close > highest_in_window
            avg_volume = volume.shift(1).rolling(20, min_periods=10).mean()
            is_volume_confirmed = volume > (avg_volume * VOLUME_SPIKE_RATIO)
            pivotal_signal = (is_in_uptrend & is_breakout & is_volume_confirmed).astype(int)
            return pivotal_signal

        return self.safe_calculate(_calc, default_value=pd.Series(0, index=self.data.index),
                                   factor_name="livermore_pivotal")

    def calculate_all_factors(self) -> Dict[str, pd.Series]:
        logger.info("🔄 开始计算所有因子...")
        factor_functions = {
            'momentum': self.calculate_momentum_factor, 'volatility': self.calculate_volatility_factor,
            'volume': self.calculate_volume_factor, 'price_pattern': self.calculate_price_pattern_factor,
            'trend_strength': self.calculate_trend_strength_factor,
            'mean_reversion': self.calculate_mean_reversion_factor,
            'breakout': self.calculate_breakout_factor, 'macd': self.calculate_macd_factor,
            'bollinger': self.calculate_bollinger_factor, 'vix_proxy': self.calculate_vix_proxy_factor,
            'kama_trend': self.calculate_kama_trend_factor, 'slope_trend': self.calculate_slope_factor,
            'trend_persist': self.calculate_trend_persistence_factor, 'lag_ac1': self.calculate_lag_autocorr_factor,
            'livermore_pivotal': self.calculate_livermore_pivotal_factor,
            'dxy_momentum': self.calculate_dxy_momentum_factor,
            'calm_trend': self.calculate_calm_trend_factor
        }

        with ThreadPoolExecutor(max_workers=PARAMS['MAX_THREADS']) as executor:
            future_to_factor = {executor.submit(func): name for name, func in factor_functions.items()}
            for future in as_completed(future_to_factor):
                factor_name = future_to_factor[future]
                try:
                    self.factors[factor_name] = future.result()
                    logger.debug(f"✅ 因子计算完成: {factor_name}")
                except Exception as e:
                    logger.error(f"❌ 因子计算失败 ({factor_name}): {e}")
                    self.factors[factor_name] = pd.Series([0] * len(self.data), index=self.data.index)

        logger.info(f"✅ 所有技术因子计算完成，共 {len(self.factors)} 个。开始集成宏观因子...")
        if not getattr(self, '_macro_loaded', False):
            from macro_engine_strict import StrictMacro
            self.factors.update(StrictMacro(self.data.index).factors())
            self._macro_loaded = True

        return self.factors

    @performance_monitor
    def factor_preprocessing(self):
        """【V7.4】引入QuantileTransformer实现“因子民主”，并增强宏观因子。"""
        logger.info("🔄 (V7.4) 开始统一因子预处理...")
        INVERTED_FACTORS = ['volatility', 'pce', 'job', 'vix_proxy', 'epu', 'cpi_yoy', 'ppi_yoy', 'unemployment',
                            'dxy_momentum']

        # 【【【核心升级 1：定义需要特殊处理的“不稳定”因子】】】
        UNSTABLE_FACTORS = ['macd']  # 未来可加入其他值域无界的因子

        base_index = self.data.index

        def mad_winsorize(s: pd.Series, k: float = 5.0) -> pd.Series:
            if s.dropna().size < 20: return s.fillna(0.0)
            med = s.median()
            mad = (s - med).abs().median() + 1e-8
            return s.clip(med - k * 1.4826 * mad, med + k * 1.4826 * mad)

        # 1. 初始数据对齐与去极值
        df_factors = {
            name: mad_winsorize(ser.reindex(base_index).ffill().bfill().fillna(0.0))
            for name, ser in self.factors.items() if isinstance(ser, pd.Series) and not ser.empty
        }
        factor_df = pd.DataFrame(df_factors).fillna(0.0)

        # 2. 【【【核心升级 2：对稳定与不稳定因子进行分类处理】】】
        stable_cols = [c for c in factor_df.columns if c not in UNSTABLE_FACTORS]
        unstable_cols = [c for c in factor_df.columns if c in UNSTABLE_FACTORS]

        factor_scaled_stable = pd.DataFrame(index=factor_df.index)
        if stable_cols:
            scaler = RobustScaler()
            stable_scaled = scaler.fit_transform(factor_df[stable_cols])
            factor_scaled_stable = pd.DataFrame(stable_scaled, index=factor_df.index, columns=stable_cols)

        factor_scaled_unstable = pd.DataFrame(index=factor_df.index)
        if unstable_cols:
            # 使用分位数变换，强制将不稳定因子的分布正态化，根除极端值影响
            qt = QuantileTransformer(output_distribution='normal', random_state=42)
            unstable_scaled = qt.fit_transform(factor_df[unstable_cols])
            factor_scaled_unstable = pd.DataFrame(unstable_scaled, index=factor_df.index, columns=unstable_cols)

        # 合并处理后的因子
        factor_scaled = pd.concat([factor_scaled_stable, factor_scaled_unstable], axis=1)

        # 3. 方向反转与时间平滑 (与之前相同)
        for name in factor_scaled.columns:
            if name in INVERTED_FACTORS:
                factor_scaled[name] = -factor_scaled[name]
        factor_scaled = factor_scaled.ewm(span=5, adjust=False).mean()
        self.factors = {c: factor_scaled[c] for c in factor_scaled.columns}

        # 4. 宏观因子发布滞后处理 (与之前相同)
        MACRO_NAMES_ORIGINAL = FACTOR_CLUSTERS['macro']

        def apply_release_lag(series: pd.Series, trading_index: pd.DatetimeIndex, lag_days: int = 2) -> pd.Series:
            s = series.copy()
            s.index = pd.to_datetime(s.index).to_period('M').to_timestamp('M')
            s = s[~s.index.duplicated(keep='last')].sort_index()
            s = s.reindex(trading_index, method='ffill')
            return s.shift(lag_days)

        for n in MACRO_NAMES_ORIGINAL:
            if n in self.factors:
                self.factors[n] = apply_release_lag(self.factors[n], self.data.index, lag_days=2)

        # 5. 【【【核心升级 3：从原始宏观数据中提取“动量”信号】】】
        logger.info("🔄 增强宏观因子：计算3个月变化率作为动量信号...")
        new_macro_factors = {}
        new_macro_names_in_cluster = []
        for name in MACRO_NAMES_ORIGINAL:
            if name in self.factors:
                original_series = self.factors[name]
                # 计算约3个月的变化率 (63个交易日)
                momentum_series = original_series.diff(63).fillna(0.0)

                # 对变化率再次进行滚动标准化，使其成为一个独立的、可比的因子
                rolling_mean = momentum_series.rolling(252, min_periods=120).mean()
                rolling_std = momentum_series.rolling(252, min_periods=120).std().replace(0, 1e-8)
                standardized_momentum = ((momentum_series - rolling_mean) / rolling_std).fillna(0.0)

                new_name = f"{name}_mom"
                new_macro_factors[new_name] = standardized_momentum.clip(-3.5, 3.5)
                new_macro_names_in_cluster.append(new_name)

        # 将新创建的宏观动量因子加入系统
        self.factors.update(new_macro_factors)
        FACTOR_CLUSTERS['macro'].extend(new_macro_names_in_cluster)

        logger.info(f"✅ 因子预处理完成，共 {len(self.factors)} 个因子 (新增 {len(new_macro_factors)} 个宏观动量因子)。")

    def _calculate_regime_scores(self) -> Tuple[pd.Series, pd.Series]:
        logger.info("🔄 (V7.4 统一感知引擎) 开始计算市场综合情景评分...")
        try:
            close = self.data['Close']

            # --- 支柱 1: 价格趋势组件 ---
            sma_200 = close.rolling(200, min_periods=50).mean()
            trend_strength = (close - sma_200) / (close.rolling(60).std() + 1e-8)
            trend_component = (trend_strength / 3.0).clip(-1, 1)

            # --- 支柱 2: 波动状态组件 ---
            high, low = self.data['High'], self.data['Low']
            tr = pd.concat([high - low, abs(high - close.shift(1)), abs(low - close.shift(1))], axis=1).max(axis=1)
            atr_short = tr.rolling(20, min_periods=15).mean()
            atr_long = tr.rolling(120, min_periods=100).mean()
            vol_ratio = atr_short / (atr_long + 1e-8)
            vol_component = (1.5 - vol_ratio).clip(-1, 1)

            # --- 支柱 3: 宏观健康度组件 ---
            macro_factor_names = FACTOR_CLUSTERS['macro']
            macro_factors_df = pd.DataFrame({k: self.factors[k] for k in macro_factor_names if k in self.factors})
            if not macro_factors_df.empty:
                raw_macro_signal = macro_factors_df.mean(axis=1)
                macro_component = np.tanh(raw_macro_signal).fillna(0.0)
            else:
                macro_component = pd.Series(0.0, index=self.data.index)

            # --- 最终融合: 三大支柱加权 ---
            weights = PARAMS['REGIME_SCORE_WEIGHTS']
            unified_risk_score = (
                    weights['trend'] * trend_component +
                    weights['macro'] * macro_component +
                    weights['volatility'] * vol_component
            ).ffill().bfill().fillna(0.0)

            # --- 定义状态 ---
            trend_regime = pd.Series('NEUTRAL', index=self.data.index)
            trend_regime[unified_risk_score > PARAMS['REGIME_BULL_THRESHOLD']] = 'BULL'
            trend_regime[unified_risk_score < PARAMS['REGIME_BEAR_THRESHOLD']] = 'BEAR'
            trend_regime.ffill(inplace=True)

            logger.info(
                f"✅ 统一情景评分完成。当前趋势: {trend_regime.iloc[-1]}, 统一风险指数: {unified_risk_score.iloc[-1]:.3f}")
            return trend_regime, unified_risk_score

        except Exception as e:
            logger.error(f"❌ 计算统一情景评分失败: {e}\n{traceback.format_exc()}")
            return (pd.Series('UNKNOWN', index=self.data.index), pd.Series(0.0, index=self.data.index))

    def synthesize_cluster_signals(self, ic_results: Dict) -> Dict[str, pd.Series]:
        logger.info("🔄 (动态 EWMA_IR 加权) 开始合成策略簇信号...")
        cluster_signals = {}
        ir_scores = {
            name: abs(ic.get('ewma_ic', 0.0) / (ic.get('ic_std', 1.0) + 1e-8))
            for name, ic in ic_results.items()
        }

        for cluster_name, factor_list in FACTOR_CLUSTERS.items():
            weighted_signals = pd.Series(0.0, index=self.data.index)
            total_weight = 0.0

            for factor_name in factor_list:
                if factor_name in self.factors and factor_name in ir_scores:
                    weight = min(ir_scores[factor_name], PARAMS['MAX_IR_WEIGHT'])
                    signal = self.factors[factor_name].reindex(self.data.index).ffill().bfill().fillna(0.0)
                    weighted_signals += signal * weight
                    total_weight += weight

            if total_weight > 0:
                final_cluster_signal = weighted_signals / total_weight
            else:
                logger.warning(f"策略簇 {cluster_name} 无有效IR，回退到等权平均。")
                cluster_df = pd.concat([self.factors[f] for f in factor_list if f in self.factors], axis=1)
                final_cluster_signal = cluster_df.mean(axis=1).reindex(self.data.index).ffill().bfill().fillna(0.0)

            cluster_signals[cluster_name] = final_cluster_signal
            logger.debug(f"✅ 策略簇 {cluster_name} 信号合成完成，最新值: {final_cluster_signal.iloc[-1]:.4f}")
        return cluster_signals

    def get_meta_strategy_weights(self, risk_on_off_score: float) -> Dict[str, float]:
        logger.info(f"🔄 根据风险偏好指数 '{risk_on_off_score:.3f}' 平滑分配策略权重...")
        risk_on_weights = {'trend': 0.60, 'reversion': 0.05, 'volatility': 0.10, 'macro': 0.25}
        risk_off_weights = {'trend': 0.10, 'reversion': 0.30, 'volatility': 0.40, 'macro': 0.20}

        current_weights = {
            name: np.interp(risk_on_off_score, [-1.0, 1.0], [risk_off_weights[name], risk_on_weights[name]])
            for name in risk_on_weights.keys()
        }

        total_weight = sum(current_weights.values())
        normalized_weights = {k: v / total_weight for k, v in current_weights.items()}
        logger.info(f"✅ 平滑策略权重分配完成: {normalized_weights}")
        return normalized_weights

    @performance_monitor
    def machine_learning_predict(self,
                                 horizons=None,
                                 horizon_weights=None,
                                 min_samples=120,
                                 top_quantile=0.70,
                                 bottom_quantile=0.30,
                                 n_top_features=30,
                                 tss_splits=4,
                                 pred_smooth_span=3,
                                 min_train_for_output=200):
        """
        稳健的 ML 预测模块（替换版）。
        输出: (ml_signal_series (pd.Series aligned to self.data.index), ml_confidence (float 0..1))
        设计要点:
          - 每个 horizon 单独训练二分类器（上/下分位），用 TimeSeriesSplit 做 OOF 估计
          - 用 cross_val_predict 计算 OOF 概率并计算 Brier score（训练集性能指标）
          - 用 CalibratedClassifierCV 对最终模型做概率校准（method='sigmoid' 更稳定）
          - 做简单的基于重要性的特征选择（Select top N by LGBM importance）
          - 输出每个 horizon 的概率映射到 [-1,1]，再按 horizon_weights 加权合成
          - ml_confidence 综合考虑训练时的归一化 Brier（perf）、最近信号幅度与 horizon 一致性
        """
        import numpy as np
        import pandas as pd
        from sklearn.model_selection import TimeSeriesSplit, cross_val_predict
        from sklearn.metrics import brier_score_loss
        from sklearn.feature_selection import SelectFromModel
        from sklearn.preprocessing import StandardScaler
        from sklearn.calibration import CalibratedClassifierCV
        from sklearn.utils.class_weight import compute_class_weight

        try:
            import lightgbm as lgb
        except Exception:
            # 如果没有 lightgbm，返回中性信号并给出低置信度
            logger.warning("LightGBM not available — skipping ML module.")
            return pd.Series(0.0, index=self.data.index), 0.01

        # 参数默认
        if horizons is None:
            horizons = [1, 5, 10]
        if horizon_weights is None:
            # 默认短期权重大
            horizon_weights = {1: 0.5, 5: 0.3, 10: 0.2}
        # 归一化 horizon 权重
        total_hw = sum(horizon_weights.get(h, 0.0) for h in horizons) or 1.0
        hw = {h: float(horizon_weights.get(h, 0.0) / total_hw) for h in horizons}

        # 准备特征与价格（已假设 self.factors 已做滚动 robust preprocessing）
        feats = pd.DataFrame(self.factors).reindex(self.data.index).astype(float).fillna(method='ffill').fillna(0.0)
        # 少量平滑
        feats = feats.ewm(span=pred_smooth_span, adjust=False).mean().fillna(0.0)
        close = self.data['Close'].astype(float).reindex(self.data.index).fillna(method='ffill').fillna(0.0)

        n = len(feats)
        if n < min_samples or close.isna().sum() > 0.5 * n:
            # 数据不足或价格缺失严重 -> 返回中性
            logger.warning("数据不足或价格缺失过多，跳过 ML 模块。")
            return pd.Series(0.0, index=self.data.index), 0.01

        # 预计算未来收益数组（numpy 形式）
        fut_ret = {h: (close.shift(-h) / close - 1.0).values for h in horizons}

        # 保存每个 horizon 的输出及训练指标
        horizon_signals = {}
        horizon_perf = {}  # 用于记录训练期的 normalized brier 或回退指标

        for h in horizons:
            try:
                y_all = fut_ret[h]
                # 只考虑未来收益不是 NaN 的位置
                valid_idx = np.where(~np.isnan(y_all))[0]
                if valid_idx.size < min_samples:
                    # 不足样本 -> 退化为 0 信号
                    horizon_signals[h] = pd.Series(0.0, index=feats.index)
                    horizon_perf[h] = 0.0
                    continue

                # 构造训练标签：上/下分位二分类（剔除中性区间）
                y_vec = pd.Series(y_all, index=feats.index)
                q_high = y_vec.quantile(top_quantile)
                q_low = y_vec.quantile(bottom_quantile)
                mask = (y_vec <= q_low) | (y_vec >= q_high)
                if mask.sum() < min_samples:
                    # 若上下分位样本过少，改为回归式预测（轻量 LGBM 回归）
                    X_reg = feats.loc[~y_vec.isna(), :]
                    y_reg = y_vec.loc[~y_vec.isna()]
                    if X_reg.shape[0] < min_samples:
                        horizon_signals[h] = pd.Series(0.0, index=feats.index)
                        horizon_perf[h] = 0.0
                        continue
                    # 回归训练（较保守的参数）
                    reg = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.03, random_state=42)
                    reg.fit(X_reg, y_reg)
                    pred = pd.Series(reg.predict(feats), index=feats.index).fillna(0.0)
                    # 映射到 [-1,1]
                    sig = pred / (pred.abs().rolling(63, min_periods=1).std().replace(0, 1) + 1e-8)
                    sig = np.tanh(sig.fillna(0.0))
                    horizon_signals[h] = sig
                    # 回归没有 brier，使用 R^2-like proxy (clip 0..1)
                    horizon_perf[h] = max(0.0, min(1.0, 0.5))  # 保守默认
                    continue

                # 二分类样本准备
                idx_bin = mask[mask].index
                X = feats.loc[idx_bin, :].copy()
                y_bin = (y_vec.loc[idx_bin] >= q_high).astype(int).values  # 1 表示上分位

                # 训练集大小保护
                if X.shape[0] < min_samples:
                    horizon_signals[h] = pd.Series(0.0, index=feats.index)
                    horizon_perf[h] = 0.0
                    continue

                # 计算样本权重（balanced）
                try:
                    classes = np.unique(y_bin)
                    if classes.size < 2:
                        # 单类保护
                        sample_weight = np.ones(len(y_bin), dtype=float)
                    else:
                        cw = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_bin)
                        # cw 结果位置对应 [class0_weight, class1_weight]
                        w0, w1 = float(cw[0]), float(cw[1])
                        sample_weight = np.where(y_bin == 1, w1, w0).astype(float)
                except Exception:
                    sample_weight = np.ones(len(y_bin), dtype=float)

                # 先进行简单特征选择：用一个轻量 LGBM 快速计算重要性
                fs_features = X.columns.tolist()
                try:
                    fs_clf = lgb.LGBMClassifier(n_estimators=150, learning_rate=0.05, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                    fs_clf.fit(X, y_bin, sample_weight=sample_weight)
                    importances = fs_clf.feature_importances_
                    # 选择前 n_top_features（受限于现有特征数）
                    nsel = min(int(n_top_features), len(importances))
                    if nsel < 5:
                        nsel = min(5, len(importances))
                    top_idx = np.argsort(importances)[-nsel:]
                    sel_cols = [X.columns[i] for i in sorted(top_idx)]
                    X_sel = X[sel_cols]
                    feats_sel = feats[sel_cols]
                except Exception:
                    # 若特征选择失败，退回全特征
                    sel_cols = X.columns.tolist()
                    X_sel = X
                    feats_sel = feats

                # 标准化（fit on training set）
                scaler = StandardScaler()
                X_scaled = pd.DataFrame(scaler.fit_transform(X_sel), index=X_sel.index, columns=X_sel.columns)
                feats_scaled = pd.DataFrame(scaler.transform(feats_sel), index=feats_sel.index,
                                            columns=feats_sel.columns)

                # 交叉验证配置（TimeSeriesSplit）
                n_splits = min(tss_splits, max(2, X_scaled.shape[0] // 50))
                tss = TimeSeriesSplit(n_splits=n_splits)

                # OOF 概率估计（用于训练期的 Brier score）
                try:
                    base_clf = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                    # cross_val_predict 获取 OOF 概率（每个训练样本的 out-of-fold 预测）
                    oof_proba = cross_val_predict(base_clf, X_scaled, y_bin, cv=tss, method='predict_proba', n_jobs=1)
                    oof_pos = oof_proba[:, 1]
                    # 训练期 Brier
                    brier = brier_score_loss(y_bin, oof_pos)
                    # 归一化 brier -> (1 - normalized_brier) 作为 perf (0..1, 越大越好)
                    denom = (y_bin.mean() * (1.0 - y_bin.mean()) + 1e-8)
                    norm_brier = 1.0 - (brier / denom)
                    norm_brier = float(np.clip(norm_brier, 0.0, 1.0))
                except Exception:
                    # 如 cross_val_predict 失败则保守估计
                    norm_brier = 0.2

                # 最终使用 CalibratedClassifierCV 在全训练集上训练（带 TimeSeriesSplit 校准）
                try:
                    final_base = lgb.LGBMClassifier(n_estimators=400, learning_rate=0.02, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                    calib = CalibratedClassifierCV(final_base, method='sigmoid', cv=tss)
                    # 传入 sample_weight（CalibratedClassifierCV 会传给 base estimator 的 fit）
                    calib.fit(X_scaled, y_bin, sample_weight=sample_weight)
                    proba_all = pd.DataFrame(calib.predict_proba(feats_scaled), index=feats_scaled.index)
                    sig_all = (proba_all.iloc[:, 1] - proba_all.iloc[:, 0]).astype(float)  # P(1)-P(0), 等价于 2*p-1
                    # 映射到 [-1,1]：sig_all 已近似在 [-1,1]，但做 tanh 稳定化并返回 Series
                    sig_series = pd.Series(np.tanh(sig_all), index=feats_scaled.index).fillna(0.0)
                    horizon_signals[h] = sig_series
                    horizon_perf[h] = float(norm_brier)
                except Exception as e:
                    logger.warning(f"ML training/calibration failed for horizon {h}: {e}")
                    # 训练失败则退回到在全特征上用 LGBM 直接预测概率
                    try:
                        fallback = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                        fallback.fit(X_scaled, y_bin, sample_weight=sample_weight)
                        proba_all = pd.Series(fallback.predict_proba(feats_scaled)[:, 1], index=feats_scaled.index)
                        sig_series = pd.Series(np.tanh(2.0 * (2 * proba_all - 1.0)), index=feats_scaled.index).fillna(
                            0.0)
                        horizon_signals[h] = sig_series
                        horizon_perf[h] = float(norm_brier * 0.8)
                    except Exception:
                        horizon_signals[h] = pd.Series(0.0, index=feats.index)
                        horizon_perf[h] = 0.0

            except Exception as e_main:
                logger.exception(f"Unexpected error processing horizon {h}: {e_main}")
                horizon_signals[h] = pd.Series(0.0, index=feats.index)
                horizon_perf[h] = 0.0

        # 合并 horizon 信号（按 hw 权重）
        combined = pd.Series(0.0, index=feats.index, dtype=float)
        present_hw_total = 0.0
        for h in horizons:
            sig = horizon_signals.get(h, pd.Series(0.0, index=feats.index)).fillna(0.0)
            w = hw.get(h, 0.0)
            combined += sig * w
            present_hw_total += (w if not sig.isna().all() else 0.0)

        if present_hw_total == 0:
            # 保险兜底
            return pd.Series(0.0, index=feats.index), 0.01

        # 平滑并裁剪到 [-1,1]
        combined = combined.ewm(span=3, adjust=False).mean().fillna(0.0)
        combined = combined.apply(lambda x: float(np.clip(x, -1.0, 1.0)))

        # 最低训练样本保护：在训练不足 min_train_for_output 时把早期输出设为 0
        if n >= min_train_for_output:
            earliest_valid_idx = feats.index[min_train_for_output] if min_train_for_output < n else feats.index[-1]
            combined.loc[:earliest_valid_idx] = 0.0
        else:
            combined[:] = 0.0

        # 计算 ml_confidence（综合 training perf + recent magnitude + horizon 一致性）
        try:
            # training perf: 平均 horizon_perf（0..1）
            perf_vals = np.array(list(horizon_perf.values())) if horizon_perf else np.array([0.0])
            training_perf = float(np.nanmean(perf_vals)) if perf_vals.size else 0.0
            training_perf = float(np.clip(training_perf, 0.0, 1.0))

            # recent magnitude: 最近 5 天信号的绝对值均值
            recent_mag = float(combined.tail(5).abs().mean()) if len(combined) >= 5 else float(combined.abs().mean())

            # horizon 一致性: 对最后 10 天不同 horizon 信号的 std（越小越一致）
            stacked = pd.DataFrame({h: horizon_signals[h].fillna(0.0) for h in horizons})
            horiz_std = float(stacked.tail(10).std(axis=1).mean()) if stacked.shape[0] >= 2 else 0.0
            horiz_consistency = 1.0 / (1.0 + horiz_std)

            # 组合：权重（可调） -> 更看重训练期 perf（0.5），再看最近幅度（0.3），和一致性（0.2）
            ml_confidence = float(np.clip(0.5 * training_perf + 0.3 * recent_mag + 0.2 * horiz_consistency, 0.0, 0.99))
        except Exception:
            ml_confidence = 0.05

        logger.info(f"ML module finished. ml_confidence={ml_confidence:.3f}, latest_ml_signal={combined.iloc[-1]:.4f}")
        return combined.fillna(0.0), float(ml_confidence)

    @performance_monitor
    def calculate_factor_ic(self,
                            lookback: int = 252,
                            ic_window: int = 63,
                            horizons: list = None,
                            horizon_weights: dict = None,
                            ewma_span: int = 63,
                            min_periods_ratio: float = 0.5,
                            regime_filter: Optional[str] = None):
        """
        计算因子 Information Coefficient (IC) 的替换版（滚动 + EWMA）。
        输出: dict: { factor_name: {
                            'mean_ic': float,
                            'ic_std': float,
                            'ewma_ic': float,
                            'last_ic': float,
                            'ic_series': pd.Series
                        }, ... }
        参数说明:
          - lookback: 用于产出 mean_ic/ic_std 的历史窗口长度（默认 252 日）
          - ic_window: 每个滚动 IC 计算的窗口长度（默认 63 日）
          - horizons: 未来收益的 horizon 列表（例如 [1,5,10]），默认 [1,5,10]
          - horizon_weights: 每个 horizon 的权重字典，默认等权或 [1,0.6,0.4]（短期比重更大）
          - ewma_span: 用于计算 EWMA 平滑 IC 的 span
          - min_periods_ratio: 滚动窗口内最少有效数据比例阈值 (例如 0.5 => 至少 window*0.5 个有效点)
        设计要点:
          - 对每个因子与每个 horizon 计算“向后滚动的 Pearson 相关（IC）”；
          - 各 horizon 的滚动 IC 用权重合并得到单一的因子滚动 IC 序列；
          - 对最近 lookback 窗口计算 mean 和 std，并返回 EWMA 平稳值与最后值；
          - 全过程只使用历史窗口（无 look-ahead）。
        """
        import numpy as np
        import pandas as pd

        # 默认参数
        if horizons is None:
            horizons = [1, 5, 10]
        if horizon_weights is None:
            # 推荐短期更重（可按需修改）
            horizon_weights = {1: 1.0, 5: 0.6, 10: 0.4}
        # normalize horizon weights
        total_hw = sum([horizon_weights.get(h, 0.0) for h in horizons]) or 1.0
        hw = np.array([horizon_weights.get(h, 0.0) / total_hw for h in horizons])

        # 准备因子矩阵与价格序列（对齐索引）
        factor_df = pd.DataFrame(self.factors).reindex(self.data.index).astype(float).copy()
        # 若有缺失，使用前向填充再后向填充作为兜底（因子预处理应已做过）
        factor_df = factor_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)

        close = self.data['Close'].astype(float).reindex(self.data.index).copy()
        close = close.fillna(method='ffill').fillna(method='bfill')
        n = len(factor_df)
        if n == 0:
            return {}

        # 预先计算每个 horizon 的未来收益（后向看）
        fut_ret = {}
        for h in horizons:
            fut_ret[h] = (close.shift(-h) / close - 1.0).values  # numpy array with NaNs at tail

        # 内部函数: 计算单列与某个 horizon 的向后滚动 Pearson 相关（array->array）
        def _rolling_corr(x: np.ndarray, y: np.ndarray, window: int, min_periods: int):
            """
            返回长度=n 的数组，index 对应原序列索引；不足时为 np.nan。
            x,y: 1D numpy arrays (可能有 nan)
            """
            res = np.full_like(x, np.nan, dtype=float)
            if window <= 1 or len(x) < window:
                return res
            # 遍历终点（以窗口为单位）
            for i in range(window - 1, len(x)):
                xs = x[(i - window + 1):(i + 1)]
                ys = y[(i - window + 1):(i + 1)]
                mask = (~np.isnan(xs)) & (~np.isnan(ys))
                cnt = int(mask.sum())
                if cnt < min_periods:
                    continue
                xv = xs[mask]
                yv = ys[mask]
                # 零方差保护
                if xv.std(ddof=0) < 1e-12 or yv.std(ddof=0) < 1e-12:
                    # 如果无波动，相关性定义为 0（更稳健）
                    res[i] = 0.0
                else:
                    c = np.corrcoef(xv, yv)[0, 1]
                    # 有时会产生微小超出 [-1,1] 的值，裁剪
                    res[i] = float(np.clip(c, -1.0, 1.0))
            return res

        # 最低有效点数
        min_periods = max(4, int(ic_window * min_periods_ratio))

        ic_results = {}
        index = factor_df.index

        # 对每个因子循环计算（因子数量通常不大，直接 loop 可读性高）
        for col in factor_df.columns:
            x_arr = factor_df[col].values.astype(float)

            # 【核心修改】如果传入了状态过滤器，则用NaN屏蔽非该状态的数据
            if regime_filter and regime_filter in ['BULL', 'BEAR']:
                # 从 self.regime_series 获取布尔掩码
                mask = (self.regime_series == regime_filter).values
                # 将不符合条件的位置设置为NaN
                x_arr[~mask] = np.nan

            # per-horizon rolling ICs (shape: n_horizons x n)
            horizon_ic_matrix = []
            for h in horizons:
                y_arr = fut_ret[h].copy()  # 使用副本以防污染

                # 【核心修改】同样屏蔽未来收益中的非该状态数据
                if regime_filter and regime_filter in ['BULL', 'BEAR']:
                    y_arr[~mask] = np.nan

                # 注意：y_arr 的 tail 部分会含 NaN（shift 导致），rolling_corr 会自动跳过
                rc = _rolling_corr(x_arr, y_arr, ic_window, min_periods)
                horizon_ic_matrix.append(rc)
            # 转置为 (n, num_horizons)
            mat = np.vstack(horizon_ic_matrix).T  # shape (n, H)

            # 行合并：对存在 value 的 horizon 做加权平均（忽略 NaN）
            combined = np.full(n, np.nan, dtype=float)
            for i in range(n):
                row = mat[i, :]
                valid = ~np.isnan(row)
                if not valid.any():
                    continue
                wvalid = hw[valid]
                if wvalid.sum() == 0:
                    combined[i] = np.nan
                else:
                    combined[i] = float(np.dot(row[valid], wvalid) / wvalid.sum())

            ic_series = pd.Series(combined, index=index).astype(float)

            # 轻微的 EWMA 平滑（可选），然后用于输出 ewma_ic
            ic_ewma = ic_series.ewm(span=ewma_span, adjust=False).mean()

            # recent window 用于 mean/std 计算
            if lookback <= 0:
                # 若 lookback 非正，则用全部有效样本
                recent = ic_series.dropna()
            else:
                recent = ic_series.dropna().iloc[-lookback:]

            if recent.empty:
                mean_ic = 0.0
                ic_std = 0.0
            else:
                mean_ic = float(recent.mean())
                ic_std = float(recent.std(ddof=0))  # population std，optimize 中使用即可

            last_ic = float(ic_series.dropna().iloc[-1]) if not ic_series.dropna().empty else 0.0
            ewma_ic = float(ic_ewma.dropna().iloc[-1]) if not ic_ewma.dropna().empty else mean_ic

            ic_results[col] = {
                'mean_ic': mean_ic,
                'ic_std': ic_std,
                'ewma_ic': ewma_ic,
                'last_ic': last_ic,
                'ic_series': ic_series  # pandas Series，供 debug / 可视化使用
            }

        # 返回结果（字典）
        return ic_results

    def run_full_analysis(self) -> Dict[str, Any]:
        """【V7.4 】完整分析流程，包含完备的情景IC选择逻辑"""
        try:
            logger.info("🔄 (V7.4 ) 开始完整分析流程...")
            self.calculate_all_factors()
            self.factor_preprocessing()
            trend_regime, unified_risk_score = self._calculate_regime_scores()
            self.regime_series, self.unified_risk_score = trend_regime, unified_risk_score

            current_trend_regime = trend_regime.iloc[-1]
            current_risk_score = unified_risk_score.iloc[-1]

            logger.info("为簇内动态加权计算多情景IC...")
            ic_results_bull = self.calculate_factor_ic(regime_filter='BULL')
            ic_results_bear = self.calculate_factor_ic(regime_filter='BEAR')
            ic_results_all = self.calculate_factor_ic(regime_filter=None)

            # --- 【V7.4 逻辑修正】根据情景选择最合适的IC ---
            if current_trend_regime == 'BULL':
                ic_for_synthesis, ic_source = ic_results_bull, "BULL"
            elif current_trend_regime == 'BEAR':
                ic_for_synthesis, ic_source = ic_results_bear, "BEAR"
            else:  # NEUTRAL or UNKNOWN
                ic_for_synthesis, ic_source = ic_results_all, "ALL (NEUTRAL)"
            logger.info(f"当前市场情景: {current_trend_regime}，已选择 {ic_source} 周期的IC数据进行合成。")

            cluster_signals = self.synthesize_cluster_signals(ic_for_synthesis)
            meta_weights = self.get_meta_strategy_weights(current_risk_score)

            composite_signal = sum(signal * meta_weights.get(name, 0.0) for name, signal in cluster_signals.items())

            ml_signal, ml_confidence = self.machine_learning_predict()

            cluster_model_confidence = np.clip(abs(current_risk_score), PARAMS['CLUSTER_CONF_MIN'],
                                               PARAMS['CLUSTER_CONF_MAX'])
            total_confidence = cluster_model_confidence + ml_confidence + 1e-8
            weight_ic = cluster_model_confidence / total_confidence
            weight_ml = ml_confidence / total_confidence

            logger.info(f"自适应融合: 策略簇置信度={cluster_model_confidence:.2%}, ML置信度={ml_confidence:.2%}")
            logger.info(f"最终融合权重: 策略簇模型={weight_ic:.2%}, ML模型={weight_ml:.2%}")

            final_signal = (composite_signal * weight_ic) + (ml_signal * weight_ml)
            disagreement = ((composite_signal - ml_signal).abs() / (
                        composite_signal.abs() + ml_signal.abs() + 1e-8)).fillna(0.0)

            return {
                'composite_signal': composite_signal, 'ml_signal': ml_signal, 'ml_confidence': ml_confidence,
                'final_signal': final_signal, 'disagreement': disagreement,
                'current_trend_regime': current_trend_regime,
                'current_risk_score': current_risk_score, 'cluster_signals': cluster_signals,
                'meta_weights': meta_weights,
                'fusion_weights': {'ml': weight_ml, 'ic': weight_ic},
                'cluster_model_confidence': cluster_model_confidence,
                'all_factors': self.factors, 'ic_results': ic_for_synthesis, 'ic_source': ic_source
            }
        except Exception as e:
            logger.error(f"因子分析失败: {e}\n{traceback.format_exc()}")
            # 返回安全的默认结构
            return {
                'composite_signal': pd.Series(0.0, index=self.data.index),
                'ml_signal': pd.Series(0.0, index=self.data.index),
                'ml_confidence': 0.0, 'final_signal': pd.Series(0.0, index=self.data.index),
                'disagreement': pd.Series(0.0, index=self.data.index),
                'current_trend_regime': 'UNKNOWN', 'current_risk_score': 0.0, 'cluster_signals': {}, 'meta_weights': {},
                'fusion_weights': {}, 'cluster_model_confidence': 0.0, 'all_factors': {}, 'ic_source': 'ERROR'
            }


# ===================================
# 风险管理系统 (策略定制版：仅止损)
# ===================================
class RiskManager:
    def __init__(self, data: pd.DataFrame):
        self.data = data.copy()
        logger.info("风险管理模块初始化完成 (策略: 仅止损)。")

    def calculate_atr(self, period: int = 14) -> float:
        if len(self.data) < period:
            logger.warning("数据不足以计算ATR。")
            return 0.0
        high, low, close = self.data['High'], self.data['Low'], self.data['Close']
        tr = pd.concat([high - low, abs(high - close.shift(1)), abs(low - close.shift(1))], axis=1).max(axis=1)
        atr = tr.ewm(alpha=1 / period, adjust=False).mean().iloc[-1]
        return float(atr) if pd.notna(atr) else 0.0

    def calculate_position_size(self, capital: float, risk_per_trade: float = None, atr_period: int = 14) -> float:
        try:
            if risk_per_trade is None: risk_per_trade = float(os.getenv("RISK_PER_TRADE", "0.02"))
            risk_per_trade = float(np.clip(risk_per_trade, 0.005, 0.05))
            atr = self.calculate_atr(atr_period)
            current_price = float(self.data['Close'].iloc[-1])
            if atr <= 0 or capital <= 0 or current_price <= 0: return 0.0

            risk_amount_per_trade = capital * risk_per_trade
            position_quantity_by_risk = risk_amount_per_trade / atr
            max_position_quantity_by_capital = (capital * 0.5) / current_price
            final_quantity = min(position_quantity_by_risk, max_position_quantity_by_capital)
            position_value = final_quantity * current_price
            logger.info(
                f"建议头寸: {final_quantity:.4f} 单位 (价值 ${position_value:,.2f}), 资本=${capital:,.2f}, 风险比例={risk_per_trade:.2%}")
            return final_quantity
        except Exception as e:
            logger.error(f"❌ 头寸计算失败: {e}")
            return 0.0

    def calculate_stop_loss(self, entry_price: float, atr_period: int = 14) -> float:
        try:
            atr = self.calculate_atr(atr_period)
            if atr <= 0 or entry_price <= 0: return 0.0
            stop_loss = entry_price - atr * PARAMS['ATR_STOP_LOSS_MULTIPLIER']
            logger.debug(f"止损位计算完成: 入场价=${entry_price:.2f}, 止损=${stop_loss:.2f}")
            return float(stop_loss)
        except Exception as e:
            logger.error(f"❌ 止损位计算失败: {e}")
            return 0.0


# ===================================
# 预测信号生成与报告
# ===================================

def _calculate_dynamic_thresholds(base_buy: float, base_exit: float, risk_score: float, composite_confidence: float,
                                  signal_volatility: float, historical_noise: float, trend_adjust_factor: float,
                                  disagreement: float) -> Tuple[float, float]:
    """【V7.4】动态门槛计算，新增了对‘系统确定性’的考量。"""

    # 1. 风险调整 (与之前相同)
    risk_adjustment = trend_adjust_factor * np.tanh(risk_score * 1.5)

    # 2. 置信度调整 (与之前相同)
    conf_scale = 1.0 / np.sqrt(max(0.05, composite_confidence))

    # 3. 噪声调整 (与之前相同)
    noise_ratio = signal_volatility / (historical_noise + 1e-8)
    noise_scale = np.clip(1.0 + 0.75 * (noise_ratio - 1.0), 0.7, 1.6)

    # 4. 【【【核心升级：引入确定性因子】】】
    # disagreement 范围 [0, 1]。disagreement 越大，系统越“困惑”，确定性越低。
    # certainty_factor 范围 [0.7, 1.0]。分歧度越高，确定性因子越小。
    certainty_factor = np.clip(1.0 - 0.3 * disagreement, 0.7, 1.0)

    # 确定性因子将直接影响门槛：系统越不确定，买入门槛越高，卖出门槛越低，中间的“观察区”越宽。
    buy_th_core = base_buy * (1 - risk_adjustment) * conf_scale * noise_scale / certainty_factor
    exit_th_core = base_exit * (1 + risk_adjustment) * conf_scale * noise_scale * certainty_factor

    # 5. 滞回调整 (与之前相同)
    hysteresis = np.clip(0.5 * signal_volatility, 0.05, 0.15)

    buy_threshold_final = np.clip(buy_th_core + hysteresis, 0.20, 0.80)
    exit_threshold_final = np.clip(exit_th_core - hysteresis, -0.80, -0.20)

    logger.info(
        f"动态门槛计算(V7.4): 风险因子={risk_adjustment:.2f}, 置信Scale={conf_scale:.2f}, "
        f"噪声Scale={noise_scale:.2f}, 【确定性因子={certainty_factor:.2f}】 -> "
        f"最终买入 > {buy_threshold_final:.3f}, 最终离场 < {exit_threshold_final:.3f}"
    )
    return buy_threshold_final, exit_threshold_final


def _get_top_factor_contributors(factor_results: Dict[str, Any], top_n: int = 3) -> str:
    try:
        meta_weights = factor_results.get('meta_weights', {})
        cluster_signals = factor_results.get('cluster_signals', {})
        all_factors = factor_results.get('all_factors', {})

        if not all([meta_weights, cluster_signals, all_factors]):
            return "   • 贡献度分析数据不足。\n"

        # 1. 计算每个策略簇的最终贡献
        cluster_contributions = {
            name: meta_weights.get(name, 0.0) * (cluster_signals.get(name, pd.Series([0.0])).iloc[-1])
            for name in meta_weights
        }

        # 2. 找到贡献最大的策略簇
        dominant_cluster = max(cluster_contributions, key=lambda k: abs(cluster_contributions[k]))

        # 3. 在这个主导簇内，找到贡献最大的因子
        report_lines = []
        cluster_factor_names = FACTOR_CLUSTERS.get(dominant_cluster, [])
        factor_values = {
            name: all_factors[name].iloc[-1]
            for name in cluster_factor_names if name in all_factors and not all_factors[name].empty
        }

        if not factor_values:
            return f"   • 主导策略簇 '{dominant_cluster.upper()}' (贡献: {cluster_contributions[dominant_cluster]:+.3f})，但内部因子数据缺失。\n"

        # 排序因子，找到最正和最负的
        sorted_factors = sorted(factor_values.items(), key=lambda item: item[1])
        top_positive = sorted_factors[-top_n:]
        top_negative = sorted_factors[:top_n]

        report_lines.append(
            f"   • 主导策略簇: **{dominant_cluster.upper()}** (总贡献: {cluster_contributions[dominant_cluster]:+.3f})")

        report_lines.append("     - 最强看多因子:")
        for name, value in reversed(top_positive):
            if value > 0:
                report_lines.append(f"       • {name:<18s}: {value:+.3f}")

        report_lines.append("     - 最强看空因子:")
        for name, value in top_negative:
            if value < 0:
                report_lines.append(f"       • {name:<18s}: {value:+.3f}")

        return "\n".join(report_lines) + "\n"
    except Exception as e:
        # 确保此辅助函数绝不引发主程序崩溃
        return f"   • [错误] 贡献度分析失败: {e}\n"


def generate_signal_and_report(data: pd.DataFrame, factor_results: Dict[str, Any], current_price: float, source: str) -> \
        Tuple[Dict[str, Any], str]:
    """
    (V7.4 ) 决策报告
    - 注入了动态门槛与滞回机制，提升实盘稳健性。
    - [FIX] 修正了 'fusion_weights' 的引用错误。
    """
    try:
        # --- 步骤 1: 安全地提取所有核心数据 ---
        final_signal_series = factor_results.get('final_signal', pd.Series(dtype=float))
        final_strength = final_signal_series.iloc[-1] if not final_signal_series.empty else 0.0

        composite_signal_series = factor_results.get('composite_signal', pd.Series(dtype=float))
        composite_strength = composite_signal_series.iloc[-1] if not composite_signal_series.empty else 0.0

        ml_signal_series = factor_results.get('ml_signal', pd.Series(dtype=float))
        ml_strength = ml_signal_series.iloc[-1] if not ml_signal_series.empty else 0.0

        # --- 【V7.4 修正】将单行赋值拆分为多行，解决引用错误 ---
        fusion_weights = factor_results.get('fusion_weights', {})
        w_ml = fusion_weights.get('ml', 0.0)
        w_ic = fusion_weights.get('ic', 0.0)
        # --- 修正结束 ---

        trend_regime = factor_results.get('current_trend_regime', '数据缺失')
        risk_score = factor_results.get('current_risk_score', 0.0)
        cluster_signals = factor_results.get('cluster_signals', {})
        meta_weights = factor_results.get('meta_weights', {})
        cluster_confidence = factor_results.get('cluster_model_confidence', 0.0)
        ml_confidence = factor_results.get('ml_confidence', 0.0)
        ic_source = factor_results.get('ic_source', 'UNKNOWN')

        # --- 步骤 2: 【核心升级】计算动态决策门槛与滞回 ---
        sig_vol = float(final_signal_series.tail(20).std() or 0.0)
        hist_vol = float((final_signal_series.rolling(60).std().median() or 1e-8))

        composite_conf = float(np.clip((cluster_confidence * w_ic + ml_confidence * w_ml), 0.05, 0.95))

        # 调用新的、经过优化的门槛计算函数
        disagreement_series = factor_results.get('disagreement', pd.Series(dtype=float))
        current_disagreement = disagreement_series.iloc[-1] if not disagreement_series.empty else 0.0

        BUY_THRESHOLD, EXIT_THRESHOLD = _calculate_dynamic_thresholds(
            base_buy=PARAMS['BASE_BUY_THRESHOLD'],
            base_exit=PARAMS['EXIT_THRESHOLD'],
            risk_score=float(risk_score),
            composite_confidence=composite_conf,
            signal_volatility=sig_vol,
            historical_noise=hist_vol,
            trend_adjust_factor=PARAMS.get('TREND_ADJUST_FACTOR', 0.15),
            disagreement=float(current_disagreement)  # 【【【新增传递参数】】】
        )

        if final_strength > BUY_THRESHOLD:
            recommendation = "买入或增持"
        elif final_strength < EXIT_THRESHOLD:
            recommendation = "平仓离场 (趋势反转警告)"
        else:
            recommendation = "持仓观察"

        # --- 步骤 3: 组装结果对象 ---
        result = {
            'signal': recommendation,
            'final_strength': float(final_strength),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'buy_threshold': float(BUY_THRESHOLD),
            'exit_threshold': float(EXIT_THRESHOLD)
        }

        # --- 步骤 4: 生成健壮的报告字符串 ---
        risk_interp = "极度风险偏好 (Risk-On)" if risk_score > 0.7 else \
            "温和风险偏好" if risk_score > 0.2 else \
                "中性/观望" if risk_score > -0.2 else \
                    "温和风险规避" if risk_score > -0.7 else "极度风险规避 (Risk-Off)"
        if trend_regime == '数据缺失':
            risk_interp = "无法判断 (上游数据缺失)"

        report_str = f"""
【纳斯达克决策报告 (V7.4 )】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
📅 时间: {result['timestamp']} (时区: Asia/Shanghai)
📈 当前点位: {current_price:.2f} | 📊 数据来源: {source}

🌐 第一层: 市场情景深度诊断
   • 长期趋势状态: {trend_regime}
   • 综合风险指数: {risk_score:+.3f} [-1, +1]
   • 情景解读: 当前市场处于 {risk_interp} 状态。

🧠 第二层: 动态策略配置
   (IC数据源: {ic_source} 周期)
"""
        if not meta_weights:
            report_str += "   • 策略权重数据缺失。\n"
        else:
            sorted_weights = sorted(meta_weights.items(), key=lambda item: item[1], reverse=True)
            for name, weight in sorted_weights:
                signal_series = cluster_signals.get(name, pd.Series(dtype=float))
                value = signal_series.iloc[-1] if not signal_series.empty else 0.0
                report_str += f"   • {name.upper():<12}权重: {weight:.2%} (信号: {value: .3f})\n"
        report_str += f"   └───> 策略簇模型总信号: {composite_strength:.3f}\n"

        report_str += f"""
🎯   第三层: 自适应模型融合
   • 策略簇模型置信度: {cluster_confidence:.2%} | 机器学习模型置信度: {ml_confidence:.2%}
   • 最终融合权重: [策略簇: {w_ic:.2%}] vs [机器学习: {w_ml:.2%}]

⚡   最终决策
   • 最终融合信号: {result['final_strength']:.3f}
   • 决策门槛: 买入 > {result['buy_threshold']:.2f} | 离场 < {result['exit_threshold']:.2f}
   •   ==> 推荐动作: {recommendation} <==
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
💡   【决策核心溯源】
{_get_top_factor_contributors(factor_results)}
【决策逻辑说明】
本报告展示了一个三层决策过程：首先诊断市场情景，然后基于情景配置策略权重(IC情景: {ic_source})，最后自适应地融合策略模型与机器学习模型，得出最终的、高度情景化的交易信号。
【风险提示】
模型预测基于历史数据，不保证未来表现。请审慎决策。
"""
        return result, report_str
    except Exception as e:
        logger.error(f"信号与报告生成时发生严重意外错误: {e}\n{traceback.format_exc()}")
        err_report = f"""
【严重错误：决策报告生成失败】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
系统在尝试生成决策报告时遭遇了无法处理的内部错误。

错误类型: {type(e).__name__}
错误信息: {str(e)}

请检查日志文件 `cache/livermore_enhanced.log` 获取完整的错误追溯信息。
"""
        return {'signal': 'ERROR', 'error': str(e)}, err_report

# ===================================
# 主执行函数 (V7.4 )
# ===================================
@performance_monitor
def main(initial_capital: float = 100000.0):  # 【V7.4 修改】
    logger.info(f"🚀 纳斯达克增强版量化预测系统启动 (V7.4 )... 初始模拟资本: ${initial_capital:,.2f}")
    try:
        df, current_price, source = fetch_nasdaq_data()
        if df.empty:
            logger.error("❌ 数据获取失败，程序终止")
            return
        logger.info(f"✅ 数据获取成功: {len(df)}条, 当前价格: {current_price:.2f} (来源: {source})")

        latest_data_date = df.index[-1].date()
        today_date = datetime.now().date()
        date_diff = (today_date - latest_data_date).days

        # 允许的最大数据延迟天数（考虑周末和节假日）
        MAX_DATA_DELAY_DAYS = 3

        if date_diff > MAX_DATA_DELAY_DAYS:
            error_msg = f"🔴 CRITICAL: 数据严重陈旧！最新数据日期为 {latest_data_date}，已延迟 {date_diff} 天。为保证安全，系统终止运行。"
            logger.critical(error_msg)
            send_notification(subject="【严重警告】量化系统因数据陈旧已自动终止", content=error_msg)
            return  # 关键：终止程序执行

        try:
            if 'Date' in df.columns: df.set_index('Date', inplace=True)
            if df.index.has_duplicates: df = df[~df.index.duplicated(keep='last')]
            df.sort_index(inplace=True)
            if df.index.tz is not None:
                logger.info(f"检测到时区信息 ({df.index.tz})，正在进行标准化处理...")
                df.index = df.index.tz_localize(None)
                logger.info("✅ 时区信息已剥离。")
        except Exception as e:
            logger.error(f"❌ 索引净化失败: {e}，程序终止。")
            return

        engine = EnhancedFactorEngine(df)
        factor_results = engine.run_full_analysis()
        signal_result, report_str = generate_signal_and_report(df, factor_results, current_price, source)

        logger.info("🔄 开始执行核心决策与风险管理流程...")
        state = load_position_state()
        market_regime = factor_results.get('current_trend_regime', 'BEAR')
        is_buy_signal = "买入" in str(signal_result.get('signal', ''))
        is_model_exit_signal = "平仓离场" in str(signal_result.get('signal', ''))

        if state.get('status') == 'long':
            if 'entry_date' not in state or not state.get('entry_date'):
                logger.warning("状态文件缺少'entry_date'，启动自动修复程序...")
                entry_price = state.get('entry_price')
                if entry_price:
                    possible_dates = df[np.isclose(df['Close'], entry_price, rtol=0.005)].index
                    if not possible_dates.empty:
                        state['entry_date'] = possible_dates[0].strftime('%Y-%m-%d')
                        save_position_state(state)
                        logger.info(f"✅ 自动修复成功！推断入场日期为: {state['entry_date']}。")
                    else:
                        logger.error(f"❌ 自动修复失败：无法在历史数据中找到与入场价 ${entry_price:.2f} 匹配的日期。")

            entry_date_str = state.get('entry_date')
            livermore_triggered = False
            livermore_stop_price = 0.0

            if entry_date_str:
                position_history_df = df[df.index >= pd.to_datetime(entry_date_str)]
                if not position_history_df.empty:
                    true_peak_price = position_history_df['High'].max()
                    prev_peak = float(state.get('peak_price_since_entry') or 0.0)
                    peak = max(prev_peak, true_peak_price)
                    livermore_stop_price = peak * (1 - PARAMS['LIVERMORE_DRAWDOWN_PERCENT'])
                    entry_price = float(state.get('entry_price', current_price))
                    pnl_percent = (current_price / entry_price - 1) * 100
                    drawdown_from_peak = (1 - current_price / peak) * 100 if peak > 0 else 0

                    # 根据盈亏状态选择表情符号，增加可读性
                    pnl_emoji = "🟢" if pnl_percent >= 0 else "🔴"

                    logger.info(
                        f"持仓检查: 入场日 {entry_date_str} | "
                        f"当前价格: {current_price:.2f} | "
                        f"{pnl_emoji} 盈亏: {pnl_percent:+.2f}% | "
                        f"📉 峰值回撤: {drawdown_from_peak:.2f}% (历史峰值: {peak:.2f}) | "
                        f"🛡️ 止损位: {livermore_stop_price:.2f}"
                    )
                    if current_price < livermore_stop_price: livermore_triggered = True
                    if peak > prev_peak:
                        state['peak_price_since_entry'] = peak
                        save_position_state(state)

            model_driven_close = is_model_exit_signal or (market_regime == 'BEAR' and not livermore_triggered)
            must_close = livermore_triggered or model_driven_close

            if must_close:
                reason = (
                    f"利弗莫尔止损" if livermore_triggered else f"模型信号平仓 ({'趋势反转' if is_model_exit_signal else '进入熊市'})")
                logger.info(f"➡️ 触发平仓，原因: {reason}")
                position_details_str = f"已执行平仓，原因: {reason}"
                stop_loss_details_str = "      (已平仓)"
                state = {'status': 'flat', 'entry_price': None, 'entry_date': None, 'peak_price_since_entry': None}
                save_position_state(state)
            else:
                entry_price = float(state.get('entry_price', 0))
                position_details_str = f"持仓观察 (入场价: ${entry_price:.2f})"
                risk_manager = RiskManager(df)
                initial_stop_loss = risk_manager.calculate_stop_loss(entry_price=entry_price)

                atr = risk_manager.calculate_atr()
                safety_score = (current_price - livermore_stop_price) / (atr + 1e-8)
                safety_tag = "✅ 安全" if safety_score >= 1.5 else "⚠️ 中性" if safety_score >= 0.7 else "🚨 警戒"
                stop_loss_details_str = (f"   • 初始ATR止损 (保底): ${initial_stop_loss:.2f}\n"
                                         f"   • 利弗莫尔动态止损: ${livermore_stop_price:.2f} (基于峰值 {state.get('peak_price_since_entry', 0):.2f})   {safety_tag}")
        else:  # 空仓中
            can_open = (market_regime == 'BULL') and is_buy_signal
            if can_open:
                entry_price = float(current_price)
                entry_date_str = df.index[-1].strftime('%Y-%m-%d')
                logger.info(f"✅ 触发开仓条件，入场价: {entry_price:.2f}, 入场日期: {entry_date_str}")
                state = {'status': 'long', 'entry_price': entry_price, 'entry_date': entry_date_str,
                         'peak_price_since_entry': entry_price}
                save_position_state(state)
                risk_manager = RiskManager(df)
                position_quantity = risk_manager.calculate_position_size(capital=initial_capital)
                disagreement = factor_results.get('disagreement', pd.Series([0.0])).iloc[-1]
                shrink_factor = max(PARAMS['POSITION_DISAGREEMENT_MIN_SCALE'],
                                    (1 - float(disagreement)) ** PARAMS['POSITION_DISAGREEMENT_POWER'])
                position_quantity *= shrink_factor
                position_value = position_quantity * current_price
                position_details_str = f"已开仓 ${position_value:,.2f} ({position_quantity:.4f} 单位) (已根据分歧度 {disagreement:.1%} 调整)"
                initial_stop_loss = risk_manager.calculate_stop_loss(entry_price=entry_price)
                livermore_sl_on_open = entry_price * (1 - PARAMS['LIVERMORE_DRAWDOWN_PERCENT'])
                stop_loss_details_str = f"   • **初始ATR止损 (保底)**: ${initial_stop_loss:.2f}\n"
                stop_loss_details_str += f"   • **利弗莫尔动态止损**: ${livermore_sl_on_open:.2f} (基于当前峰值)"
            else:
                position_details_str = "空仓或无操作"
                stop_loss_details_str = "      (当前无持仓)"

        full_report = report_str + f"""
💰   头寸与风险管理
   •   当前状态: {position_details_str}
   •   止盈策略: 无固定止盈位，让利润奔跑
   •   止损策略:
{stop_loss_details_str}
"""
        if market_regime == 'BEAR' and state.get('status') != 'long':
            full_report += "\n⚠️ 熊市警告: 系统暂停一切新开仓操作以规避风险。\n"

        logger.info(full_report)
        send_notification(subject=f"纳指决策报告: {signal_result['signal']} {datetime.now().strftime('%m-%d')}",
                          content=full_report)
        with open('cache/nasdaq_signal.json', 'w', encoding='utf-8') as f:
            json.dump(signal_result, f, ensure_ascii=False, indent=2)
        logger.info("✅ 程序执行完成！")

    except Exception as e:
        logger.error(f"💥 主程序执行失败: {e}")
        error_trace = traceback.format_exc()
        logger.error(error_trace)
        send_notification(subject="纳斯达克交易系统错误报告", content=f"系统运行错误:\n\n{str(e)}\n\n{error_trace}")


# ===================================
# 定时任务入口
# ===================================
def schedule_daily_analysis():
    """每日自动分析任务入口，可对接定时器/cron等"""
    main()


if __name__ == "__main__":
    # 您可以在这里修改模拟启动资金
    main(initial_capital=100000.0)

# ===================================
# 结尾
# ===================================
```

## 副程序
> 文件名：macro_engine_strict.py

```python
# -*- coding: utf-8 -*-
"""
宏观数据引擎模块 (macro_engine_strict.py)
版本: V7.4 (最终版 - 集成所有已验证因子)

核心功能:
- 负责从akshare获取所有宏观经济数据。
- 内置手动计算逻辑，以处理无直接接口的数据（如PPI年率）。
- 内置磁盘缓存机制，大幅提升二次运行时的加载速度。
"""

import pandas as pd
import numpy as np
import akshare as ak
from typing import Dict
import logging
import traceback
import os
from datetime import datetime, timedelta

# =================================================================
# 日志系统
# =================================================================
logger = logging.getLogger('macro_engine')


# =================================================================
# 辅助数据获取与计算函数 (核心逻辑)
# =================================================================

def get_usa_ppi_yoy_from_monthly():
    """
    通过文档中存在的 ak.macro_usa_ppi (月率) 接口，手动计算年率(YoY)。
    """
    try:
        df_monthly = ak.macro_usa_ppi()
        if df_monthly.empty: return pd.DataFrame()

        df_monthly['日期'] = pd.to_datetime(df_monthly['日期'])
        df_monthly = df_monthly.set_index('日期').sort_index()

        monthly_returns = df_monthly['今值'] / 100.0
        yoy_returns = (1 + monthly_returns).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1

        final_df = (yoy_returns * 100).to_frame('今值').reset_index()
        final_df.columns = ['日期', '今值']
        final_df['今值'] = pd.to_numeric(final_df['今值'], errors='coerce')
        final_df.dropna(subset=['今值'], inplace=True)
        return final_df
    except Exception:
        return pd.DataFrame()


# =================================================================
# 宏观事件定义 (最终版)
# =================================================================
EVENTS = {
    # --- 原有因子 ---
    'pce': {'func': ak.macro_usa_core_pce_price},
    'nfp': {'func': ak.macro_usa_non_farm},
    'rtl': {'func': ak.macro_usa_retail_sales},
    'ism_non_mfg': {'func': ak.macro_usa_ism_non_pmi},  # 已改名以区分
    'job': {'func': ak.macro_usa_initial_jobless},
    'epu': {'func': lambda: ak.article_epu_index(symbol="USA")},

    # --- 所有已通过测试的新因子 ---
    'cpi_yoy': {'func': ak.macro_usa_cpi_yoy},
    'ppi_yoy': {'func': get_usa_ppi_yoy_from_monthly},
    'ism_mfg': {'func': ak.macro_usa_ism_pmi},
    'durable': {'func': ak.macro_usa_durable_goods_orders},
    'confidence': {'func': ak.macro_usa_cb_consumer_confidence},
    'unemployment': {'func': ak.macro_usa_unemployment_rate},
}


def fetch_event(name: str) -> pd.Series:
    """
    (V7.4) 宏观数据获取引擎。
    """
    cfg = EVENTS[name]
    CACHE_DIR = "cache"
    CACHE_EXPIRATION = timedelta(hours=6)
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"macro_{name}.pkl")

    if os.path.exists(cache_file):
        file_mod_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
        if datetime.now() - file_mod_time < CACHE_EXPIRATION:
            try:
                logger.info(f"✅ 从缓存加载宏观数据: {name}")
                final_series = pd.read_pickle(cache_file)
                if not final_series.empty:
                    return final_series
            except Exception as e:
                logger.warning(f"加载宏观数据缓存 {name} 失败: {e}，将重新获取。")

    try:
        logger.info(f"🔄 从网络获取宏观数据: {name}")
        raw = cfg['func']()

        if raw is None or not isinstance(raw, pd.DataFrame) or raw.empty:
            logger.warning(f"宏观数据 {name} 为空或格式不正确。")
            return pd.Series(dtype=float)

        raw.columns = [str(c).lower() for c in raw.columns]

        val_col_name = None
        possible_value_cols = ['今值', '现值', '最新值', '值', 'index']
        for col in raw.columns:
            if any(k in col for k in possible_value_cols):
                val_col_name = col
                break
        if not val_col_name:
            logger.warning(f"在 {name} 数据中无法识别数值列: {raw.columns}")
            return pd.Series(dtype=float)

        date_series = None
        if 'year' in raw.columns and 'month' in raw.columns:
            try:
                date_series = pd.to_datetime(raw['year'].astype(str) + '-' + raw['month'].astype(str), errors='coerce')
            except Exception:
                pass

        if date_series is None or date_series.isna().all():
            date_col_name = None
            possible_date_cols = ['日期', 'date', '时间', '统计时间']
            for col in raw.columns:
                if any(k in col for k in possible_date_cols):
                    date_col_name = col
                    break
            if date_col_name:
                date_series = pd.to_datetime(raw[date_col_name], errors='coerce')

        if date_series is None or date_series.isna().all():
            logger.error(f"❌ 在 {name} 数据中无法构建有效的日期索引。列: {raw.columns}")
            return pd.Series(dtype=float)

        final_series = pd.Series(data=pd.to_numeric(raw[val_col_name], errors='coerce').values, index=date_series)
        final_series.dropna(inplace=True)

        if final_series.empty:
            logger.warning(f"因子 {name} 处理后数据为空。")
            return pd.Series(dtype=float)

        final_series_sorted = final_series[~final_series.index.duplicated(keep='last')].sort_index()
        logger.info(f"✅ 获取宏观数据 {name} 成功, 最新值: {final_series_sorted.iloc[-1]}")

        try:
            pd.to_pickle(final_series_sorted, cache_file)
            logger.info(f"💾 宏观数据 {name} 已缓存至本地: {cache_file}")
        except Exception as e:
            logger.error(f"缓存宏观数据 {name} 失败: {e}")

        return final_series_sorted

    except Exception as e:
        logger.error(f"❌ 获取宏观数据 {name} 失败: {e}\n{traceback.format_exc()}")
        return pd.Series(dtype=float)


class StrictMacro:
    def __init__(self, idx: pd.DatetimeIndex):
        self.idx = idx if isinstance(idx, pd.DatetimeIndex) else pd.to_datetime(idx)
        self.idx = self.idx.normalize().drop_duplicates().sort_values()
        self.logger = logger
        self.logger.info("严格宏观因子构建器 (StrictMacro) 初始化完成。")

    def factor(self, name: str) -> pd.Series:
        raw = fetch_event(name)
        if raw.empty:
            return pd.Series(0.0, index=self.idx)
        try:
            factor_series = raw.reindex(self.idx, method='ffill')
            factor_series.fillna(method='bfill', inplace=True)
            factor_series.fillna(0.0, inplace=True)
            self.logger.info(f"宏观因子 {name} 构建完成, 返回其水平值。最新值: {factor_series.iloc[-1]:.4f}")
            return factor_series
        except Exception as e:
            self.logger.error(f"构建宏观因子 {name} 失败: {e}\n{traceback.format_exc()}")
            return pd.Series(0.0, index=self.idx)

    def factors(self) -> Dict[str, pd.Series]:
        self.logger.info("开始构建所有宏观因子...")
        all_factors = {n: self.factor(n) for n in EVENTS}
        self.logger.info("✅ 所有宏观因子构建完成。")
        return all_factors
```
