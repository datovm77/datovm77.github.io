## ä¸»ç¨‹åº
> æ–‡ä»¶åï¼šçº³æ–¯è¾¾å…‹é‡åŒ–æ¨¡å‹ï¼ˆä¸»ç¨‹åºï¼‰.py

> æ‰€éœ€åº“ï¼špip install pandas numpy tqdm akshare yfinance scipy scikit-learn yagmail pyarrow fastparquet

> çº³æ–¯è¾¾å…‹è„šæœ¬é»˜è®¤ç”¨ QQ é‚®ç®±ï¼Œéœ€å¼€é€š IMAP/SMTP æœåŠ¡ï¼Œç”Ÿæˆ æˆæƒç ï¼ˆéç™»å½•å¯†ç ï¼‰ï¼Œå¡«å…¥EMAIL_USERå¤„

> åŠŸèƒ½ï¼šç›®æ ‡ï¼šåŸºäºå¤šå› å­æ¨¡å‹ã€æœºå™¨å­¦ä¹ å’Œé£é™©æ§åˆ¶ï¼Œå¯¹ çº³æ–¯è¾¾å…‹æŒ‡æ•° è¿›è¡Œè¶‹åŠ¿é¢„æµ‹å¹¶ç”Ÿæˆä¹°å–ä¿¡å·ï¼Œå¹¶ä¸”å‘é€æŠ¥å‘Šåˆ°é‚®ç®± 

```python
# -*- coding: utf-8 -*-
"""
 Livermoreçº³æ–¯è¾¾å…‹é‡åŒ–é¢„æµ‹ç³»ç»Ÿ v7.4
- ä¿è¯åŸºç¡€åŠŸèƒ½ï¼ˆæ•°æ®è·å–ã€é‚®ä»¶æ”¶å‘ã€æ—¥å¿—ã€å¼‚å¸¸å¤„ç†ï¼‰
- æœ€å¤§åŒ–æ‰©å±•å› å­ä½“ç³»ã€ä¿¡å·ç”Ÿæˆã€é£é™©æ§åˆ¶ã€æ¨¡å‹èåˆä¸ç²¾åº¦æå‡
- å…¼å®¹akshare, yfinanceç­‰ä¸»æµæ•°æ®/åˆ†æåº“
- [V7.4] æœ€ç»ˆå®¡æŸ¥ä¸ç²¾ä¿®ï¼šå‚æ•°ä¸­å¤®åŒ–ã€æ•°æ®èåˆé€»è¾‘ä¿®å¤ã€æƒ…æ™¯åˆ†æå®Œå¤‡æ€§ã€å†³ç­–é€æ˜åº¦æå‡
-æœ¬é¡¹ç›®ä½¿ç”¨äº† AKShare æ•°æ®æ¥å£: https://github.com/akfamily/akshare
"""

import pandas as pd
import numpy as np
import akshare as ak
import yfinance as yf
import time
import logging
from datetime import datetime, timedelta
import yagmail
import os
import warnings
from scipy import stats
from sklearn.preprocessing import RobustScaler, StandardScaler, QuantileTransformer
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error
import traceback
import json
from functools import lru_cache, wraps
from typing import Dict, List, Tuple, Optional, Any, Callable
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore")
os.environ['TZ'] = 'Asia/Shanghai'

# ===================================
# åŸºç¡€å‚æ•°é…ç½®åŒºï¼ˆå¯ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
# ===================================
EMAIL_USER = os.getenv('LIVERMORE_EMAIL_USER', 'your Emali')
EMAIL_PASS = os.getenv('LIVERMORE_EMAIL_PASS', 'your email password')
RECIPIENTS = [os.getenv('LIVERMORE_EMAIL_RCPT', 'your Emali')]

PARAMS = {
    # --- æ ¸å¿ƒäº¤æ˜“ä¸é£æ§å‚æ•° ---
    'BASE_BUY_THRESHOLD': 0.30,  # åŸºç¡€ä¹°å…¥é—¨æ§›
    'EXIT_THRESHOLD': -0.40,  # åŸºç¡€ç¦»åœºé—¨æ§›
    'TREND_ADJUST_FACTOR': 0.15,  # è¶‹åŠ¿å¯¹é—¨æ§›çš„è°ƒæ•´æ•æ„Ÿåº¦
    'LIVERMORE_DRAWDOWN_PERCENT': 0.07,  # åˆ©å¼—è«å°”åŠ¨æ€æ­¢æŸçš„å›æ’¤æ¯”ä¾‹
    'ATR_STOP_LOSS_MULTIPLIER': 1.5,  # åˆå§‹ATRæ­¢æŸçš„ä¹˜æ•°
    'POSITION_DISAGREEMENT_MIN_SCALE': 0.3,  # æ¨¡å‹åˆ†æ­§æ—¶å¤´å¯¸ç¼©æ”¾çš„æœ€å°æ¯”ä¾‹
    'POSITION_DISAGREEMENT_POWER': 1.5,  # æ¨¡å‹åˆ†æ­§å¯¹å¤´å¯¸ç¼©æ”¾çš„æŒ‡æ•°å¼ºåº¦

    # --- æƒ…æ™¯åˆ†æå‚æ•° ---
    'REGIME_BULL_THRESHOLD': 0.15,  # å®šä¹‰ç‰›å¸‚çš„ç»Ÿä¸€é£é™©æŒ‡æ•°ä¸‹é™
    'REGIME_BEAR_THRESHOLD': -0.15,  # å®šä¹‰ç†Šå¸‚çš„ç»Ÿä¸€é£é™©æŒ‡æ•°ä¸Šé™
    'REGIME_SCORE_WEIGHTS': {'trend': 0.5, 'macro': 0.3, 'volatility': 0.2},  # ç»Ÿä¸€é£é™©æŒ‡æ•°çš„æˆåˆ†æƒé‡
    'CLUSTER_CONF_MIN': 0.1,  # ç­–ç•¥ç°‡ç½®ä¿¡åº¦çš„ä¸‹é™
    'CLUSTER_CONF_MAX': 0.9,  # ç­–ç•¥ç°‡ç½®ä¿¡åº¦çš„ä¸Šé™

    # --- å› å­ä¸æ¨¡å‹å‚æ•° ---
    'MAX_IR_WEIGHT': 5.0,  # ç°‡å†…åŠ æƒæ—¶å•ä¸ªå› å­çš„æœ€å¤§IRæƒé‡
    'MIN_DATA_POINTS': 100,  # æ•°æ®æ¸…æ´—åè¦æ±‚çš„æœ€å°‘æ•°æ®ç‚¹
    'VOLATILITY_WINDOW': 20,  # æ³¢åŠ¨ç‡å› å­çš„è®¡ç®—çª—å£
    'CACHE_EXPIRE_MINUTES': 5,  # æ•°æ®ç¼“å­˜çš„æœ‰æ•ˆåˆ†é’Ÿæ•°
    'MAX_THREADS': 6,  # å¹¶è¡Œè®¡ç®—çš„æœ€å¤§çº¿ç¨‹æ•°

    # --- (ä»¥ä¸‹ä¸ºåŸå‚æ•°ï¼Œä¿æŒä¸å˜) ---
    'BREAKOUT_DAYS': 55, 'CONFIRMATION_DAYS': 3, 'VOLUME_THRESHOLD': 1.3, 'RSI_PERIOD': 14,
    'MARKET_SMA': 200, 'LOOKBACK_PERIOD': 252, 'IC_THRESHOLD': 0.025, 'MOMENTUM_WINDOW': 10,
    'REVERSION_WINDOW': 5, 'SIGNAL_DECAY': 0.95, 'MAX_FACTOR_WEIGHT': 0.25, 'MIN_SAMPLES_FOR_CALC': 30,
    'OUTLIER_STD_THRESHOLD': 3.5, 'MODEL_WINDOW': 256, 'ML_N_ESTIMATORS': 90, 'ML_ALPHA': 0.3,
    'ML_MAXMODELS': 4, 'IC_CONF_SCALE': 5.0, 'IC_CONF_BASE': 0.05, 'ML_CONF_MIN': 0.05,
}

FACTOR_BASE_WEIGHTS = {
    'momentum': 0.13, 'volatility': 0.12, 'volume': 0.11,
    'price_pattern': 0.12, 'trend_strength': 0.10, 'mean_reversion': 0.08,
    'breakout': 0.12, 'macd': 0.07, 'bollinger': 0.07, 'vix_proxy': 0.08
}

# å°†æ‰€æœ‰å› å­æŒ‰å…¶ç»æµå­¦é€»è¾‘å½’ç±»ï¼Œå½¢æˆç­–ç•¥ç»„åˆçš„åŸºç¡€ã€‚
FACTOR_CLUSTERS = {
    'trend': [
        'momentum', 'trend_strength', 'breakout', 'macd',
        'kama_trend', 'slope_trend', 'trend_persist', 'livermore_pivotal',
        'calm_trend'
    ],
    'reversion': ['mean_reversion', 'price_pattern', 'bollinger'],
    'volatility': ['volatility', 'vix_proxy', 'lag_ac1'],
    'macro': [
        'pce', 'nfp', 'rtl', 'ism_non_mfg', 'job', 'epu',
        'cpi_yoy', 'ppi_yoy', 'ism_mfg', 'durable', 'confidence', 'unemployment'
    ]
}

trading_calendar_cache = None


# ===================================
# æ—¥å¿—ç³»ç»Ÿå¢å¼º
# ===================================
def setup_enhanced_logging():
    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()

    class ColoredFormatter(logging.Formatter):
        COLORS = {
            'DEBUG': '\033[36m',
            'INFO': '\033[32m',
            'WARNING': '\033[33m',
            'ERROR': '\033[31m',
            'CRITICAL': '\033[35m',
            'ENDC': '\033[0m'
        }

        def format(self, record):
            log_color = self.COLORS.get(record.levelname, self.COLORS['ENDC'])
            levelname_colored = f"{log_color}{record.levelname}{self.COLORS['ENDC']}"
            message = super().format(record)
            return message.replace(record.levelname, levelname_colored, 1)

    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level))
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    console_handler = logging.StreamHandler()
    console_formatter = ColoredFormatter(
        '%(asctime)s | %(levelname)-8s | %(filename)s:%(lineno)d | %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    try:
        if not os.path.exists('cache'):
            os.makedirs('cache')
        file_handler = logging.FileHandler('cache/livermore_enhanced.log', encoding='utf-8')
        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(funcName)s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        logger.warning(f"æ–‡ä»¶æ—¥å¿—é…ç½®å¤±è´¥: {e}")
    return logger


logger = setup_enhanced_logging()


# ===================================
# ç¼“å­˜ç³»ç»Ÿ
# ===================================
class EnhancedCache:
    def __init__(self, expire_minutes: int = 5):
        self._cache = {}
        self._timestamps = {}
        self._expire_seconds = expire_minutes * 60
        self._lock = threading.RLock()

    def get(self, key: str) -> Any:
        with self._lock:
            if key not in self._cache: return None
            if time.time() - self._timestamps[key] > self._expire_seconds:
                del self._cache[key];
                del self._timestamps[key];
                return None
            return self._cache[key]

    def set(self, key: str, value: Any) -> None:
        with self._lock:
            self._cache[key] = value
            self._timestamps[key] = time.time()

    def clear(self) -> None:
        with self._lock:
            self._cache.clear();
            self._timestamps.clear()


cache_manager = EnhancedCache(PARAMS['CACHE_EXPIRE_MINUTES'])

# === æŒä»“çŠ¶æ€æŒä¹…åŒ–å·¥å…· ===
STATE_FILE = os.path.join("cache", "position_state.json")


def load_position_state() -> Dict[str, Any]:
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.warning(f"è¯»å–æŒä»“çŠ¶æ€å¤±è´¥: {e}")
    return {'status': 'flat', 'entry_price': None}


def save_position_state(state: Dict[str, Any]) -> None:
    try:
        os.makedirs("cache", exist_ok=True)
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"å†™å…¥æŒä»“çŠ¶æ€å¤±è´¥: {e}")


def retry_on_exception(max_retries: int = 3, delay: float = 1.0, backoff: float = 2.0):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt == max_retries - 1: break
                    logger.warning(f"{func.__name__} ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}, {current_delay:.1f}ç§’åé‡è¯•")
                    time.sleep(current_delay);
                    current_delay *= backoff
            logger.error(f"{func.__name__} æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {last_exception}")
            raise last_exception

        return wrapper

    return decorator


def performance_monitor(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            logger.debug(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"{func.__name__} æ‰§è¡Œå¤±è´¥ ({execution_time:.3f}ç§’): {e}")
            raise

    return wrapper


# ===================================
# æ•°æ®è·å–ï¼ˆå¤šæºèåˆå¢å¼ºï¼‰
# ===================================
@performance_monitor
@retry_on_exception(max_retries=3, delay=2.0)
def fetch_nasdaq_data_eastmoney() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("ğŸ”„ ä¸œæ–¹è´¢å¯Œè·å–æ•°æ®...")
        cache_key = f"nasdaq_em_{datetime.now().strftime('%Y%m%d_%H%M')}"
        cached_data = cache_manager.get(cache_key)
        if cached_data: logger.info("âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®"); return cached_data
        spot_df = ak.index_global_spot_em()
        nasdaq_row = spot_df[spot_df["åç§°"].str.contains("çº³æ–¯è¾¾å…‹", na=False)]
        if nasdaq_row.empty: raise ValueError("æœªæ‰¾åˆ°çº³æ–¯è¾¾å…‹æŒ‡æ•°")
        symbol = nasdaq_row.iloc[0]["åç§°"]
        hist_df = ak.index_global_hist_em(symbol=symbol)
        column_mapping = {
            "æ—¥æœŸ": "Date", "ä»Šå¼€": "Open", "æœ€é«˜": "High",
            "æœ€ä½": "Low", "æœ€æ–°ä»·": "Close", "æ”¶ç›˜": "Close",
            "æˆäº¤é‡": "Volume"
        }
        hist_df.rename(columns=column_mapping, inplace=True)
        hist_df['Date'] = pd.to_datetime(hist_df['Date'])
        hist_df.set_index('Date', inplace=True)
        hist_df.sort_index(inplace=True)
        current_price = float(nasdaq_row.iloc[0]["æœ€æ–°ä»·"])

        hist_df = quality_check_and_clean(hist_df)
        hist_df = align_to_trading_calendar(hist_df)
        # (å®‰å…¨ä¿®å¤) åªé€‰æ‹©æ•°æ®å¸§ä¸­å®é™…å­˜åœ¨çš„åˆ—
        final_cols = [col for col in ['Open', 'High', 'Low', 'Close', 'Volume'] if col in hist_df.columns]
        result = (hist_df[final_cols], current_price, "ä¸œæ–¹è´¢å¯Œ")
        cache_manager.set(cache_key, result)
        logger.info(f"âœ… ä¸œæ–¹è´¢å¯Œæ•°æ®è·å–: {len(hist_df)}æ¡, å½“å‰ä»·: {current_price:.2f}")
        return result
    except Exception as e:
        logger.warning(f"âŒ ä¸œæ–¹è´¢å¯Œè·å–å¤±è´¥: {e}")
        raise


@performance_monitor
@retry_on_exception(max_retries=2, delay=2.0)
def fetch_nasdaq_data_sina() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("ğŸ”„ æ–°æµªè´¢ç»è·å–æ•°æ®...")
        cache_key = f"nasdaq_sina_{datetime.now().strftime('%Y%m%d_%H%M')}"
        cached_data = cache_manager.get(cache_key)
        if cached_data: logger.info("âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®"); return cached_data

        df = ak.index_us_stock_sina(".IXIC")
        if df.empty or len(df) < PARAMS['MIN_DATA_POINTS']:
            raise ValueError("æ–°æµªè´¢ç»æ•°æ®ä¸è¶³")

        current_price = float(df['close'].iloc[-1])
        df = standardize_dataframe_columns(df)
        df = quality_check_and_clean(df)

        # ã€å…³é”®ä¿®å¤ã€‘: é‡æ–°å¯ç”¨äº¤æ˜“æ—¥å†å¯¹é½ï¼Œä¿è¯æ•°æ®å¤„ç†ä¸€è‡´æ€§
        df = align_to_trading_calendar(df)

        result = (df, current_price, "æ–°æµªè´¢ç»")
        cache_manager.set(cache_key, result)
        logger.info(f"âœ… æ–°æµªè´¢ç»æ•°æ®è·å–ä¸å¯¹é½å®Œæˆ: {len(df)}æ¡, å½“å‰ä»·: {current_price:.2f}")
        return result
    except Exception as e:
        logger.warning(f"âŒ æ–°æµªè´¢ç»è·å–å¤±è´¥: {e}")
        raise


@performance_monitor
@retry_on_exception(max_retries=2, delay=2.0)
def fetch_nasdaq_data_yahoo() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("ğŸ”„ Yahoo Financeè·å–æ•°æ®...")
        cache_key = f"nasdaq_yahoo_{datetime.now().strftime('%Y%m%d_%H%M')}"
        cached_data = cache_manager.get(cache_key)
        if cached_data: logger.info("âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®"); return cached_data
        ticker = yf.Ticker("^IXIC")
        end_date = datetime.now()
        start_date = end_date - timedelta(days=400)
        df = ticker.history(start=start_date, end=end_date, interval="1d")
        if df.empty or len(df) < PARAMS['MIN_DATA_POINTS']:
            raise ValueError("Yahoo Financeæ•°æ®ä¸è¶³")
        df = df.reset_index()
        df = standardize_dataframe_columns(df)
        df = quality_check_and_clean(df)
        df = align_to_trading_calendar(df)
        current_price = float(df['Close'].iloc[-1])
        result = (df, current_price, "Yahoo Finance")
        cache_manager.set(cache_key, result)
        logger.info(f"âœ… Yahoo Financeæ•°æ®è·å–: {len(df)}æ¡, å½“å‰ä»·: {current_price:.2f}")
        return result
    except Exception as e:
        logger.warning(f"âŒ Yahoo Financeè·å–å¤±è´¥: {e}")
        raise


@performance_monitor
@retry_on_exception(max_retries=2, delay=2.0)
def fetch_nasdaq_data_investing() -> Tuple[pd.DataFrame, float, str]:
    try:
        logger.info("ğŸ”„ investing.comè·å–æ•°æ®ï¼ˆå¤‡ç”¨ï¼‰...")
        # å¤ç”¨æ–°æµªè´¢ç»
        return fetch_nasdaq_data_sina()
    except Exception as e:
        logger.warning(f"âŒ investing.comè·å–å¤±è´¥: {e}")
        raise


def align_to_trading_calendar(df: pd.DataFrame) -> pd.DataFrame:
    """
    ã€V7.4ã€‘: æœ¬å‡½æ•°ä¸å†è¿›è¡Œä»»ä½•ç½‘ç»œè¯·æ±‚ã€‚
    å®ƒåªä»æœ¬åœ°ç¼“å­˜æ–‡ä»¶ 'cache/trading_calendar_cache.pkl' åŠ è½½äº¤æ˜“æ—¥å†ã€‚
    è¿™ä¸ªç¼“å­˜æ–‡ä»¶ç”±ä¸»æ•°æ®è·å–å‡½æ•° fetch_nasdaq_data() è´Ÿè´£åˆ›å»ºã€‚
    """
    global trading_calendar_cache

    cal_idx = None
    cache_file = os.path.join("cache", "trading_calendar_cache.pkl")

    if 'trading_calendar_cache' in globals() and trading_calendar_cache is not None:
        cal_idx = trading_calendar_cache
    elif os.path.exists(cache_file):
        try:
            cal_idx = pd.read_pickle(cache_file)
            trading_calendar_cache = cal_idx
        except Exception as e:
            logger.error(f"âŒ è¯»å–äº¤æ˜“æ—¥å†ç¼“å­˜å¤±è´¥: {e}")
            return df

    if cal_idx is None:
        logger.warning("äº¤æ˜“æ—¥å†ç¼“å­˜å°šæœªåˆ›å»ºï¼Œæš‚æ—¶ä¸å¯¹é½æ•°æ®ã€‚")
        return df

    df_aligned = df.copy()
    if 'Date' in df_aligned.columns:
        df_aligned['Date'] = pd.to_datetime(df_aligned['Date'], errors='coerce')
        df_aligned = df_aligned.dropna(subset=['Date']).set_index('Date')

    if not isinstance(df_aligned.index, pd.DatetimeIndex):
        logger.warning("DataFrame ç´¢å¼•ä¸æ˜¯æ—¥æœŸç±»å‹ï¼Œæ— æ³•å¯¹é½ï¼Œè¿”å›åŸå§‹dfã€‚")
        return df

    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        if col not in df_aligned.columns:
            df_aligned[col] = 0.0

    df_aligned = df_aligned[['Open', 'High', 'Low', 'Close', 'Volume']].reindex(cal_idx)
    df_aligned[['Open', 'High', 'Low', 'Close']] = df_aligned[['Open', 'High', 'Low', 'Close']].fillna(method='ffill')
    df_aligned['Volume'] = df_aligned['Volume'].fillna(0)
    df_aligned = df_aligned.dropna(subset=['Close'])

    logger.debug(f"æ•°æ®å·²æˆåŠŸå¯¹é½åˆ°è‡ªå»ºçš„äº¤æ˜“æ—¥å†ï¼Œæ•°æ®ç‚¹: {len(df_aligned)}ã€‚")
    return df_aligned


def reconcile_data(primary_res: Tuple, backup_res: Tuple, tol: float = 0.005) -> Tuple:
    """
    ã€V7.4 æ–°å¢ã€‘å¯¹ä¸¤ä¸ªæˆåŠŸè·å–çš„æ•°æ®æºç»“æœè¿›è¡Œäº¤å‰éªŒè¯å’Œä¿®æ­£ã€‚
    """
    df_p, price_p, src_p = primary_res
    df_b, price_b, src_b = backup_res

    try:
        # 1. å¯¹æ¯”å®æ—¶ä»·æ ¼
        price_diff = abs(price_p - price_b) / max(price_p, 1e-6)
        if price_diff > tol:
            logger.warning(
                f"å¤šæºå®æ—¶ä»·æ ¼å·®å¼‚è¾ƒå¤§: {src_p} ({price_p:.2f}) vs {src_b} ({price_b:.2f}) - å·®å¼‚ {price_diff:.2%}")

        # 2. å¯¹æ¯”å†å²æ”¶ç›˜ä»·
        if not isinstance(df_p.index, pd.DatetimeIndex) and 'Date' in df_p.columns:
            df_p = df_p.set_index('Date')
        if not isinstance(df_b.index, pd.DatetimeIndex) and 'Date' in df_b.columns:
            df_b = df_b.set_index('Date')

        idx = df_p.index.intersection(df_b.index)
        if not idx.empty:
            p = df_p.loc[idx, 'Close'].astype(float)
            b = df_b.loc[idx, 'Close'].astype(float)
            diff = (p - b).abs() / p.clip(1e-8)
            bad_days = diff > tol
            if bad_days.any():
                logger.warning(f"ä¸»å¤‡å†å²Closeå·®å¼‚è¶…é˜ˆ {bad_days.sum()} å¤©ï¼ŒæŒ‰å¤‡æº({src_b})ä¿®æ­£ã€‚")
                df_p.loc[idx[bad_days], 'Close'] = b[bad_days]
                df_p.loc[idx[bad_days], 'High'] = np.maximum(df_p.loc[idx[bad_days], 'High'],
                                                             df_p.loc[idx[bad_days], 'Close'])
                df_p.loc[idx[bad_days], 'Low'] = np.minimum(df_p.loc[idx[bad_days], 'Low'],
                                                            df_p.loc[idx[bad_days], 'Close'])

    except Exception as e:
        logger.warning(f"æ•°æ®æºäº¤å‰éªŒè¯å¤±è´¥: {e}")

    return (df_p, price_p, f"{src_p} (å·²ä¸{src_b}äº¤å‰éªŒè¯)")


def fetch_nasdaq_data() -> Tuple[pd.DataFrame, float, str]:
    """ã€V7.4 ã€‘è·å–çº³æ–¯è¾¾å…‹æ•°æ®ï¼Œå¹¶ä¿®å¤äº†å¤šæºèåˆé€»è¾‘"""
    data_sources = [fetch_nasdaq_data_eastmoney, fetch_nasdaq_data_sina]
    results = []
    last_error = None

    for i, func in enumerate(data_sources, 1):
        try:
            logger.info(f"ğŸ“Š å°è¯•æ•°æ®æº {i}/{len(data_sources)}: {func.__name__}")
            df, price, src = func()
            if not df.empty:
                results.append((df, price, src))
        except Exception as e:
            last_error = e
            logger.warning(f"æ•°æ®æº {func.__name__} å¤±è´¥: {e}ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")

    if not results:
        logger.error(f"ğŸ’¥ æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}")
        return pd.DataFrame(), 0.0, "æ— æ•°æ®æº"

    final_result = results[0]
    if len(results) > 1:
        logger.info(f"æˆåŠŸè·å– {len(results)} ä¸ªæ•°æ®æºï¼Œå¼€å§‹äº¤å‰éªŒè¯...")
        final_result = reconcile_data(results[0], results[1])

    # --- åˆ›å»º/åˆ·æ–°äº¤æ˜“æ—¥å†ç¼“å­˜ï¼ˆåŸºäºæœ€ç»ˆç¡®å®šçš„æ•°æ®æºï¼‰---
    try:
        df_final, price_final, src_final = final_result
        if not isinstance(df_final.index, pd.DatetimeIndex):
            df_final = df_final.copy()
            if 'Date' in df_final.columns:
                df_final['Date'] = pd.to_datetime(df_final['Date'], errors='coerce')
                df_final = df_final.dropna(subset=['Date']).set_index('Date')

        cal_idx = pd.DatetimeIndex(df_final.index).normalize().drop_duplicates().sort_values()
        global trading_calendar_cache
        trading_calendar_cache = cal_idx
        os.makedirs("cache", exist_ok=True)
        pd.to_pickle(cal_idx, os.path.join("cache", "trading_calendar_cache.pkl"))
        logger.info(f"âœ… äº¤æ˜“æ—¥å†ç¼“å­˜å·²åˆ›å»º/æ›´æ–°ï¼Œé•¿åº¦: {len(cal_idx)}")

        return df_final, price_final, src_final

    except Exception as e:
        logger.warning(f"äº¤æ˜“æ—¥å†ç¼“å­˜åˆ›å»ºå¤±è´¥: {e}ï¼Œç›´æ¥è¿”å›åŸç»“æœã€‚")
        return final_result


def standardize_dataframe_columns(df: pd.DataFrame) -> pd.DataFrame:
    try:
        column_mapping = {
            'date': 'Date', 'Date': 'Date',
            'open': 'Open', 'Open': 'Open', 'opening': 'Open',
            'high': 'High', 'High': 'High', 'highest': 'High',
            'low': 'Low', 'Low': 'Low', 'lowest': 'Low',
            'close': 'Close', 'Close': 'Close', 'closing': 'Close', 'adj close': 'Close', 'Adj Close': 'Close',
            'adj_close': 'Close',
            'volume': 'Volume', 'Volume': 'Volume', 'vol': 'Volume'
        }
        df = df.copy()
        df.rename(columns=column_mapping, inplace=True)

        if 'Date' not in df.columns and (df.index.name in ['Date', 'date']):
            df = df.reset_index()

        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

        def clean_numeric(s):
            if s.dtype == 'O':
                s = s.astype(str).str.replace(',', '', regex=False).str.replace('%', '', regex=False)
            s = pd.to_numeric(s, errors='coerce')
            return s

        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
            if col in df.columns:
                df[col] = clean_numeric(df[col])

        if 'Volume' not in df.columns:
            df['Volume'] = np.nan

        if 'Date' in df.columns:
            df = df.dropna(subset=['Date']).sort_values('Date')
            df = df[~df['Date'].duplicated(keep='last')].reset_index(drop=True)

        logger.debug(f"åˆ—åæ ‡å‡†åŒ–å®Œæˆï¼Œæœ€ç»ˆåˆ—: {list(df.columns)}")
        return df
    except Exception as e:
        logger.error(f"åˆ—åæ ‡å‡†åŒ–å¤±è´¥: {e}")
        return df


@performance_monitor
def quality_check_and_clean(df: pd.DataFrame) -> pd.DataFrame:
    try:
        if df.empty:
            return df

        logger.debug(f"å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥ï¼ŒåŸå§‹æ•°æ®: {len(df)} æ¡")
        required_columns = ['Open', 'High', 'Low', 'Close']
        for col in required_columns:
            if col not in df.columns:
                logger.error(f"ç¼ºå¤±å¿…è¦åˆ—: {col}")
                return pd.DataFrame()

        df = df.copy()
        keep_cols = [c for c in ['Date', 'Open', 'High', 'Low', 'Close', 'Volume'] if c in df.columns]
        df = df[keep_cols]

        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df = df.dropna(subset=['Date']).set_index('Date')
        df = df.sort_index()
        if df.index.has_duplicates:
            df = df[~df.index.duplicated(keep='last')]

        for col in ['Open', 'High', 'Low', 'Close']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=['Open', 'High', 'Low', 'Close'])
        df = df[(df[['Open', 'High', 'Low', 'Close']] > 0).all(axis=1)]

        df['High'] = df[['High', 'Open', 'Close']].max(axis=1)
        df['Low'] = df[['Low', 'Open', 'Close']].min(axis=1)

        if 'Volume' in df.columns:
            df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce').fillna(0)
            df['Volume'] = df['Volume'].clip(lower=0)

        if len(df) < PARAMS['MIN_DATA_POINTS']:
            logger.error(f"æ¸…æ´—åæ•°æ®ä¸è¶³: {len(df)} < {PARAMS['MIN_DATA_POINTS']}")
            return pd.DataFrame()

        logger.debug(f"æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®: {len(df)} æ¡")
        return df
    except Exception as e:
        logger.error(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
        return pd.DataFrame()


# ===================================
# é‚®ä»¶é€šçŸ¥
# ===================================
def send_notification(subject: str, content: str, attachments: Optional[List[str]] = None):
    try:
        yag = yagmail.SMTP(user=EMAIL_USER, password=EMAIL_PASS, host='smtp.qq.com', port=465, smtp_ssl=True)
        if attachments is None: attachments = []
        yag.send(to=RECIPIENTS, subject=subject, contents=content, attachments=attachments)
        logger.info(f"ğŸ“§ é‚®ä»¶å‘é€æˆåŠŸ: {subject}")
    except Exception as e:
        logger.error(f"ğŸ“§ é‚®ä»¶å‘é€å¤±è´¥: {e}")


from functools import lru_cache


@lru_cache(maxsize=1)
def fetch_dxy_data() -> pd.Series:
    """
    è·å–å¹¶ç¼“å­˜ç¾å…ƒæŒ‡æ•°(DXY)å†å²æ•°æ®ã€‚
    """
    try:
        logger.info("ğŸ”„ (æ–°) å°è¯•è·å–ç¾å…ƒæŒ‡æ•°(DXY)å†å²æ•°æ®...")
        dxy_df = ak.index_global_hist_em(symbol="ç¾å…ƒæŒ‡æ•°")

        column_mapping = {"æ—¥æœŸ": "Date", "æœ€æ–°ä»·": "Close"}
        dxy_df.rename(columns=column_mapping, inplace=True)

        dxy_df['Date'] = pd.to_datetime(dxy_df['Date'])
        dxy_series = pd.Series(dxy_df['Close'].values, index=dxy_df['Date'].values)
        dxy_series = dxy_series[~dxy_series.index.duplicated(keep='last')].sort_index()

        if dxy_series.empty:
            raise ValueError("è·å–çš„ç¾å…ƒæŒ‡æ•°æ•°æ®ä¸ºç©º")

        logger.info(f"âœ… (æ–°) ç¾å…ƒæŒ‡æ•°æ•°æ®è·å–æˆåŠŸï¼Œå…± {len(dxy_series)} æ¡ã€‚")
        return dxy_series
    except Exception as e:
        logger.error(f"âŒ (æ–°) è·å–ç¾å…ƒæŒ‡æ•°æ•°æ®å¤±è´¥: {e}")
        return pd.Series(dtype=float)


# ===================================
# å¢å¼ºç‰ˆå› å­å¼•æ“ï¼ˆå¤šå› å­å¹¶è¡Œ+èåˆ+æœºå™¨å­¦ä¹ ï¼‰
# ===================================
class EnhancedFactorEngine:
    """
    æ ¸å¿ƒå¤šå› å­å¼•æ“ï¼Œæ‰©å±•å› å­æ—ã€æ”¯æŒMLèåˆ
    """

    def __init__(self, data: pd.DataFrame):
        logger.info(f"å› å­å¼•æ“åˆå§‹åŒ–ä¸­ï¼Œä¼ å…¥æ•°æ®ç‚¹æ•°: {len(data)}...")
        self.data = data.copy()
        self.factors = {}
        self.factor_weights = {}
        self.ic_history = {}
        self.factor_performance = {}
        self.scaler = RobustScaler()
        self._calculation_cache = {}
        self.base_weights = FACTOR_BASE_WEIGHTS.copy()

        logger.info("...æ­£åœ¨åˆå§‹åŒ–å¹¶å¯¹é½ç¾å…ƒæŒ‡æ•°(DXY)æ•°æ®...")
        dxy_raw = fetch_dxy_data()
        if not dxy_raw.empty:
            self.dxy_series = dxy_raw.reindex(self.data.index, method='ffill').bfill()
        else:
            self.dxy_series = pd.Series(0.0, index=self.data.index)
        logger.info("...ç¾å…ƒæŒ‡æ•°æ•°æ®åˆå§‹åŒ–å®Œæˆã€‚")

        logger.info(f"âœ… å› å­å¼•æ“åˆå§‹åŒ–å®Œæˆã€‚")

    def safe_calculate(self, func: Callable, default_value: Any = 0, factor_name: str = "") -> Any:
        try:
            cache_key = f"{factor_name}_{len(self.data)}_{hash(str(self.data.index[-1]) if len(self.data) > 0 else 'empty')}"
            if cache_key in self._calculation_cache:
                return self._calculation_cache[cache_key]
            result = func()
            if isinstance(result, (pd.Series, pd.DataFrame)):
                result = result.fillna(default_value)
                if isinstance(result, pd.Series) and len(result) > 0:
                    result = self._winsorize_series(result)
            elif isinstance(result, (np.ndarray, list)):
                result = np.array(result)
                result = np.nan_to_num(result, nan=default_value, posinf=default_value, neginf=default_value)
            elif pd.isna(result) or np.isinf(result):
                result = default_value
            self._calculation_cache[cache_key] = result
            return result
        except Exception as e:
            logger.warning(f"å› å­è®¡ç®—å¤±è´¥ ({factor_name}): {e}")
            if isinstance(default_value, (pd.Series, pd.DataFrame)):
                return default_value.fillna(0) if hasattr(default_value, 'fillna') else default_value
            return default_value

    def _winsorize_series(self, series: pd.Series, limits: Tuple[float, float] = (0.05, 0.05)) -> pd.Series:
        try:
            if len(series) < 10: return series
            lower_limit = series.quantile(limits[0])
            upper_limit = series.quantile(1 - limits[1])
            return series.clip(lower=lower_limit, upper=upper_limit)
        except:
            return series

    @performance_monitor
    def calculate_momentum_factor(self) -> pd.Series:
        def _calc():
            short_window, medium_window, long_window = 20, 60, 120
            if len(self.data) < long_window + 10: return pd.Series(0.0, index=self.data.index)
            close = self.data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).ffill().bfill()
            lr = np.log(close).diff()
            mom_short = lr.rolling(short_window, min_periods=3).sum()
            mom_medium = lr.rolling(medium_window, min_periods=10).sum()
            mom_long = lr.rolling(long_window, min_periods=20).sum()
            weighted = 0.5 * mom_short + 0.3 * mom_medium + 0.2 * mom_long
            lower, upper = weighted.quantile(0.01), weighted.quantile(0.99)
            weighted = weighted.clip(lower, upper)
            roll_w = max(60, int(min(len(weighted), 252)))
            med = weighted.rolling(roll_w, min_periods=10).median()
            mad = (weighted - med).abs().rolling(roll_w, min_periods=10).median()
            robust_z = (weighted - med) / (mad * 1.4826 + 1e-8)
            z_smoothed = robust_z.ewm(span=5, adjust=False).mean().fillna(0.0)
            return z_smoothed.clip(-4, 4).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index), factor_name="momentum")

    @performance_monitor
    def calculate_volatility_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < PARAMS['VOLATILITY_WINDOW'] + 5: return pd.Series([0] * len(self.data),
                                                                                  index=self.data.index)
            returns = self.data['Close'].pct_change()
            hist_vol = returns.rolling(PARAMS['VOLATILITY_WINDOW'], min_periods=5).std()
            log_ho, log_lo = np.log(self.data['High'] / self.data['Open']), np.log(self.data['Low'] / self.data['Open'])
            log_co, log_oc = np.log(self.data['Close'] / self.data['Open']), np.log(
                self.data['Open'] / self.data['Close'].shift(1))
            log_cc = np.log(self.data['Close'] / self.data['Close'].shift(1))
            yang_zhang = log_oc ** 2 + 0.5 * log_ho * log_co - 0.386 * log_cc ** 2 + log_lo * log_co
            yz_vol = yang_zhang.rolling(PARAMS['VOLATILITY_WINDOW'], min_periods=5).mean().apply(np.sqrt)
            combined_vol = 0.6 * hist_vol + 0.4 * yz_vol
            long_term_vol = combined_vol.rolling(120, min_periods=20).mean()
            relative_vol = combined_vol / (long_term_vol + 1e-8)
            return relative_vol - 1

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="volatility")

    @performance_monitor
    def calculate_volume_factor(self) -> pd.Series:
        def _calc():
            if 'Volume' not in self.data.columns:
                volume = pd.Series(0.0, index=self.data.index)
            else:
                volume = self.data['Volume'].fillna(0)
            if volume.nunique() <= 2: return pd.Series(0.0, index=self.data.index)
            if len(self.data) < 20: return pd.Series([0] * len(self.data), index=self.data.index)
            volume = self.data['Volume'].replace(0, 1)
            vol_ratio_5 = volume / volume.rolling(5, min_periods=2).mean()
            vol_ratio_20 = volume / volume.rolling(20, min_periods=5).mean()
            vol_ratio_60 = volume / volume.rolling(60, min_periods=10).mean()
            price_change = self.data['Close'].pct_change()
            volume_change = volume.pct_change()
            rolling_corr = price_change.rolling(20, min_periods=5).corr(volume_change)
            volume_factor = (0.4 * vol_ratio_5 + 0.3 * vol_ratio_20 + 0.2 * vol_ratio_60 + 0.1 * rolling_corr)
            return (volume_factor - volume_factor.rolling(60, min_periods=10).mean()) / (
                        volume_factor.rolling(60, min_periods=10).std() + 1e-8)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="volume")

    # ... (æ­¤å¤„çœç•¥å…¶ä»–å› å­è®¡ç®—å‡½æ•°ï¼Œå†…å®¹ä¿æŒä¸å˜ï¼Œä»¥èŠ‚çº¦ç¯‡å¹…) ...
    # calculate_price_pattern_factor, calculate_trend_strength_factor, etc.
    @performance_monitor
    def calculate_price_pattern_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 50:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            high = self.data['High']
            low = self.data['Low']
            open_ = self.data['Open']
            support_levels = []
            resistance_levels = []
            for i in range(20, len(close)):
                window_low = low.iloc[i - 20:i + 1]
                window_high = high.iloc[i - 20:i + 1]
                local_min_idx = window_low.idxmin()
                if local_min_idx == window_low.index[-1]:
                    support_levels.append(low.iloc[i])
                local_max_idx = window_high.idxmax()
                if local_max_idx == window_high.index[-1]:
                    resistance_levels.append(high.iloc[i])
            price_range = high.rolling(20, min_periods=5).max() - low.rolling(20, min_periods=5).min()
            price_position = (close - low.rolling(20, min_periods=5).min()) / (price_range + 1e-8)
            body = abs(close - open_)
            upper_shadow = high - np.maximum(close, open_)
            lower_shadow = np.minimum(close, open_) - low
            hammer = ((lower_shadow > 2 * body) & (upper_shadow < 0.3 * body)).astype(int)
            doji = (body < 0.1 * (high - low)).astype(int)
            pattern_factor = (
                    0.5 * price_position +
                    0.3 * hammer.rolling(3, min_periods=1).sum() +
                    0.2 * doji.rolling(3, min_periods=1).sum()
            )
            return (pattern_factor - 0.5) * 2

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="price_pattern")

    @performance_monitor
    def calculate_trend_strength_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 50:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            high = self.data['High']
            low = self.data['Low']
            tr1 = high - low
            tr2 = abs(high - close.shift(1))
            tr3 = abs(low - close.shift(1))
            tr = pd.DataFrame({'tr1': tr1, 'tr2': tr2, 'tr3': tr3}).max(axis=1)
            plus_dm = high.diff()
            minus_dm = -low.diff()
            plus_dm[(plus_dm < 0) | (plus_dm < minus_dm)] = 0
            minus_dm[(minus_dm < 0) | (minus_dm < plus_dm)] = 0
            atr = tr.rolling(14, min_periods=5).mean()
            plus_di = 100 * plus_dm.rolling(14, min_periods=5).mean() / atr
            minus_di = 100 * minus_dm.rolling(14, min_periods=5).mean() / atr
            dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-8)
            adx = dx.rolling(14, min_periods=5).mean()

            def rolling_trend(x):
                if len(x) < 10: return 0
                x_vals = np.arange(len(x))
                slope, _, r_value, _, _ = stats.linregress(x_vals, x)
                return abs(slope) * r_value ** 2

            trend_strength = close.rolling(20, min_periods=10).apply(rolling_trend)
            trend_strength = (trend_strength - trend_strength.rolling(60, min_periods=10).mean()) / \
                             (trend_strength.rolling(60, min_periods=10).std() + 1e-8)
            combined_strength = 0.6 * (adx / 100 - 0.5) * 2 + 0.4 * trend_strength
            return combined_strength.clip(-1, 1)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="trend_strength")

    @performance_monitor
    def calculate_mean_reversion_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 50:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            sma_20 = close.rolling(20, min_periods=10).mean()
            sma_50 = close.rolling(50, min_periods=20).mean()
            dev_20 = (close - sma_20) / sma_20
            dev_50 = (close - sma_50) / sma_50
            bb_upper = sma_20 + 2 * close.rolling(20, min_periods=10).std()
            bb_lower = sma_20 - 2 * close.rolling(20, min_periods=10).std()
            bb_position = (close - bb_lower) / (bb_upper - bb_lower + 1e-8)
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(14, min_periods=5).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14, min_periods=5).mean()
            rs = gain / (loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
            mean_reversion = (
                    0.4 * (dev_20 + dev_50).clip(-0.1, 0.1) * 10 +
                    0.3 * (bb_position - 0.5) * 2 +
                    0.3 * (rsi - 50) / 50
            )
            return mean_reversion.clip(-1, 1)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="mean_reversion")

    @performance_monitor
    def calculate_breakout_factor(self) -> pd.Series:
        def _calc():
            if len(self.data) < 60:
                return pd.Series([0] * len(self.data), index=self.data.index)
            close = self.data['Close']
            high = self.data['High']
            low = self.data['Low']
            period = 20
            upper_channel = high.rolling(period, min_periods=10).max()
            lower_channel = low.rolling(period, min_periods=10).min()
            channel_width = upper_channel - lower_channel
            breakout_strength = (close - lower_channel) / (channel_width + 1e-8) - 0.5
            if 'Volume' in self.data.columns and self.data['Volume'].nunique() > 1:
                volume = self.data['Volume']
                avg_volume = volume.rolling(20, min_periods=5).mean()
                volume_ratio = volume / (avg_volume + 1e-8)
                breakout_signal = ((close > upper_channel.shift(1)) | (close < lower_channel.shift(1))).astype(int)
                confirmed_breakout = breakout_signal * (volume_ratio > 1.2).astype(int)
                breakout_factor = (0.5 * breakout_strength * 2 + 0.5 * confirmed_breakout)
            else:
                logger.debug("calculate_breakout_factor: æœªæ‰¾åˆ°æœ‰æ•ˆçš„Volumeåˆ—ï¼Œä»…ä½¿ç”¨ä»·æ ¼ä¿¡å·ã€‚")
                breakout_factor = breakout_strength * 2
            return breakout_factor.clip(-1, 1)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="breakout")

    @performance_monitor
    def calculate_macd_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            ema_fast = close.ewm(span=12, min_periods=6).mean()
            ema_slow = close.ewm(span=26, min_periods=13).mean()
            macd_line = ema_fast - ema_slow
            macd_signal = macd_line.ewm(span=9, min_periods=5).mean()
            return macd_line - macd_signal

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="macd")

    @performance_monitor
    def calculate_bollinger_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            sma = close.rolling(20, min_periods=10).mean()
            std = close.rolling(20, min_periods=10).std()
            upper = sma + 2 * std
            lower = sma - 2 * std
            return (close - lower) / (upper - lower + 1e-8) - 0.5

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="bollinger")

    @performance_monitor
    def calculate_vix_proxy_factor(self) -> pd.Series:
        def _calc():
            returns = self.data['Close'].pct_change()
            vol_20d = returns.rolling(20, min_periods=10).std()
            vol_60d = returns.rolling(60, min_periods=30).std()
            return vol_20d / (vol_60d + 1e-8)

        return self.safe_calculate(_calc, default_value=pd.Series([0] * len(self.data), index=self.data.index),
                                   factor_name="vix_proxy")

    @performance_monitor
    def calculate_kama_trend_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            n_er, n_fast, n_slow = 10, 2, 30
            if len(close) < max(2 * n_slow, 60): return pd.Series(0.0, index=self.data.index)
            change = close.diff(n_er).abs()
            vol = close.diff().abs().rolling(n_er).sum()
            er = (change / (vol + 1e-8)).clip(0, 1)
            fast_sc, slow_sc = 2 / (n_fast + 1), 2 / (n_slow + 1)
            sc = (er * (fast_sc - slow_sc) + slow_sc) ** 2
            kama = pd.Series(index=close.index, dtype=float)
            kama.iloc[n_er] = close.iloc[:n_er + 1].mean()
            for i in range(n_er + 1, len(close)):
                kama.iloc[i] = kama.iloc[i - 1] + sc.iloc[i] * (close.iloc[i] - kama.iloc[i - 1])
            slope = kama.diff(5) / (close.rolling(20).std() + 1e-8)
            slope = slope.fillna(0.0)
            z = (slope - slope.rolling(60, min_periods=20).mean()) / (slope.rolling(60, min_periods=20).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index), factor_name="kama_trend")

    @performance_monitor
    def calculate_slope_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            window = 50
            if len(close) < window + 10: return pd.Series(0.0, index=self.data.index)
            idx = np.arange(window)

            def _slope(x):
                y = np.array(x)
                slope, _, r, _, _ = stats.linregress(idx, y)
                return (slope / (np.mean(y) + 1e-8)) * (r ** 2)

            s = close.rolling(window, min_periods=window).apply(_slope, raw=False)
            z = (s - s.rolling(120, min_periods=30).mean()) / (s.rolling(120, min_periods=30).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="slope_trend")

    @performance_monitor
    def calculate_trend_persistence_factor(self) -> pd.Series:
        def _calc():
            close = self.data['Close']
            ret = close.pct_change().fillna(0.0)
            window = 20
            if len(close) < window + 5: return pd.Series(0.0, index=self.data.index)
            up_ratio = (ret > 0).rolling(window, min_periods=int(window * 0.6)).mean() - 0.5
            z = (up_ratio - up_ratio.rolling(120, min_periods=30).mean()) / (
                        up_ratio.rolling(120, min_periods=30).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="trend_persist")

    @performance_monitor
    def calculate_lag_autocorr_factor(self) -> pd.Series:
        def _calc():
            ret = self.data['Close'].pct_change().fillna(0.0)
            window = 40
            if len(ret) < window + 5: return pd.Series(0.0, index=self.data.index)
            r0, r1 = ret, ret.shift(1)
            roll_cov = (r0 * r1).rolling(window, min_periods=int(window * 0.6)).mean() - r0.rolling(
                window).mean() * r1.rolling(window).mean()
            roll_var = r0.rolling(window, min_periods=int(window * 0.6)).var()
            ac1 = (roll_cov / (roll_var + 1e-8)).clip(-1, 1)
            z = (ac1 - ac1.rolling(120, min_periods=30).mean()) / (ac1.rolling(120, min_periods=30).std() + 1e-8)
            return z.clip(-3, 3).fillna(0.0)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index), factor_name="lag_ac1")

    @performance_monitor
    def calculate_dxy_momentum_factor(self) -> pd.Series:
        def _calc():
            if self.dxy_series.nunique() < 20: return pd.Series(0.0, index=self.data.index)
            log_returns = np.log(self.dxy_series).diff()
            momentum = log_returns.rolling(20, min_periods=15).sum().fillna(0.0)
            rolling_mean = momentum.rolling(120, min_periods=60).mean()
            rolling_std = momentum.rolling(120, min_periods=60).std()
            z_score = ((momentum - rolling_mean) / (rolling_std + 1e-8)).fillna(0.0)
            return z_score.clip(-3.5, 3.5)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="dxy_momentum")

    @performance_monitor
    def calculate_calm_trend_factor(self) -> pd.Series:
        """
        ã€æ–°å¢äº¤äº’ç‰¹å¾ã€‘: å†·é™è¶‹åŠ¿å› å­ (Calm Trend Factor)
        é€»è¾‘: è¶‹åŠ¿å¼ºåº¦ / (æ³¢åŠ¨ç‡ä»£ç† + Îµ)ï¼Œå¯»æ‰¾åœ¨å¸‚åœºå¹³ç¨³ï¼ˆéææ…Œï¼‰çŠ¶æ€ä¸‹çš„å¼ºåŠ²è¶‹åŠ¿ã€‚
        è¿™æ˜¯å¯¹è¶‹åŠ¿è´¨é‡çš„æ›´é«˜é˜¶åˆ¤æ–­ã€‚
        """

        def _calc():
            # å®‰å…¨åœ°è·å–ä¾èµ–çš„å› å­ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ä¸­æ€§å€¼
            trend_strength = self.factors.get('trend_strength', pd.Series(0.0, index=self.data.index))
            vix_proxy = self.factors.get('vix_proxy', pd.Series(1.0, index=self.data.index))  # æ³¢åŠ¨ç‡ä»£ç†ï¼Œé»˜è®¤ä¸º1é¿å…é™¤é›¶

            # æ ¸å¿ƒé€»è¾‘ï¼šè¶‹åŠ¿å¼ºåº¦ / (VIXä»£ç† + ä¸€ä¸ªå°å¸¸æ•°ä»¥é˜²é™¤é›¶)
            # VIXä»£ç†å€¼è¶Šå¤§ï¼ˆå¸‚åœºè¶Šææ…Œï¼‰ï¼Œå†·é™è¶‹åŠ¿å› å­çš„å€¼å°±è¶Šè¢«å‹åˆ¶
            calm_trend = trend_strength / (vix_proxy + 1e-8)

            # å¯¹ç»“æœè¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶ä¸å…¶ä»–å› å­é‡çº²å¯æ¯”
            rolling_mean = calm_trend.rolling(120, min_periods=60).mean()
            rolling_std = calm_trend.rolling(120, min_periods=60).std()
            z_score = ((calm_trend - rolling_mean) / (rolling_std + 1e-8)).fillna(0.0)

            return z_score.clip(-3.5, 3.5)

        return self.safe_calculate(_calc, default_value=pd.Series(0.0, index=self.data.index),
                                   factor_name="calm_trend")


    @performance_monitor
    def calculate_livermore_pivotal_factor(self) -> pd.Series:
        def _calc():
            BREAKOUT_WINDOW, VOLUME_SPIKE_RATIO, TREND_FILTER_WINDOW = 55, 1.5, 50
            if len(self.data) < BREAKOUT_WINDOW + 10: return pd.Series(0, index=self.data.index)
            close, high, volume = self.data['Close'], self.data['High'], self.data['Volume']
            is_in_uptrend = close > close.rolling(TREND_FILTER_WINDOW, min_periods=20).mean()
            highest_in_window = high.shift(1).rolling(BREAKOUT_WINDOW, min_periods=30).max()
            is_breakout = close > highest_in_window
            avg_volume = volume.shift(1).rolling(20, min_periods=10).mean()
            is_volume_confirmed = volume > (avg_volume * VOLUME_SPIKE_RATIO)
            pivotal_signal = (is_in_uptrend & is_breakout & is_volume_confirmed).astype(int)
            return pivotal_signal

        return self.safe_calculate(_calc, default_value=pd.Series(0, index=self.data.index),
                                   factor_name="livermore_pivotal")

    def calculate_all_factors(self) -> Dict[str, pd.Series]:
        logger.info("ğŸ”„ å¼€å§‹è®¡ç®—æ‰€æœ‰å› å­...")
        factor_functions = {
            'momentum': self.calculate_momentum_factor, 'volatility': self.calculate_volatility_factor,
            'volume': self.calculate_volume_factor, 'price_pattern': self.calculate_price_pattern_factor,
            'trend_strength': self.calculate_trend_strength_factor,
            'mean_reversion': self.calculate_mean_reversion_factor,
            'breakout': self.calculate_breakout_factor, 'macd': self.calculate_macd_factor,
            'bollinger': self.calculate_bollinger_factor, 'vix_proxy': self.calculate_vix_proxy_factor,
            'kama_trend': self.calculate_kama_trend_factor, 'slope_trend': self.calculate_slope_factor,
            'trend_persist': self.calculate_trend_persistence_factor, 'lag_ac1': self.calculate_lag_autocorr_factor,
            'livermore_pivotal': self.calculate_livermore_pivotal_factor,
            'dxy_momentum': self.calculate_dxy_momentum_factor,
            'calm_trend': self.calculate_calm_trend_factor
        }

        with ThreadPoolExecutor(max_workers=PARAMS['MAX_THREADS']) as executor:
            future_to_factor = {executor.submit(func): name for name, func in factor_functions.items()}
            for future in as_completed(future_to_factor):
                factor_name = future_to_factor[future]
                try:
                    self.factors[factor_name] = future.result()
                    logger.debug(f"âœ… å› å­è®¡ç®—å®Œæˆ: {factor_name}")
                except Exception as e:
                    logger.error(f"âŒ å› å­è®¡ç®—å¤±è´¥ ({factor_name}): {e}")
                    self.factors[factor_name] = pd.Series([0] * len(self.data), index=self.data.index)

        logger.info(f"âœ… æ‰€æœ‰æŠ€æœ¯å› å­è®¡ç®—å®Œæˆï¼Œå…± {len(self.factors)} ä¸ªã€‚å¼€å§‹é›†æˆå®è§‚å› å­...")
        if not getattr(self, '_macro_loaded', False):
            from macro_engine_strict import StrictMacro
            self.factors.update(StrictMacro(self.data.index).factors())
            self._macro_loaded = True

        return self.factors

    @performance_monitor
    def factor_preprocessing(self):
        """ã€V7.4ã€‘å¼•å…¥QuantileTransformerå®ç°â€œå› å­æ°‘ä¸»â€ï¼Œå¹¶å¢å¼ºå®è§‚å› å­ã€‚"""
        logger.info("ğŸ”„ (V7.4) å¼€å§‹ç»Ÿä¸€å› å­é¢„å¤„ç†...")
        INVERTED_FACTORS = ['volatility', 'pce', 'job', 'vix_proxy', 'epu', 'cpi_yoy', 'ppi_yoy', 'unemployment',
                            'dxy_momentum']

        # ã€ã€ã€æ ¸å¿ƒå‡çº§ 1ï¼šå®šä¹‰éœ€è¦ç‰¹æ®Šå¤„ç†çš„â€œä¸ç¨³å®šâ€å› å­ã€‘ã€‘ã€‘
        UNSTABLE_FACTORS = ['macd']  # æœªæ¥å¯åŠ å…¥å…¶ä»–å€¼åŸŸæ— ç•Œçš„å› å­

        base_index = self.data.index

        def mad_winsorize(s: pd.Series, k: float = 5.0) -> pd.Series:
            if s.dropna().size < 20: return s.fillna(0.0)
            med = s.median()
            mad = (s - med).abs().median() + 1e-8
            return s.clip(med - k * 1.4826 * mad, med + k * 1.4826 * mad)

        # 1. åˆå§‹æ•°æ®å¯¹é½ä¸å»æå€¼
        df_factors = {
            name: mad_winsorize(ser.reindex(base_index).ffill().bfill().fillna(0.0))
            for name, ser in self.factors.items() if isinstance(ser, pd.Series) and not ser.empty
        }
        factor_df = pd.DataFrame(df_factors).fillna(0.0)

        # 2. ã€ã€ã€æ ¸å¿ƒå‡çº§ 2ï¼šå¯¹ç¨³å®šä¸ä¸ç¨³å®šå› å­è¿›è¡Œåˆ†ç±»å¤„ç†ã€‘ã€‘ã€‘
        stable_cols = [c for c in factor_df.columns if c not in UNSTABLE_FACTORS]
        unstable_cols = [c for c in factor_df.columns if c in UNSTABLE_FACTORS]

        factor_scaled_stable = pd.DataFrame(index=factor_df.index)
        if stable_cols:
            scaler = RobustScaler()
            stable_scaled = scaler.fit_transform(factor_df[stable_cols])
            factor_scaled_stable = pd.DataFrame(stable_scaled, index=factor_df.index, columns=stable_cols)

        factor_scaled_unstable = pd.DataFrame(index=factor_df.index)
        if unstable_cols:
            # ä½¿ç”¨åˆ†ä½æ•°å˜æ¢ï¼Œå¼ºåˆ¶å°†ä¸ç¨³å®šå› å­çš„åˆ†å¸ƒæ­£æ€åŒ–ï¼Œæ ¹é™¤æç«¯å€¼å½±å“
            qt = QuantileTransformer(output_distribution='normal', random_state=42)
            unstable_scaled = qt.fit_transform(factor_df[unstable_cols])
            factor_scaled_unstable = pd.DataFrame(unstable_scaled, index=factor_df.index, columns=unstable_cols)

        # åˆå¹¶å¤„ç†åçš„å› å­
        factor_scaled = pd.concat([factor_scaled_stable, factor_scaled_unstable], axis=1)

        # 3. æ–¹å‘åè½¬ä¸æ—¶é—´å¹³æ»‘ (ä¸ä¹‹å‰ç›¸åŒ)
        for name in factor_scaled.columns:
            if name in INVERTED_FACTORS:
                factor_scaled[name] = -factor_scaled[name]
        factor_scaled = factor_scaled.ewm(span=5, adjust=False).mean()
        self.factors = {c: factor_scaled[c] for c in factor_scaled.columns}

        # 4. å®è§‚å› å­å‘å¸ƒæ»åå¤„ç† (ä¸ä¹‹å‰ç›¸åŒ)
        MACRO_NAMES_ORIGINAL = FACTOR_CLUSTERS['macro']

        def apply_release_lag(series: pd.Series, trading_index: pd.DatetimeIndex, lag_days: int = 2) -> pd.Series:
            s = series.copy()
            s.index = pd.to_datetime(s.index).to_period('M').to_timestamp('M')
            s = s[~s.index.duplicated(keep='last')].sort_index()
            s = s.reindex(trading_index, method='ffill')
            return s.shift(lag_days)

        for n in MACRO_NAMES_ORIGINAL:
            if n in self.factors:
                self.factors[n] = apply_release_lag(self.factors[n], self.data.index, lag_days=2)

        # 5. ã€ã€ã€æ ¸å¿ƒå‡çº§ 3ï¼šä»åŸå§‹å®è§‚æ•°æ®ä¸­æå–â€œåŠ¨é‡â€ä¿¡å·ã€‘ã€‘ã€‘
        logger.info("ğŸ”„ å¢å¼ºå®è§‚å› å­ï¼šè®¡ç®—3ä¸ªæœˆå˜åŒ–ç‡ä½œä¸ºåŠ¨é‡ä¿¡å·...")
        new_macro_factors = {}
        new_macro_names_in_cluster = []
        for name in MACRO_NAMES_ORIGINAL:
            if name in self.factors:
                original_series = self.factors[name]
                # è®¡ç®—çº¦3ä¸ªæœˆçš„å˜åŒ–ç‡ (63ä¸ªäº¤æ˜“æ—¥)
                momentum_series = original_series.diff(63).fillna(0.0)

                # å¯¹å˜åŒ–ç‡å†æ¬¡è¿›è¡Œæ»šåŠ¨æ ‡å‡†åŒ–ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€å¯æ¯”çš„å› å­
                rolling_mean = momentum_series.rolling(252, min_periods=120).mean()
                rolling_std = momentum_series.rolling(252, min_periods=120).std().replace(0, 1e-8)
                standardized_momentum = ((momentum_series - rolling_mean) / rolling_std).fillna(0.0)

                new_name = f"{name}_mom"
                new_macro_factors[new_name] = standardized_momentum.clip(-3.5, 3.5)
                new_macro_names_in_cluster.append(new_name)

        # å°†æ–°åˆ›å»ºçš„å®è§‚åŠ¨é‡å› å­åŠ å…¥ç³»ç»Ÿ
        self.factors.update(new_macro_factors)
        FACTOR_CLUSTERS['macro'].extend(new_macro_names_in_cluster)

        logger.info(f"âœ… å› å­é¢„å¤„ç†å®Œæˆï¼Œå…± {len(self.factors)} ä¸ªå› å­ (æ–°å¢ {len(new_macro_factors)} ä¸ªå®è§‚åŠ¨é‡å› å­)ã€‚")

    def _calculate_regime_scores(self) -> Tuple[pd.Series, pd.Series]:
        logger.info("ğŸ”„ (V7.4 ç»Ÿä¸€æ„ŸçŸ¥å¼•æ“) å¼€å§‹è®¡ç®—å¸‚åœºç»¼åˆæƒ…æ™¯è¯„åˆ†...")
        try:
            close = self.data['Close']

            # --- æ”¯æŸ± 1: ä»·æ ¼è¶‹åŠ¿ç»„ä»¶ ---
            sma_200 = close.rolling(200, min_periods=50).mean()
            trend_strength = (close - sma_200) / (close.rolling(60).std() + 1e-8)
            trend_component = (trend_strength / 3.0).clip(-1, 1)

            # --- æ”¯æŸ± 2: æ³¢åŠ¨çŠ¶æ€ç»„ä»¶ ---
            high, low = self.data['High'], self.data['Low']
            tr = pd.concat([high - low, abs(high - close.shift(1)), abs(low - close.shift(1))], axis=1).max(axis=1)
            atr_short = tr.rolling(20, min_periods=15).mean()
            atr_long = tr.rolling(120, min_periods=100).mean()
            vol_ratio = atr_short / (atr_long + 1e-8)
            vol_component = (1.5 - vol_ratio).clip(-1, 1)

            # --- æ”¯æŸ± 3: å®è§‚å¥åº·åº¦ç»„ä»¶ ---
            macro_factor_names = FACTOR_CLUSTERS['macro']
            macro_factors_df = pd.DataFrame({k: self.factors[k] for k in macro_factor_names if k in self.factors})
            if not macro_factors_df.empty:
                raw_macro_signal = macro_factors_df.mean(axis=1)
                macro_component = np.tanh(raw_macro_signal).fillna(0.0)
            else:
                macro_component = pd.Series(0.0, index=self.data.index)

            # --- æœ€ç»ˆèåˆ: ä¸‰å¤§æ”¯æŸ±åŠ æƒ ---
            weights = PARAMS['REGIME_SCORE_WEIGHTS']
            unified_risk_score = (
                    weights['trend'] * trend_component +
                    weights['macro'] * macro_component +
                    weights['volatility'] * vol_component
            ).ffill().bfill().fillna(0.0)

            # --- å®šä¹‰çŠ¶æ€ ---
            trend_regime = pd.Series('NEUTRAL', index=self.data.index)
            trend_regime[unified_risk_score > PARAMS['REGIME_BULL_THRESHOLD']] = 'BULL'
            trend_regime[unified_risk_score < PARAMS['REGIME_BEAR_THRESHOLD']] = 'BEAR'
            trend_regime.ffill(inplace=True)

            logger.info(
                f"âœ… ç»Ÿä¸€æƒ…æ™¯è¯„åˆ†å®Œæˆã€‚å½“å‰è¶‹åŠ¿: {trend_regime.iloc[-1]}, ç»Ÿä¸€é£é™©æŒ‡æ•°: {unified_risk_score.iloc[-1]:.3f}")
            return trend_regime, unified_risk_score

        except Exception as e:
            logger.error(f"âŒ è®¡ç®—ç»Ÿä¸€æƒ…æ™¯è¯„åˆ†å¤±è´¥: {e}\n{traceback.format_exc()}")
            return (pd.Series('UNKNOWN', index=self.data.index), pd.Series(0.0, index=self.data.index))

    def synthesize_cluster_signals(self, ic_results: Dict) -> Dict[str, pd.Series]:
        logger.info("ğŸ”„ (åŠ¨æ€ EWMA_IR åŠ æƒ) å¼€å§‹åˆæˆç­–ç•¥ç°‡ä¿¡å·...")
        cluster_signals = {}
        ir_scores = {
            name: abs(ic.get('ewma_ic', 0.0) / (ic.get('ic_std', 1.0) + 1e-8))
            for name, ic in ic_results.items()
        }

        for cluster_name, factor_list in FACTOR_CLUSTERS.items():
            weighted_signals = pd.Series(0.0, index=self.data.index)
            total_weight = 0.0

            for factor_name in factor_list:
                if factor_name in self.factors and factor_name in ir_scores:
                    weight = min(ir_scores[factor_name], PARAMS['MAX_IR_WEIGHT'])
                    signal = self.factors[factor_name].reindex(self.data.index).ffill().bfill().fillna(0.0)
                    weighted_signals += signal * weight
                    total_weight += weight

            if total_weight > 0:
                final_cluster_signal = weighted_signals / total_weight
            else:
                logger.warning(f"ç­–ç•¥ç°‡ {cluster_name} æ— æœ‰æ•ˆIRï¼Œå›é€€åˆ°ç­‰æƒå¹³å‡ã€‚")
                cluster_df = pd.concat([self.factors[f] for f in factor_list if f in self.factors], axis=1)
                final_cluster_signal = cluster_df.mean(axis=1).reindex(self.data.index).ffill().bfill().fillna(0.0)

            cluster_signals[cluster_name] = final_cluster_signal
            logger.debug(f"âœ… ç­–ç•¥ç°‡ {cluster_name} ä¿¡å·åˆæˆå®Œæˆï¼Œæœ€æ–°å€¼: {final_cluster_signal.iloc[-1]:.4f}")
        return cluster_signals

    def get_meta_strategy_weights(self, risk_on_off_score: float) -> Dict[str, float]:
        logger.info(f"ğŸ”„ æ ¹æ®é£é™©åå¥½æŒ‡æ•° '{risk_on_off_score:.3f}' å¹³æ»‘åˆ†é…ç­–ç•¥æƒé‡...")
        risk_on_weights = {'trend': 0.60, 'reversion': 0.05, 'volatility': 0.10, 'macro': 0.25}
        risk_off_weights = {'trend': 0.10, 'reversion': 0.30, 'volatility': 0.40, 'macro': 0.20}

        current_weights = {
            name: np.interp(risk_on_off_score, [-1.0, 1.0], [risk_off_weights[name], risk_on_weights[name]])
            for name in risk_on_weights.keys()
        }

        total_weight = sum(current_weights.values())
        normalized_weights = {k: v / total_weight for k, v in current_weights.items()}
        logger.info(f"âœ… å¹³æ»‘ç­–ç•¥æƒé‡åˆ†é…å®Œæˆ: {normalized_weights}")
        return normalized_weights

    @performance_monitor
    def machine_learning_predict(self,
                                 horizons=None,
                                 horizon_weights=None,
                                 min_samples=120,
                                 top_quantile=0.70,
                                 bottom_quantile=0.30,
                                 n_top_features=30,
                                 tss_splits=4,
                                 pred_smooth_span=3,
                                 min_train_for_output=200):
        """
        ç¨³å¥çš„ ML é¢„æµ‹æ¨¡å—ï¼ˆæ›¿æ¢ç‰ˆï¼‰ã€‚
        è¾“å‡º: (ml_signal_series (pd.Series aligned to self.data.index), ml_confidence (float 0..1))
        è®¾è®¡è¦ç‚¹:
          - æ¯ä¸ª horizon å•ç‹¬è®­ç»ƒäºŒåˆ†ç±»å™¨ï¼ˆä¸Š/ä¸‹åˆ†ä½ï¼‰ï¼Œç”¨ TimeSeriesSplit åš OOF ä¼°è®¡
          - ç”¨ cross_val_predict è®¡ç®— OOF æ¦‚ç‡å¹¶è®¡ç®— Brier scoreï¼ˆè®­ç»ƒé›†æ€§èƒ½æŒ‡æ ‡ï¼‰
          - ç”¨ CalibratedClassifierCV å¯¹æœ€ç»ˆæ¨¡å‹åšæ¦‚ç‡æ ¡å‡†ï¼ˆmethod='sigmoid' æ›´ç¨³å®šï¼‰
          - åšç®€å•çš„åŸºäºé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©ï¼ˆSelect top N by LGBM importanceï¼‰
          - è¾“å‡ºæ¯ä¸ª horizon çš„æ¦‚ç‡æ˜ å°„åˆ° [-1,1]ï¼Œå†æŒ‰ horizon_weights åŠ æƒåˆæˆ
          - ml_confidence ç»¼åˆè€ƒè™‘è®­ç»ƒæ—¶çš„å½’ä¸€åŒ– Brierï¼ˆperfï¼‰ã€æœ€è¿‘ä¿¡å·å¹…åº¦ä¸ horizon ä¸€è‡´æ€§
        """
        import numpy as np
        import pandas as pd
        from sklearn.model_selection import TimeSeriesSplit, cross_val_predict
        from sklearn.metrics import brier_score_loss
        from sklearn.feature_selection import SelectFromModel
        from sklearn.preprocessing import StandardScaler
        from sklearn.calibration import CalibratedClassifierCV
        from sklearn.utils.class_weight import compute_class_weight

        try:
            import lightgbm as lgb
        except Exception:
            # å¦‚æœæ²¡æœ‰ lightgbmï¼Œè¿”å›ä¸­æ€§ä¿¡å·å¹¶ç»™å‡ºä½ç½®ä¿¡åº¦
            logger.warning("LightGBM not available â€” skipping ML module.")
            return pd.Series(0.0, index=self.data.index), 0.01

        # å‚æ•°é»˜è®¤
        if horizons is None:
            horizons = [1, 5, 10]
        if horizon_weights is None:
            # é»˜è®¤çŸ­æœŸæƒé‡å¤§
            horizon_weights = {1: 0.5, 5: 0.3, 10: 0.2}
        # å½’ä¸€åŒ– horizon æƒé‡
        total_hw = sum(horizon_weights.get(h, 0.0) for h in horizons) or 1.0
        hw = {h: float(horizon_weights.get(h, 0.0) / total_hw) for h in horizons}

        # å‡†å¤‡ç‰¹å¾ä¸ä»·æ ¼ï¼ˆå·²å‡è®¾ self.factors å·²åšæ»šåŠ¨ robust preprocessingï¼‰
        feats = pd.DataFrame(self.factors).reindex(self.data.index).astype(float).fillna(method='ffill').fillna(0.0)
        # å°‘é‡å¹³æ»‘
        feats = feats.ewm(span=pred_smooth_span, adjust=False).mean().fillna(0.0)
        close = self.data['Close'].astype(float).reindex(self.data.index).fillna(method='ffill').fillna(0.0)

        n = len(feats)
        if n < min_samples or close.isna().sum() > 0.5 * n:
            # æ•°æ®ä¸è¶³æˆ–ä»·æ ¼ç¼ºå¤±ä¸¥é‡ -> è¿”å›ä¸­æ€§
            logger.warning("æ•°æ®ä¸è¶³æˆ–ä»·æ ¼ç¼ºå¤±è¿‡å¤šï¼Œè·³è¿‡ ML æ¨¡å—ã€‚")
            return pd.Series(0.0, index=self.data.index), 0.01

        # é¢„è®¡ç®—æœªæ¥æ”¶ç›Šæ•°ç»„ï¼ˆnumpy å½¢å¼ï¼‰
        fut_ret = {h: (close.shift(-h) / close - 1.0).values for h in horizons}

        # ä¿å­˜æ¯ä¸ª horizon çš„è¾“å‡ºåŠè®­ç»ƒæŒ‡æ ‡
        horizon_signals = {}
        horizon_perf = {}  # ç”¨äºè®°å½•è®­ç»ƒæœŸçš„ normalized brier æˆ–å›é€€æŒ‡æ ‡

        for h in horizons:
            try:
                y_all = fut_ret[h]
                # åªè€ƒè™‘æœªæ¥æ”¶ç›Šä¸æ˜¯ NaN çš„ä½ç½®
                valid_idx = np.where(~np.isnan(y_all))[0]
                if valid_idx.size < min_samples:
                    # ä¸è¶³æ ·æœ¬ -> é€€åŒ–ä¸º 0 ä¿¡å·
                    horizon_signals[h] = pd.Series(0.0, index=feats.index)
                    horizon_perf[h] = 0.0
                    continue

                # æ„é€ è®­ç»ƒæ ‡ç­¾ï¼šä¸Š/ä¸‹åˆ†ä½äºŒåˆ†ç±»ï¼ˆå‰”é™¤ä¸­æ€§åŒºé—´ï¼‰
                y_vec = pd.Series(y_all, index=feats.index)
                q_high = y_vec.quantile(top_quantile)
                q_low = y_vec.quantile(bottom_quantile)
                mask = (y_vec <= q_low) | (y_vec >= q_high)
                if mask.sum() < min_samples:
                    # è‹¥ä¸Šä¸‹åˆ†ä½æ ·æœ¬è¿‡å°‘ï¼Œæ”¹ä¸ºå›å½’å¼é¢„æµ‹ï¼ˆè½»é‡ LGBM å›å½’ï¼‰
                    X_reg = feats.loc[~y_vec.isna(), :]
                    y_reg = y_vec.loc[~y_vec.isna()]
                    if X_reg.shape[0] < min_samples:
                        horizon_signals[h] = pd.Series(0.0, index=feats.index)
                        horizon_perf[h] = 0.0
                        continue
                    # å›å½’è®­ç»ƒï¼ˆè¾ƒä¿å®ˆçš„å‚æ•°ï¼‰
                    reg = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.03, random_state=42)
                    reg.fit(X_reg, y_reg)
                    pred = pd.Series(reg.predict(feats), index=feats.index).fillna(0.0)
                    # æ˜ å°„åˆ° [-1,1]
                    sig = pred / (pred.abs().rolling(63, min_periods=1).std().replace(0, 1) + 1e-8)
                    sig = np.tanh(sig.fillna(0.0))
                    horizon_signals[h] = sig
                    # å›å½’æ²¡æœ‰ brierï¼Œä½¿ç”¨ R^2-like proxy (clip 0..1)
                    horizon_perf[h] = max(0.0, min(1.0, 0.5))  # ä¿å®ˆé»˜è®¤
                    continue

                # äºŒåˆ†ç±»æ ·æœ¬å‡†å¤‡
                idx_bin = mask[mask].index
                X = feats.loc[idx_bin, :].copy()
                y_bin = (y_vec.loc[idx_bin] >= q_high).astype(int).values  # 1 è¡¨ç¤ºä¸Šåˆ†ä½

                # è®­ç»ƒé›†å¤§å°ä¿æŠ¤
                if X.shape[0] < min_samples:
                    horizon_signals[h] = pd.Series(0.0, index=feats.index)
                    horizon_perf[h] = 0.0
                    continue

                # è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆbalancedï¼‰
                try:
                    classes = np.unique(y_bin)
                    if classes.size < 2:
                        # å•ç±»ä¿æŠ¤
                        sample_weight = np.ones(len(y_bin), dtype=float)
                    else:
                        cw = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_bin)
                        # cw ç»“æœä½ç½®å¯¹åº” [class0_weight, class1_weight]
                        w0, w1 = float(cw[0]), float(cw[1])
                        sample_weight = np.where(y_bin == 1, w1, w0).astype(float)
                except Exception:
                    sample_weight = np.ones(len(y_bin), dtype=float)

                # å…ˆè¿›è¡Œç®€å•ç‰¹å¾é€‰æ‹©ï¼šç”¨ä¸€ä¸ªè½»é‡ LGBM å¿«é€Ÿè®¡ç®—é‡è¦æ€§
                fs_features = X.columns.tolist()
                try:
                    fs_clf = lgb.LGBMClassifier(n_estimators=150, learning_rate=0.05, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                    fs_clf.fit(X, y_bin, sample_weight=sample_weight)
                    importances = fs_clf.feature_importances_
                    # é€‰æ‹©å‰ n_top_featuresï¼ˆå—é™äºç°æœ‰ç‰¹å¾æ•°ï¼‰
                    nsel = min(int(n_top_features), len(importances))
                    if nsel < 5:
                        nsel = min(5, len(importances))
                    top_idx = np.argsort(importances)[-nsel:]
                    sel_cols = [X.columns[i] for i in sorted(top_idx)]
                    X_sel = X[sel_cols]
                    feats_sel = feats[sel_cols]
                except Exception:
                    # è‹¥ç‰¹å¾é€‰æ‹©å¤±è´¥ï¼Œé€€å›å…¨ç‰¹å¾
                    sel_cols = X.columns.tolist()
                    X_sel = X
                    feats_sel = feats

                # æ ‡å‡†åŒ–ï¼ˆfit on training setï¼‰
                scaler = StandardScaler()
                X_scaled = pd.DataFrame(scaler.fit_transform(X_sel), index=X_sel.index, columns=X_sel.columns)
                feats_scaled = pd.DataFrame(scaler.transform(feats_sel), index=feats_sel.index,
                                            columns=feats_sel.columns)

                # äº¤å‰éªŒè¯é…ç½®ï¼ˆTimeSeriesSplitï¼‰
                n_splits = min(tss_splits, max(2, X_scaled.shape[0] // 50))
                tss = TimeSeriesSplit(n_splits=n_splits)

                # OOF æ¦‚ç‡ä¼°è®¡ï¼ˆç”¨äºè®­ç»ƒæœŸçš„ Brier scoreï¼‰
                try:
                    base_clf = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                    # cross_val_predict è·å– OOF æ¦‚ç‡ï¼ˆæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„ out-of-fold é¢„æµ‹ï¼‰
                    oof_proba = cross_val_predict(base_clf, X_scaled, y_bin, cv=tss, method='predict_proba', n_jobs=1)
                    oof_pos = oof_proba[:, 1]
                    # è®­ç»ƒæœŸ Brier
                    brier = brier_score_loss(y_bin, oof_pos)
                    # å½’ä¸€åŒ– brier -> (1 - normalized_brier) ä½œä¸º perf (0..1, è¶Šå¤§è¶Šå¥½)
                    denom = (y_bin.mean() * (1.0 - y_bin.mean()) + 1e-8)
                    norm_brier = 1.0 - (brier / denom)
                    norm_brier = float(np.clip(norm_brier, 0.0, 1.0))
                except Exception:
                    # å¦‚ cross_val_predict å¤±è´¥åˆ™ä¿å®ˆä¼°è®¡
                    norm_brier = 0.2

                # æœ€ç»ˆä½¿ç”¨ CalibratedClassifierCV åœ¨å…¨è®­ç»ƒé›†ä¸Šè®­ç»ƒï¼ˆå¸¦ TimeSeriesSplit æ ¡å‡†ï¼‰
                try:
                    final_base = lgb.LGBMClassifier(n_estimators=400, learning_rate=0.02, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                    calib = CalibratedClassifierCV(final_base, method='sigmoid', cv=tss)
                    # ä¼ å…¥ sample_weightï¼ˆCalibratedClassifierCV ä¼šä¼ ç»™ base estimator çš„ fitï¼‰
                    calib.fit(X_scaled, y_bin, sample_weight=sample_weight)
                    proba_all = pd.DataFrame(calib.predict_proba(feats_scaled), index=feats_scaled.index)
                    sig_all = (proba_all.iloc[:, 1] - proba_all.iloc[:, 0]).astype(float)  # P(1)-P(0), ç­‰ä»·äº 2*p-1
                    # æ˜ å°„åˆ° [-1,1]ï¼šsig_all å·²è¿‘ä¼¼åœ¨ [-1,1]ï¼Œä½†åš tanh ç¨³å®šåŒ–å¹¶è¿”å› Series
                    sig_series = pd.Series(np.tanh(sig_all), index=feats_scaled.index).fillna(0.0)
                    horizon_signals[h] = sig_series
                    horizon_perf[h] = float(norm_brier)
                except Exception as e:
                    logger.warning(f"ML training/calibration failed for horizon {h}: {e}")
                    # è®­ç»ƒå¤±è´¥åˆ™é€€å›åˆ°åœ¨å…¨ç‰¹å¾ä¸Šç”¨ LGBM ç›´æ¥é¢„æµ‹æ¦‚ç‡
                    try:
                        fallback = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, random_state=42, min_gain_to_split=0.0, lambda_l1=0.1, lambda_l2=0.1)
                        fallback.fit(X_scaled, y_bin, sample_weight=sample_weight)
                        proba_all = pd.Series(fallback.predict_proba(feats_scaled)[:, 1], index=feats_scaled.index)
                        sig_series = pd.Series(np.tanh(2.0 * (2 * proba_all - 1.0)), index=feats_scaled.index).fillna(
                            0.0)
                        horizon_signals[h] = sig_series
                        horizon_perf[h] = float(norm_brier * 0.8)
                    except Exception:
                        horizon_signals[h] = pd.Series(0.0, index=feats.index)
                        horizon_perf[h] = 0.0

            except Exception as e_main:
                logger.exception(f"Unexpected error processing horizon {h}: {e_main}")
                horizon_signals[h] = pd.Series(0.0, index=feats.index)
                horizon_perf[h] = 0.0

        # åˆå¹¶ horizon ä¿¡å·ï¼ˆæŒ‰ hw æƒé‡ï¼‰
        combined = pd.Series(0.0, index=feats.index, dtype=float)
        present_hw_total = 0.0
        for h in horizons:
            sig = horizon_signals.get(h, pd.Series(0.0, index=feats.index)).fillna(0.0)
            w = hw.get(h, 0.0)
            combined += sig * w
            present_hw_total += (w if not sig.isna().all() else 0.0)

        if present_hw_total == 0:
            # ä¿é™©å…œåº•
            return pd.Series(0.0, index=feats.index), 0.01

        # å¹³æ»‘å¹¶è£å‰ªåˆ° [-1,1]
        combined = combined.ewm(span=3, adjust=False).mean().fillna(0.0)
        combined = combined.apply(lambda x: float(np.clip(x, -1.0, 1.0)))

        # æœ€ä½è®­ç»ƒæ ·æœ¬ä¿æŠ¤ï¼šåœ¨è®­ç»ƒä¸è¶³ min_train_for_output æ—¶æŠŠæ—©æœŸè¾“å‡ºè®¾ä¸º 0
        if n >= min_train_for_output:
            earliest_valid_idx = feats.index[min_train_for_output] if min_train_for_output < n else feats.index[-1]
            combined.loc[:earliest_valid_idx] = 0.0
        else:
            combined[:] = 0.0

        # è®¡ç®— ml_confidenceï¼ˆç»¼åˆ training perf + recent magnitude + horizon ä¸€è‡´æ€§ï¼‰
        try:
            # training perf: å¹³å‡ horizon_perfï¼ˆ0..1ï¼‰
            perf_vals = np.array(list(horizon_perf.values())) if horizon_perf else np.array([0.0])
            training_perf = float(np.nanmean(perf_vals)) if perf_vals.size else 0.0
            training_perf = float(np.clip(training_perf, 0.0, 1.0))

            # recent magnitude: æœ€è¿‘ 5 å¤©ä¿¡å·çš„ç»å¯¹å€¼å‡å€¼
            recent_mag = float(combined.tail(5).abs().mean()) if len(combined) >= 5 else float(combined.abs().mean())

            # horizon ä¸€è‡´æ€§: å¯¹æœ€å 10 å¤©ä¸åŒ horizon ä¿¡å·çš„ stdï¼ˆè¶Šå°è¶Šä¸€è‡´ï¼‰
            stacked = pd.DataFrame({h: horizon_signals[h].fillna(0.0) for h in horizons})
            horiz_std = float(stacked.tail(10).std(axis=1).mean()) if stacked.shape[0] >= 2 else 0.0
            horiz_consistency = 1.0 / (1.0 + horiz_std)

            # ç»„åˆï¼šæƒé‡ï¼ˆå¯è°ƒï¼‰ -> æ›´çœ‹é‡è®­ç»ƒæœŸ perfï¼ˆ0.5ï¼‰ï¼Œå†çœ‹æœ€è¿‘å¹…åº¦ï¼ˆ0.3ï¼‰ï¼Œå’Œä¸€è‡´æ€§ï¼ˆ0.2ï¼‰
            ml_confidence = float(np.clip(0.5 * training_perf + 0.3 * recent_mag + 0.2 * horiz_consistency, 0.0, 0.99))
        except Exception:
            ml_confidence = 0.05

        logger.info(f"ML module finished. ml_confidence={ml_confidence:.3f}, latest_ml_signal={combined.iloc[-1]:.4f}")
        return combined.fillna(0.0), float(ml_confidence)

    @performance_monitor
    def calculate_factor_ic(self,
                            lookback: int = 252,
                            ic_window: int = 63,
                            horizons: list = None,
                            horizon_weights: dict = None,
                            ewma_span: int = 63,
                            min_periods_ratio: float = 0.5,
                            regime_filter: Optional[str] = None):
        """
        è®¡ç®—å› å­ Information Coefficient (IC) çš„æ›¿æ¢ç‰ˆï¼ˆæ»šåŠ¨ + EWMAï¼‰ã€‚
        è¾“å‡º: dict: { factor_name: {
                            'mean_ic': float,
                            'ic_std': float,
                            'ewma_ic': float,
                            'last_ic': float,
                            'ic_series': pd.Series
                        }, ... }
        å‚æ•°è¯´æ˜:
          - lookback: ç”¨äºäº§å‡º mean_ic/ic_std çš„å†å²çª—å£é•¿åº¦ï¼ˆé»˜è®¤ 252 æ—¥ï¼‰
          - ic_window: æ¯ä¸ªæ»šåŠ¨ IC è®¡ç®—çš„çª—å£é•¿åº¦ï¼ˆé»˜è®¤ 63 æ—¥ï¼‰
          - horizons: æœªæ¥æ”¶ç›Šçš„ horizon åˆ—è¡¨ï¼ˆä¾‹å¦‚ [1,5,10]ï¼‰ï¼Œé»˜è®¤ [1,5,10]
          - horizon_weights: æ¯ä¸ª horizon çš„æƒé‡å­—å…¸ï¼Œé»˜è®¤ç­‰æƒæˆ– [1,0.6,0.4]ï¼ˆçŸ­æœŸæ¯”é‡æ›´å¤§ï¼‰
          - ewma_span: ç”¨äºè®¡ç®— EWMA å¹³æ»‘ IC çš„ span
          - min_periods_ratio: æ»šåŠ¨çª—å£å†…æœ€å°‘æœ‰æ•ˆæ•°æ®æ¯”ä¾‹é˜ˆå€¼ (ä¾‹å¦‚ 0.5 => è‡³å°‘ window*0.5 ä¸ªæœ‰æ•ˆç‚¹)
        è®¾è®¡è¦ç‚¹:
          - å¯¹æ¯ä¸ªå› å­ä¸æ¯ä¸ª horizon è®¡ç®—â€œå‘åæ»šåŠ¨çš„ Pearson ç›¸å…³ï¼ˆICï¼‰â€ï¼›
          - å„ horizon çš„æ»šåŠ¨ IC ç”¨æƒé‡åˆå¹¶å¾—åˆ°å•ä¸€çš„å› å­æ»šåŠ¨ IC åºåˆ—ï¼›
          - å¯¹æœ€è¿‘ lookback çª—å£è®¡ç®— mean å’Œ stdï¼Œå¹¶è¿”å› EWMA å¹³ç¨³å€¼ä¸æœ€åå€¼ï¼›
          - å…¨è¿‡ç¨‹åªä½¿ç”¨å†å²çª—å£ï¼ˆæ—  look-aheadï¼‰ã€‚
        """
        import numpy as np
        import pandas as pd

        # é»˜è®¤å‚æ•°
        if horizons is None:
            horizons = [1, 5, 10]
        if horizon_weights is None:
            # æ¨èçŸ­æœŸæ›´é‡ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰
            horizon_weights = {1: 1.0, 5: 0.6, 10: 0.4}
        # normalize horizon weights
        total_hw = sum([horizon_weights.get(h, 0.0) for h in horizons]) or 1.0
        hw = np.array([horizon_weights.get(h, 0.0) / total_hw for h in horizons])

        # å‡†å¤‡å› å­çŸ©é˜µä¸ä»·æ ¼åºåˆ—ï¼ˆå¯¹é½ç´¢å¼•ï¼‰
        factor_df = pd.DataFrame(self.factors).reindex(self.data.index).astype(float).copy()
        # è‹¥æœ‰ç¼ºå¤±ï¼Œä½¿ç”¨å‰å‘å¡«å……å†åå‘å¡«å……ä½œä¸ºå…œåº•ï¼ˆå› å­é¢„å¤„ç†åº”å·²åšè¿‡ï¼‰
        factor_df = factor_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)

        close = self.data['Close'].astype(float).reindex(self.data.index).copy()
        close = close.fillna(method='ffill').fillna(method='bfill')
        n = len(factor_df)
        if n == 0:
            return {}

        # é¢„å…ˆè®¡ç®—æ¯ä¸ª horizon çš„æœªæ¥æ”¶ç›Šï¼ˆåå‘çœ‹ï¼‰
        fut_ret = {}
        for h in horizons:
            fut_ret[h] = (close.shift(-h) / close - 1.0).values  # numpy array with NaNs at tail

        # å†…éƒ¨å‡½æ•°: è®¡ç®—å•åˆ—ä¸æŸä¸ª horizon çš„å‘åæ»šåŠ¨ Pearson ç›¸å…³ï¼ˆarray->arrayï¼‰
        def _rolling_corr(x: np.ndarray, y: np.ndarray, window: int, min_periods: int):
            """
            è¿”å›é•¿åº¦=n çš„æ•°ç»„ï¼Œindex å¯¹åº”åŸåºåˆ—ç´¢å¼•ï¼›ä¸è¶³æ—¶ä¸º np.nanã€‚
            x,y: 1D numpy arrays (å¯èƒ½æœ‰ nan)
            """
            res = np.full_like(x, np.nan, dtype=float)
            if window <= 1 or len(x) < window:
                return res
            # éå†ç»ˆç‚¹ï¼ˆä»¥çª—å£ä¸ºå•ä½ï¼‰
            for i in range(window - 1, len(x)):
                xs = x[(i - window + 1):(i + 1)]
                ys = y[(i - window + 1):(i + 1)]
                mask = (~np.isnan(xs)) & (~np.isnan(ys))
                cnt = int(mask.sum())
                if cnt < min_periods:
                    continue
                xv = xs[mask]
                yv = ys[mask]
                # é›¶æ–¹å·®ä¿æŠ¤
                if xv.std(ddof=0) < 1e-12 or yv.std(ddof=0) < 1e-12:
                    # å¦‚æœæ— æ³¢åŠ¨ï¼Œç›¸å…³æ€§å®šä¹‰ä¸º 0ï¼ˆæ›´ç¨³å¥ï¼‰
                    res[i] = 0.0
                else:
                    c = np.corrcoef(xv, yv)[0, 1]
                    # æœ‰æ—¶ä¼šäº§ç”Ÿå¾®å°è¶…å‡º [-1,1] çš„å€¼ï¼Œè£å‰ª
                    res[i] = float(np.clip(c, -1.0, 1.0))
            return res

        # æœ€ä½æœ‰æ•ˆç‚¹æ•°
        min_periods = max(4, int(ic_window * min_periods_ratio))

        ic_results = {}
        index = factor_df.index

        # å¯¹æ¯ä¸ªå› å­å¾ªç¯è®¡ç®—ï¼ˆå› å­æ•°é‡é€šå¸¸ä¸å¤§ï¼Œç›´æ¥ loop å¯è¯»æ€§é«˜ï¼‰
        for col in factor_df.columns:
            x_arr = factor_df[col].values.astype(float)

            # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å¦‚æœä¼ å…¥äº†çŠ¶æ€è¿‡æ»¤å™¨ï¼Œåˆ™ç”¨NaNå±è”½éè¯¥çŠ¶æ€çš„æ•°æ®
            if regime_filter and regime_filter in ['BULL', 'BEAR']:
                # ä» self.regime_series è·å–å¸ƒå°”æ©ç 
                mask = (self.regime_series == regime_filter).values
                # å°†ä¸ç¬¦åˆæ¡ä»¶çš„ä½ç½®è®¾ç½®ä¸ºNaN
                x_arr[~mask] = np.nan

            # per-horizon rolling ICs (shape: n_horizons x n)
            horizon_ic_matrix = []
            for h in horizons:
                y_arr = fut_ret[h].copy()  # ä½¿ç”¨å‰¯æœ¬ä»¥é˜²æ±¡æŸ“

                # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åŒæ ·å±è”½æœªæ¥æ”¶ç›Šä¸­çš„éè¯¥çŠ¶æ€æ•°æ®
                if regime_filter and regime_filter in ['BULL', 'BEAR']:
                    y_arr[~mask] = np.nan

                # æ³¨æ„ï¼šy_arr çš„ tail éƒ¨åˆ†ä¼šå« NaNï¼ˆshift å¯¼è‡´ï¼‰ï¼Œrolling_corr ä¼šè‡ªåŠ¨è·³è¿‡
                rc = _rolling_corr(x_arr, y_arr, ic_window, min_periods)
                horizon_ic_matrix.append(rc)
            # è½¬ç½®ä¸º (n, num_horizons)
            mat = np.vstack(horizon_ic_matrix).T  # shape (n, H)

            # è¡Œåˆå¹¶ï¼šå¯¹å­˜åœ¨ value çš„ horizon åšåŠ æƒå¹³å‡ï¼ˆå¿½ç•¥ NaNï¼‰
            combined = np.full(n, np.nan, dtype=float)
            for i in range(n):
                row = mat[i, :]
                valid = ~np.isnan(row)
                if not valid.any():
                    continue
                wvalid = hw[valid]
                if wvalid.sum() == 0:
                    combined[i] = np.nan
                else:
                    combined[i] = float(np.dot(row[valid], wvalid) / wvalid.sum())

            ic_series = pd.Series(combined, index=index).astype(float)

            # è½»å¾®çš„ EWMA å¹³æ»‘ï¼ˆå¯é€‰ï¼‰ï¼Œç„¶åç”¨äºè¾“å‡º ewma_ic
            ic_ewma = ic_series.ewm(span=ewma_span, adjust=False).mean()

            # recent window ç”¨äº mean/std è®¡ç®—
            if lookback <= 0:
                # è‹¥ lookback éæ­£ï¼Œåˆ™ç”¨å…¨éƒ¨æœ‰æ•ˆæ ·æœ¬
                recent = ic_series.dropna()
            else:
                recent = ic_series.dropna().iloc[-lookback:]

            if recent.empty:
                mean_ic = 0.0
                ic_std = 0.0
            else:
                mean_ic = float(recent.mean())
                ic_std = float(recent.std(ddof=0))  # population stdï¼Œoptimize ä¸­ä½¿ç”¨å³å¯

            last_ic = float(ic_series.dropna().iloc[-1]) if not ic_series.dropna().empty else 0.0
            ewma_ic = float(ic_ewma.dropna().iloc[-1]) if not ic_ewma.dropna().empty else mean_ic

            ic_results[col] = {
                'mean_ic': mean_ic,
                'ic_std': ic_std,
                'ewma_ic': ewma_ic,
                'last_ic': last_ic,
                'ic_series': ic_series  # pandas Seriesï¼Œä¾› debug / å¯è§†åŒ–ä½¿ç”¨
            }

        # è¿”å›ç»“æœï¼ˆå­—å…¸ï¼‰
        return ic_results

    def run_full_analysis(self) -> Dict[str, Any]:
        """ã€V7.4 ã€‘å®Œæ•´åˆ†ææµç¨‹ï¼ŒåŒ…å«å®Œå¤‡çš„æƒ…æ™¯ICé€‰æ‹©é€»è¾‘"""
        try:
            logger.info("ğŸ”„ (V7.4 ) å¼€å§‹å®Œæ•´åˆ†ææµç¨‹...")
            self.calculate_all_factors()
            self.factor_preprocessing()
            trend_regime, unified_risk_score = self._calculate_regime_scores()
            self.regime_series, self.unified_risk_score = trend_regime, unified_risk_score

            current_trend_regime = trend_regime.iloc[-1]
            current_risk_score = unified_risk_score.iloc[-1]

            logger.info("ä¸ºç°‡å†…åŠ¨æ€åŠ æƒè®¡ç®—å¤šæƒ…æ™¯IC...")
            ic_results_bull = self.calculate_factor_ic(regime_filter='BULL')
            ic_results_bear = self.calculate_factor_ic(regime_filter='BEAR')
            ic_results_all = self.calculate_factor_ic(regime_filter=None)

            # --- ã€V7.4 é€»è¾‘ä¿®æ­£ã€‘æ ¹æ®æƒ…æ™¯é€‰æ‹©æœ€åˆé€‚çš„IC ---
            if current_trend_regime == 'BULL':
                ic_for_synthesis, ic_source = ic_results_bull, "BULL"
            elif current_trend_regime == 'BEAR':
                ic_for_synthesis, ic_source = ic_results_bear, "BEAR"
            else:  # NEUTRAL or UNKNOWN
                ic_for_synthesis, ic_source = ic_results_all, "ALL (NEUTRAL)"
            logger.info(f"å½“å‰å¸‚åœºæƒ…æ™¯: {current_trend_regime}ï¼Œå·²é€‰æ‹© {ic_source} å‘¨æœŸçš„ICæ•°æ®è¿›è¡Œåˆæˆã€‚")

            cluster_signals = self.synthesize_cluster_signals(ic_for_synthesis)
            meta_weights = self.get_meta_strategy_weights(current_risk_score)

            composite_signal = sum(signal * meta_weights.get(name, 0.0) for name, signal in cluster_signals.items())

            ml_signal, ml_confidence = self.machine_learning_predict()

            cluster_model_confidence = np.clip(abs(current_risk_score), PARAMS['CLUSTER_CONF_MIN'],
                                               PARAMS['CLUSTER_CONF_MAX'])
            total_confidence = cluster_model_confidence + ml_confidence + 1e-8
            weight_ic = cluster_model_confidence / total_confidence
            weight_ml = ml_confidence / total_confidence

            logger.info(f"è‡ªé€‚åº”èåˆ: ç­–ç•¥ç°‡ç½®ä¿¡åº¦={cluster_model_confidence:.2%}, MLç½®ä¿¡åº¦={ml_confidence:.2%}")
            logger.info(f"æœ€ç»ˆèåˆæƒé‡: ç­–ç•¥ç°‡æ¨¡å‹={weight_ic:.2%}, MLæ¨¡å‹={weight_ml:.2%}")

            final_signal = (composite_signal * weight_ic) + (ml_signal * weight_ml)
            disagreement = ((composite_signal - ml_signal).abs() / (
                        composite_signal.abs() + ml_signal.abs() + 1e-8)).fillna(0.0)

            return {
                'composite_signal': composite_signal, 'ml_signal': ml_signal, 'ml_confidence': ml_confidence,
                'final_signal': final_signal, 'disagreement': disagreement,
                'current_trend_regime': current_trend_regime,
                'current_risk_score': current_risk_score, 'cluster_signals': cluster_signals,
                'meta_weights': meta_weights,
                'fusion_weights': {'ml': weight_ml, 'ic': weight_ic},
                'cluster_model_confidence': cluster_model_confidence,
                'all_factors': self.factors, 'ic_results': ic_for_synthesis, 'ic_source': ic_source
            }
        except Exception as e:
            logger.error(f"å› å­åˆ†æå¤±è´¥: {e}\n{traceback.format_exc()}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤ç»“æ„
            return {
                'composite_signal': pd.Series(0.0, index=self.data.index),
                'ml_signal': pd.Series(0.0, index=self.data.index),
                'ml_confidence': 0.0, 'final_signal': pd.Series(0.0, index=self.data.index),
                'disagreement': pd.Series(0.0, index=self.data.index),
                'current_trend_regime': 'UNKNOWN', 'current_risk_score': 0.0, 'cluster_signals': {}, 'meta_weights': {},
                'fusion_weights': {}, 'cluster_model_confidence': 0.0, 'all_factors': {}, 'ic_source': 'ERROR'
            }


# ===================================
# é£é™©ç®¡ç†ç³»ç»Ÿ (ç­–ç•¥å®šåˆ¶ç‰ˆï¼šä»…æ­¢æŸ)
# ===================================
class RiskManager:
    def __init__(self, data: pd.DataFrame):
        self.data = data.copy()
        logger.info("é£é™©ç®¡ç†æ¨¡å—åˆå§‹åŒ–å®Œæˆ (ç­–ç•¥: ä»…æ­¢æŸ)ã€‚")

    def calculate_atr(self, period: int = 14) -> float:
        if len(self.data) < period:
            logger.warning("æ•°æ®ä¸è¶³ä»¥è®¡ç®—ATRã€‚")
            return 0.0
        high, low, close = self.data['High'], self.data['Low'], self.data['Close']
        tr = pd.concat([high - low, abs(high - close.shift(1)), abs(low - close.shift(1))], axis=1).max(axis=1)
        atr = tr.ewm(alpha=1 / period, adjust=False).mean().iloc[-1]
        return float(atr) if pd.notna(atr) else 0.0

    def calculate_position_size(self, capital: float, risk_per_trade: float = None, atr_period: int = 14) -> float:
        try:
            if risk_per_trade is None: risk_per_trade = float(os.getenv("RISK_PER_TRADE", "0.02"))
            risk_per_trade = float(np.clip(risk_per_trade, 0.005, 0.05))
            atr = self.calculate_atr(atr_period)
            current_price = float(self.data['Close'].iloc[-1])
            if atr <= 0 or capital <= 0 or current_price <= 0: return 0.0

            risk_amount_per_trade = capital * risk_per_trade
            position_quantity_by_risk = risk_amount_per_trade / atr
            max_position_quantity_by_capital = (capital * 0.5) / current_price
            final_quantity = min(position_quantity_by_risk, max_position_quantity_by_capital)
            position_value = final_quantity * current_price
            logger.info(
                f"å»ºè®®å¤´å¯¸: {final_quantity:.4f} å•ä½ (ä»·å€¼ ${position_value:,.2f}), èµ„æœ¬=${capital:,.2f}, é£é™©æ¯”ä¾‹={risk_per_trade:.2%}")
            return final_quantity
        except Exception as e:
            logger.error(f"âŒ å¤´å¯¸è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def calculate_stop_loss(self, entry_price: float, atr_period: int = 14) -> float:
        try:
            atr = self.calculate_atr(atr_period)
            if atr <= 0 or entry_price <= 0: return 0.0
            stop_loss = entry_price - atr * PARAMS['ATR_STOP_LOSS_MULTIPLIER']
            logger.debug(f"æ­¢æŸä½è®¡ç®—å®Œæˆ: å…¥åœºä»·=${entry_price:.2f}, æ­¢æŸ=${stop_loss:.2f}")
            return float(stop_loss)
        except Exception as e:
            logger.error(f"âŒ æ­¢æŸä½è®¡ç®—å¤±è´¥: {e}")
            return 0.0


# ===================================
# é¢„æµ‹ä¿¡å·ç”Ÿæˆä¸æŠ¥å‘Š
# ===================================

def _calculate_dynamic_thresholds(base_buy: float, base_exit: float, risk_score: float, composite_confidence: float,
                                  signal_volatility: float, historical_noise: float, trend_adjust_factor: float,
                                  disagreement: float) -> Tuple[float, float]:
    """ã€V7.4ã€‘åŠ¨æ€é—¨æ§›è®¡ç®—ï¼Œæ–°å¢äº†å¯¹â€˜ç³»ç»Ÿç¡®å®šæ€§â€™çš„è€ƒé‡ã€‚"""

    # 1. é£é™©è°ƒæ•´ (ä¸ä¹‹å‰ç›¸åŒ)
    risk_adjustment = trend_adjust_factor * np.tanh(risk_score * 1.5)

    # 2. ç½®ä¿¡åº¦è°ƒæ•´ (ä¸ä¹‹å‰ç›¸åŒ)
    conf_scale = 1.0 / np.sqrt(max(0.05, composite_confidence))

    # 3. å™ªå£°è°ƒæ•´ (ä¸ä¹‹å‰ç›¸åŒ)
    noise_ratio = signal_volatility / (historical_noise + 1e-8)
    noise_scale = np.clip(1.0 + 0.75 * (noise_ratio - 1.0), 0.7, 1.6)

    # 4. ã€ã€ã€æ ¸å¿ƒå‡çº§ï¼šå¼•å…¥ç¡®å®šæ€§å› å­ã€‘ã€‘ã€‘
    # disagreement èŒƒå›´ [0, 1]ã€‚disagreement è¶Šå¤§ï¼Œç³»ç»Ÿè¶Šâ€œå›°æƒ‘â€ï¼Œç¡®å®šæ€§è¶Šä½ã€‚
    # certainty_factor èŒƒå›´ [0.7, 1.0]ã€‚åˆ†æ­§åº¦è¶Šé«˜ï¼Œç¡®å®šæ€§å› å­è¶Šå°ã€‚
    certainty_factor = np.clip(1.0 - 0.3 * disagreement, 0.7, 1.0)

    # ç¡®å®šæ€§å› å­å°†ç›´æ¥å½±å“é—¨æ§›ï¼šç³»ç»Ÿè¶Šä¸ç¡®å®šï¼Œä¹°å…¥é—¨æ§›è¶Šé«˜ï¼Œå–å‡ºé—¨æ§›è¶Šä½ï¼Œä¸­é—´çš„â€œè§‚å¯ŸåŒºâ€è¶Šå®½ã€‚
    buy_th_core = base_buy * (1 - risk_adjustment) * conf_scale * noise_scale / certainty_factor
    exit_th_core = base_exit * (1 + risk_adjustment) * conf_scale * noise_scale * certainty_factor

    # 5. æ»å›è°ƒæ•´ (ä¸ä¹‹å‰ç›¸åŒ)
    hysteresis = np.clip(0.5 * signal_volatility, 0.05, 0.15)

    buy_threshold_final = np.clip(buy_th_core + hysteresis, 0.20, 0.80)
    exit_threshold_final = np.clip(exit_th_core - hysteresis, -0.80, -0.20)

    logger.info(
        f"åŠ¨æ€é—¨æ§›è®¡ç®—(V7.4): é£é™©å› å­={risk_adjustment:.2f}, ç½®ä¿¡Scale={conf_scale:.2f}, "
        f"å™ªå£°Scale={noise_scale:.2f}, ã€ç¡®å®šæ€§å› å­={certainty_factor:.2f}ã€‘ -> "
        f"æœ€ç»ˆä¹°å…¥ > {buy_threshold_final:.3f}, æœ€ç»ˆç¦»åœº < {exit_threshold_final:.3f}"
    )
    return buy_threshold_final, exit_threshold_final


def _get_top_factor_contributors(factor_results: Dict[str, Any], top_n: int = 3) -> str:
    try:
        meta_weights = factor_results.get('meta_weights', {})
        cluster_signals = factor_results.get('cluster_signals', {})
        all_factors = factor_results.get('all_factors', {})

        if not all([meta_weights, cluster_signals, all_factors]):
            return "   â€¢ è´¡çŒ®åº¦åˆ†ææ•°æ®ä¸è¶³ã€‚\n"

        # 1. è®¡ç®—æ¯ä¸ªç­–ç•¥ç°‡çš„æœ€ç»ˆè´¡çŒ®
        cluster_contributions = {
            name: meta_weights.get(name, 0.0) * (cluster_signals.get(name, pd.Series([0.0])).iloc[-1])
            for name in meta_weights
        }

        # 2. æ‰¾åˆ°è´¡çŒ®æœ€å¤§çš„ç­–ç•¥ç°‡
        dominant_cluster = max(cluster_contributions, key=lambda k: abs(cluster_contributions[k]))

        # 3. åœ¨è¿™ä¸ªä¸»å¯¼ç°‡å†…ï¼Œæ‰¾åˆ°è´¡çŒ®æœ€å¤§çš„å› å­
        report_lines = []
        cluster_factor_names = FACTOR_CLUSTERS.get(dominant_cluster, [])
        factor_values = {
            name: all_factors[name].iloc[-1]
            for name in cluster_factor_names if name in all_factors and not all_factors[name].empty
        }

        if not factor_values:
            return f"   â€¢ ä¸»å¯¼ç­–ç•¥ç°‡ '{dominant_cluster.upper()}' (è´¡çŒ®: {cluster_contributions[dominant_cluster]:+.3f})ï¼Œä½†å†…éƒ¨å› å­æ•°æ®ç¼ºå¤±ã€‚\n"

        # æ’åºå› å­ï¼Œæ‰¾åˆ°æœ€æ­£å’Œæœ€è´Ÿçš„
        sorted_factors = sorted(factor_values.items(), key=lambda item: item[1])
        top_positive = sorted_factors[-top_n:]
        top_negative = sorted_factors[:top_n]

        report_lines.append(
            f"   â€¢ ä¸»å¯¼ç­–ç•¥ç°‡: **{dominant_cluster.upper()}** (æ€»è´¡çŒ®: {cluster_contributions[dominant_cluster]:+.3f})")

        report_lines.append("     - æœ€å¼ºçœ‹å¤šå› å­:")
        for name, value in reversed(top_positive):
            if value > 0:
                report_lines.append(f"       â€¢ {name:<18s}: {value:+.3f}")

        report_lines.append("     - æœ€å¼ºçœ‹ç©ºå› å­:")
        for name, value in top_negative:
            if value < 0:
                report_lines.append(f"       â€¢ {name:<18s}: {value:+.3f}")

        return "\n".join(report_lines) + "\n"
    except Exception as e:
        # ç¡®ä¿æ­¤è¾…åŠ©å‡½æ•°ç»ä¸å¼•å‘ä¸»ç¨‹åºå´©æºƒ
        return f"   â€¢ [é”™è¯¯] è´¡çŒ®åº¦åˆ†æå¤±è´¥: {e}\n"


def generate_signal_and_report(data: pd.DataFrame, factor_results: Dict[str, Any], current_price: float, source: str) -> \
        Tuple[Dict[str, Any], str]:
    """
    (V7.4 ) å†³ç­–æŠ¥å‘Š
    - æ³¨å…¥äº†åŠ¨æ€é—¨æ§›ä¸æ»å›æœºåˆ¶ï¼Œæå‡å®ç›˜ç¨³å¥æ€§ã€‚
    - [FIX] ä¿®æ­£äº† 'fusion_weights' çš„å¼•ç”¨é”™è¯¯ã€‚
    """
    try:
        # --- æ­¥éª¤ 1: å®‰å…¨åœ°æå–æ‰€æœ‰æ ¸å¿ƒæ•°æ® ---
        final_signal_series = factor_results.get('final_signal', pd.Series(dtype=float))
        final_strength = final_signal_series.iloc[-1] if not final_signal_series.empty else 0.0

        composite_signal_series = factor_results.get('composite_signal', pd.Series(dtype=float))
        composite_strength = composite_signal_series.iloc[-1] if not composite_signal_series.empty else 0.0

        ml_signal_series = factor_results.get('ml_signal', pd.Series(dtype=float))
        ml_strength = ml_signal_series.iloc[-1] if not ml_signal_series.empty else 0.0

        # --- ã€V7.4 ä¿®æ­£ã€‘å°†å•è¡Œèµ‹å€¼æ‹†åˆ†ä¸ºå¤šè¡Œï¼Œè§£å†³å¼•ç”¨é”™è¯¯ ---
        fusion_weights = factor_results.get('fusion_weights', {})
        w_ml = fusion_weights.get('ml', 0.0)
        w_ic = fusion_weights.get('ic', 0.0)
        # --- ä¿®æ­£ç»“æŸ ---

        trend_regime = factor_results.get('current_trend_regime', 'æ•°æ®ç¼ºå¤±')
        risk_score = factor_results.get('current_risk_score', 0.0)
        cluster_signals = factor_results.get('cluster_signals', {})
        meta_weights = factor_results.get('meta_weights', {})
        cluster_confidence = factor_results.get('cluster_model_confidence', 0.0)
        ml_confidence = factor_results.get('ml_confidence', 0.0)
        ic_source = factor_results.get('ic_source', 'UNKNOWN')

        # --- æ­¥éª¤ 2: ã€æ ¸å¿ƒå‡çº§ã€‘è®¡ç®—åŠ¨æ€å†³ç­–é—¨æ§›ä¸æ»å› ---
        sig_vol = float(final_signal_series.tail(20).std() or 0.0)
        hist_vol = float((final_signal_series.rolling(60).std().median() or 1e-8))

        composite_conf = float(np.clip((cluster_confidence * w_ic + ml_confidence * w_ml), 0.05, 0.95))

        # è°ƒç”¨æ–°çš„ã€ç»è¿‡ä¼˜åŒ–çš„é—¨æ§›è®¡ç®—å‡½æ•°
        disagreement_series = factor_results.get('disagreement', pd.Series(dtype=float))
        current_disagreement = disagreement_series.iloc[-1] if not disagreement_series.empty else 0.0

        BUY_THRESHOLD, EXIT_THRESHOLD = _calculate_dynamic_thresholds(
            base_buy=PARAMS['BASE_BUY_THRESHOLD'],
            base_exit=PARAMS['EXIT_THRESHOLD'],
            risk_score=float(risk_score),
            composite_confidence=composite_conf,
            signal_volatility=sig_vol,
            historical_noise=hist_vol,
            trend_adjust_factor=PARAMS.get('TREND_ADJUST_FACTOR', 0.15),
            disagreement=float(current_disagreement)  # ã€ã€ã€æ–°å¢ä¼ é€’å‚æ•°ã€‘ã€‘ã€‘
        )

        if final_strength > BUY_THRESHOLD:
            recommendation = "ä¹°å…¥æˆ–å¢æŒ"
        elif final_strength < EXIT_THRESHOLD:
            recommendation = "å¹³ä»“ç¦»åœº (è¶‹åŠ¿åè½¬è­¦å‘Š)"
        else:
            recommendation = "æŒä»“è§‚å¯Ÿ"

        # --- æ­¥éª¤ 3: ç»„è£…ç»“æœå¯¹è±¡ ---
        result = {
            'signal': recommendation,
            'final_strength': float(final_strength),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'buy_threshold': float(BUY_THRESHOLD),
            'exit_threshold': float(EXIT_THRESHOLD)
        }

        # --- æ­¥éª¤ 4: ç”Ÿæˆå¥å£®çš„æŠ¥å‘Šå­—ç¬¦ä¸² ---
        risk_interp = "æåº¦é£é™©åå¥½ (Risk-On)" if risk_score > 0.7 else \
            "æ¸©å’Œé£é™©åå¥½" if risk_score > 0.2 else \
                "ä¸­æ€§/è§‚æœ›" if risk_score > -0.2 else \
                    "æ¸©å’Œé£é™©è§„é¿" if risk_score > -0.7 else "æåº¦é£é™©è§„é¿ (Risk-Off)"
        if trend_regime == 'æ•°æ®ç¼ºå¤±':
            risk_interp = "æ— æ³•åˆ¤æ–­ (ä¸Šæ¸¸æ•°æ®ç¼ºå¤±)"

        report_str = f"""
ã€çº³æ–¯è¾¾å…‹å†³ç­–æŠ¥å‘Š (V7.4 )ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“… æ—¶é—´: {result['timestamp']} (æ—¶åŒº: Asia/Shanghai)
ğŸ“ˆ å½“å‰ç‚¹ä½: {current_price:.2f} | ğŸ“Š æ•°æ®æ¥æº: {source}

ğŸŒ ç¬¬ä¸€å±‚: å¸‚åœºæƒ…æ™¯æ·±åº¦è¯Šæ–­
   â€¢ é•¿æœŸè¶‹åŠ¿çŠ¶æ€: {trend_regime}
   â€¢ ç»¼åˆé£é™©æŒ‡æ•°: {risk_score:+.3f} [-1, +1]
   â€¢ æƒ…æ™¯è§£è¯»: å½“å‰å¸‚åœºå¤„äº {risk_interp} çŠ¶æ€ã€‚

ğŸ§  ç¬¬äºŒå±‚: åŠ¨æ€ç­–ç•¥é…ç½®
   (ICæ•°æ®æº: {ic_source} å‘¨æœŸ)
"""
        if not meta_weights:
            report_str += "   â€¢ ç­–ç•¥æƒé‡æ•°æ®ç¼ºå¤±ã€‚\n"
        else:
            sorted_weights = sorted(meta_weights.items(), key=lambda item: item[1], reverse=True)
            for name, weight in sorted_weights:
                signal_series = cluster_signals.get(name, pd.Series(dtype=float))
                value = signal_series.iloc[-1] if not signal_series.empty else 0.0
                report_str += f"   â€¢ {name.upper():<12}æƒé‡: {weight:.2%} (ä¿¡å·: {value: .3f})\n"
        report_str += f"   â””â”€â”€â”€> ç­–ç•¥ç°‡æ¨¡å‹æ€»ä¿¡å·: {composite_strength:.3f}\n"

        report_str += f"""
ğŸ¯   ç¬¬ä¸‰å±‚: è‡ªé€‚åº”æ¨¡å‹èåˆ
   â€¢ ç­–ç•¥ç°‡æ¨¡å‹ç½®ä¿¡åº¦: {cluster_confidence:.2%} | æœºå™¨å­¦ä¹ æ¨¡å‹ç½®ä¿¡åº¦: {ml_confidence:.2%}
   â€¢ æœ€ç»ˆèåˆæƒé‡: [ç­–ç•¥ç°‡: {w_ic:.2%}] vs [æœºå™¨å­¦ä¹ : {w_ml:.2%}]

âš¡   æœ€ç»ˆå†³ç­–
   â€¢ æœ€ç»ˆèåˆä¿¡å·: {result['final_strength']:.3f}
   â€¢ å†³ç­–é—¨æ§›: ä¹°å…¥ > {result['buy_threshold']:.2f} | ç¦»åœº < {result['exit_threshold']:.2f}
   â€¢   ==> æ¨èåŠ¨ä½œ: {recommendation} <==
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡   ã€å†³ç­–æ ¸å¿ƒæº¯æºã€‘
{_get_top_factor_contributors(factor_results)}
ã€å†³ç­–é€»è¾‘è¯´æ˜ã€‘
æœ¬æŠ¥å‘Šå±•ç¤ºäº†ä¸€ä¸ªä¸‰å±‚å†³ç­–è¿‡ç¨‹ï¼šé¦–å…ˆè¯Šæ–­å¸‚åœºæƒ…æ™¯ï¼Œç„¶ååŸºäºæƒ…æ™¯é…ç½®ç­–ç•¥æƒé‡(ICæƒ…æ™¯: {ic_source})ï¼Œæœ€åè‡ªé€‚åº”åœ°èåˆç­–ç•¥æ¨¡å‹ä¸æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¾—å‡ºæœ€ç»ˆçš„ã€é«˜åº¦æƒ…æ™¯åŒ–çš„äº¤æ˜“ä¿¡å·ã€‚
ã€é£é™©æç¤ºã€‘
æ¨¡å‹é¢„æµ‹åŸºäºå†å²æ•°æ®ï¼Œä¸ä¿è¯æœªæ¥è¡¨ç°ã€‚è¯·å®¡æ…å†³ç­–ã€‚
"""
        return result, report_str
    except Exception as e:
        logger.error(f"ä¿¡å·ä¸æŠ¥å‘Šç”Ÿæˆæ—¶å‘ç”Ÿä¸¥é‡æ„å¤–é”™è¯¯: {e}\n{traceback.format_exc()}")
        err_report = f"""
ã€ä¸¥é‡é”™è¯¯ï¼šå†³ç­–æŠ¥å‘Šç”Ÿæˆå¤±è´¥ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ç³»ç»Ÿåœ¨å°è¯•ç”Ÿæˆå†³ç­–æŠ¥å‘Šæ—¶é­é‡äº†æ— æ³•å¤„ç†çš„å†…éƒ¨é”™è¯¯ã€‚

é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}

è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ `cache/livermore_enhanced.log` è·å–å®Œæ•´çš„é”™è¯¯è¿½æº¯ä¿¡æ¯ã€‚
"""
        return {'signal': 'ERROR', 'error': str(e)}, err_report

# ===================================
# ä¸»æ‰§è¡Œå‡½æ•° (V7.4 )
# ===================================
@performance_monitor
def main(initial_capital: float = 100000.0):  # ã€V7.4 ä¿®æ”¹ã€‘
    logger.info(f"ğŸš€ çº³æ–¯è¾¾å…‹å¢å¼ºç‰ˆé‡åŒ–é¢„æµ‹ç³»ç»Ÿå¯åŠ¨ (V7.4 )... åˆå§‹æ¨¡æ‹Ÿèµ„æœ¬: ${initial_capital:,.2f}")
    try:
        df, current_price, source = fetch_nasdaq_data()
        if df.empty:
            logger.error("âŒ æ•°æ®è·å–å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
            return
        logger.info(f"âœ… æ•°æ®è·å–æˆåŠŸ: {len(df)}æ¡, å½“å‰ä»·æ ¼: {current_price:.2f} (æ¥æº: {source})")

        latest_data_date = df.index[-1].date()
        today_date = datetime.now().date()
        date_diff = (today_date - latest_data_date).days

        # å…è®¸çš„æœ€å¤§æ•°æ®å»¶è¿Ÿå¤©æ•°ï¼ˆè€ƒè™‘å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰
        MAX_DATA_DELAY_DAYS = 3

        if date_diff > MAX_DATA_DELAY_DAYS:
            error_msg = f"ğŸ”´ CRITICAL: æ•°æ®ä¸¥é‡é™ˆæ—§ï¼æœ€æ–°æ•°æ®æ—¥æœŸä¸º {latest_data_date}ï¼Œå·²å»¶è¿Ÿ {date_diff} å¤©ã€‚ä¸ºä¿è¯å®‰å…¨ï¼Œç³»ç»Ÿç»ˆæ­¢è¿è¡Œã€‚"
            logger.critical(error_msg)
            send_notification(subject="ã€ä¸¥é‡è­¦å‘Šã€‘é‡åŒ–ç³»ç»Ÿå› æ•°æ®é™ˆæ—§å·²è‡ªåŠ¨ç»ˆæ­¢", content=error_msg)
            return  # å…³é”®ï¼šç»ˆæ­¢ç¨‹åºæ‰§è¡Œ

        try:
            if 'Date' in df.columns: df.set_index('Date', inplace=True)
            if df.index.has_duplicates: df = df[~df.index.duplicated(keep='last')]
            df.sort_index(inplace=True)
            if df.index.tz is not None:
                logger.info(f"æ£€æµ‹åˆ°æ—¶åŒºä¿¡æ¯ ({df.index.tz})ï¼Œæ­£åœ¨è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†...")
                df.index = df.index.tz_localize(None)
                logger.info("âœ… æ—¶åŒºä¿¡æ¯å·²å‰¥ç¦»ã€‚")
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å‡€åŒ–å¤±è´¥: {e}ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        engine = EnhancedFactorEngine(df)
        factor_results = engine.run_full_analysis()
        signal_result, report_str = generate_signal_and_report(df, factor_results, current_price, source)

        logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œæ ¸å¿ƒå†³ç­–ä¸é£é™©ç®¡ç†æµç¨‹...")
        state = load_position_state()
        market_regime = factor_results.get('current_trend_regime', 'BEAR')
        is_buy_signal = "ä¹°å…¥" in str(signal_result.get('signal', ''))
        is_model_exit_signal = "å¹³ä»“ç¦»åœº" in str(signal_result.get('signal', ''))

        if state.get('status') == 'long':
            if 'entry_date' not in state or not state.get('entry_date'):
                logger.warning("çŠ¶æ€æ–‡ä»¶ç¼ºå°‘'entry_date'ï¼Œå¯åŠ¨è‡ªåŠ¨ä¿®å¤ç¨‹åº...")
                entry_price = state.get('entry_price')
                if entry_price:
                    possible_dates = df[np.isclose(df['Close'], entry_price, rtol=0.005)].index
                    if not possible_dates.empty:
                        state['entry_date'] = possible_dates[0].strftime('%Y-%m-%d')
                        save_position_state(state)
                        logger.info(f"âœ… è‡ªåŠ¨ä¿®å¤æˆåŠŸï¼æ¨æ–­å…¥åœºæ—¥æœŸä¸º: {state['entry_date']}ã€‚")
                    else:
                        logger.error(f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼šæ— æ³•åœ¨å†å²æ•°æ®ä¸­æ‰¾åˆ°ä¸å…¥åœºä»· ${entry_price:.2f} åŒ¹é…çš„æ—¥æœŸã€‚")

            entry_date_str = state.get('entry_date')
            livermore_triggered = False
            livermore_stop_price = 0.0

            if entry_date_str:
                position_history_df = df[df.index >= pd.to_datetime(entry_date_str)]
                if not position_history_df.empty:
                    true_peak_price = position_history_df['High'].max()
                    prev_peak = float(state.get('peak_price_since_entry') or 0.0)
                    peak = max(prev_peak, true_peak_price)
                    livermore_stop_price = peak * (1 - PARAMS['LIVERMORE_DRAWDOWN_PERCENT'])
                    entry_price = float(state.get('entry_price', current_price))
                    pnl_percent = (current_price / entry_price - 1) * 100
                    drawdown_from_peak = (1 - current_price / peak) * 100 if peak > 0 else 0

                    # æ ¹æ®ç›ˆäºçŠ¶æ€é€‰æ‹©è¡¨æƒ…ç¬¦å·ï¼Œå¢åŠ å¯è¯»æ€§
                    pnl_emoji = "ğŸŸ¢" if pnl_percent >= 0 else "ğŸ”´"

                    logger.info(
                        f"æŒä»“æ£€æŸ¥: å…¥åœºæ—¥ {entry_date_str} | "
                        f"å½“å‰ä»·æ ¼: {current_price:.2f} | "
                        f"{pnl_emoji} ç›ˆäº: {pnl_percent:+.2f}% | "
                        f"ğŸ“‰ å³°å€¼å›æ’¤: {drawdown_from_peak:.2f}% (å†å²å³°å€¼: {peak:.2f}) | "
                        f"ğŸ›¡ï¸ æ­¢æŸä½: {livermore_stop_price:.2f}"
                    )
                    if current_price < livermore_stop_price: livermore_triggered = True
                    if peak > prev_peak:
                        state['peak_price_since_entry'] = peak
                        save_position_state(state)

            model_driven_close = is_model_exit_signal or (market_regime == 'BEAR' and not livermore_triggered)
            must_close = livermore_triggered or model_driven_close

            if must_close:
                reason = (
                    f"åˆ©å¼—è«å°”æ­¢æŸ" if livermore_triggered else f"æ¨¡å‹ä¿¡å·å¹³ä»“ ({'è¶‹åŠ¿åè½¬' if is_model_exit_signal else 'è¿›å…¥ç†Šå¸‚'})")
                logger.info(f"â¡ï¸ è§¦å‘å¹³ä»“ï¼ŒåŸå› : {reason}")
                position_details_str = f"å·²æ‰§è¡Œå¹³ä»“ï¼ŒåŸå› : {reason}"
                stop_loss_details_str = "      (å·²å¹³ä»“)"
                state = {'status': 'flat', 'entry_price': None, 'entry_date': None, 'peak_price_since_entry': None}
                save_position_state(state)
            else:
                entry_price = float(state.get('entry_price', 0))
                position_details_str = f"æŒä»“è§‚å¯Ÿ (å…¥åœºä»·: ${entry_price:.2f})"
                risk_manager = RiskManager(df)
                initial_stop_loss = risk_manager.calculate_stop_loss(entry_price=entry_price)

                atr = risk_manager.calculate_atr()
                safety_score = (current_price - livermore_stop_price) / (atr + 1e-8)
                safety_tag = "âœ… å®‰å…¨" if safety_score >= 1.5 else "âš ï¸ ä¸­æ€§" if safety_score >= 0.7 else "ğŸš¨ è­¦æˆ’"
                stop_loss_details_str = (f"   â€¢ åˆå§‹ATRæ­¢æŸ (ä¿åº•): ${initial_stop_loss:.2f}\n"
                                         f"   â€¢ åˆ©å¼—è«å°”åŠ¨æ€æ­¢æŸ: ${livermore_stop_price:.2f} (åŸºäºå³°å€¼ {state.get('peak_price_since_entry', 0):.2f})   {safety_tag}")
        else:  # ç©ºä»“ä¸­
            can_open = (market_regime == 'BULL') and is_buy_signal
            if can_open:
                entry_price = float(current_price)
                entry_date_str = df.index[-1].strftime('%Y-%m-%d')
                logger.info(f"âœ… è§¦å‘å¼€ä»“æ¡ä»¶ï¼Œå…¥åœºä»·: {entry_price:.2f}, å…¥åœºæ—¥æœŸ: {entry_date_str}")
                state = {'status': 'long', 'entry_price': entry_price, 'entry_date': entry_date_str,
                         'peak_price_since_entry': entry_price}
                save_position_state(state)
                risk_manager = RiskManager(df)
                position_quantity = risk_manager.calculate_position_size(capital=initial_capital)
                disagreement = factor_results.get('disagreement', pd.Series([0.0])).iloc[-1]
                shrink_factor = max(PARAMS['POSITION_DISAGREEMENT_MIN_SCALE'],
                                    (1 - float(disagreement)) ** PARAMS['POSITION_DISAGREEMENT_POWER'])
                position_quantity *= shrink_factor
                position_value = position_quantity * current_price
                position_details_str = f"å·²å¼€ä»“ ${position_value:,.2f} ({position_quantity:.4f} å•ä½) (å·²æ ¹æ®åˆ†æ­§åº¦ {disagreement:.1%} è°ƒæ•´)"
                initial_stop_loss = risk_manager.calculate_stop_loss(entry_price=entry_price)
                livermore_sl_on_open = entry_price * (1 - PARAMS['LIVERMORE_DRAWDOWN_PERCENT'])
                stop_loss_details_str = f"   â€¢ **åˆå§‹ATRæ­¢æŸ (ä¿åº•)**: ${initial_stop_loss:.2f}\n"
                stop_loss_details_str += f"   â€¢ **åˆ©å¼—è«å°”åŠ¨æ€æ­¢æŸ**: ${livermore_sl_on_open:.2f} (åŸºäºå½“å‰å³°å€¼)"
            else:
                position_details_str = "ç©ºä»“æˆ–æ— æ“ä½œ"
                stop_loss_details_str = "      (å½“å‰æ— æŒä»“)"

        full_report = report_str + f"""
ğŸ’°   å¤´å¯¸ä¸é£é™©ç®¡ç†
   â€¢   å½“å‰çŠ¶æ€: {position_details_str}
   â€¢   æ­¢ç›ˆç­–ç•¥: æ— å›ºå®šæ­¢ç›ˆä½ï¼Œè®©åˆ©æ¶¦å¥”è·‘
   â€¢   æ­¢æŸç­–ç•¥:
{stop_loss_details_str}
"""
        if market_regime == 'BEAR' and state.get('status') != 'long':
            full_report += "\nâš ï¸ ç†Šå¸‚è­¦å‘Š: ç³»ç»Ÿæš‚åœä¸€åˆ‡æ–°å¼€ä»“æ“ä½œä»¥è§„é¿é£é™©ã€‚\n"

        logger.info(full_report)
        send_notification(subject=f"çº³æŒ‡å†³ç­–æŠ¥å‘Š: {signal_result['signal']} {datetime.now().strftime('%m-%d')}",
                          content=full_report)
        with open('cache/nasdaq_signal.json', 'w', encoding='utf-8') as f:
            json.dump(signal_result, f, ensure_ascii=False, indent=2)
        logger.info("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")

    except Exception as e:
        logger.error(f"ğŸ’¥ ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        error_trace = traceback.format_exc()
        logger.error(error_trace)
        send_notification(subject="çº³æ–¯è¾¾å…‹äº¤æ˜“ç³»ç»Ÿé”™è¯¯æŠ¥å‘Š", content=f"ç³»ç»Ÿè¿è¡Œé”™è¯¯:\n\n{str(e)}\n\n{error_trace}")


# ===================================
# å®šæ—¶ä»»åŠ¡å…¥å£
# ===================================
def schedule_daily_analysis():
    """æ¯æ—¥è‡ªåŠ¨åˆ†æä»»åŠ¡å…¥å£ï¼Œå¯å¯¹æ¥å®šæ—¶å™¨/cronç­‰"""
    main()


if __name__ == "__main__":
    # æ‚¨å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹æ¨¡æ‹Ÿå¯åŠ¨èµ„é‡‘
    main(initial_capital=100000.0)

# ===================================
# ç»“å°¾
# ===================================
```

## å‰¯ç¨‹åº
> æ–‡ä»¶åï¼šmacro_engine_strict.py

```python
# -*- coding: utf-8 -*-
"""
å®è§‚æ•°æ®å¼•æ“æ¨¡å— (macro_engine_strict.py)
ç‰ˆæœ¬: V7.4 (æœ€ç»ˆç‰ˆ - é›†æˆæ‰€æœ‰å·²éªŒè¯å› å­)

æ ¸å¿ƒåŠŸèƒ½:
- è´Ÿè´£ä»akshareè·å–æ‰€æœ‰å®è§‚ç»æµæ•°æ®ã€‚
- å†…ç½®æ‰‹åŠ¨è®¡ç®—é€»è¾‘ï¼Œä»¥å¤„ç†æ— ç›´æ¥æ¥å£çš„æ•°æ®ï¼ˆå¦‚PPIå¹´ç‡ï¼‰ã€‚
- å†…ç½®ç£ç›˜ç¼“å­˜æœºåˆ¶ï¼Œå¤§å¹…æå‡äºŒæ¬¡è¿è¡Œæ—¶çš„åŠ è½½é€Ÿåº¦ã€‚
"""

import pandas as pd
import numpy as np
import akshare as ak
from typing import Dict
import logging
import traceback
import os
from datetime import datetime, timedelta

# =================================================================
# æ—¥å¿—ç³»ç»Ÿ
# =================================================================
logger = logging.getLogger('macro_engine')


# =================================================================
# è¾…åŠ©æ•°æ®è·å–ä¸è®¡ç®—å‡½æ•° (æ ¸å¿ƒé€»è¾‘)
# =================================================================

def get_usa_ppi_yoy_from_monthly():
    """
    é€šè¿‡æ–‡æ¡£ä¸­å­˜åœ¨çš„ ak.macro_usa_ppi (æœˆç‡) æ¥å£ï¼Œæ‰‹åŠ¨è®¡ç®—å¹´ç‡(YoY)ã€‚
    """
    try:
        df_monthly = ak.macro_usa_ppi()
        if df_monthly.empty: return pd.DataFrame()

        df_monthly['æ—¥æœŸ'] = pd.to_datetime(df_monthly['æ—¥æœŸ'])
        df_monthly = df_monthly.set_index('æ—¥æœŸ').sort_index()

        monthly_returns = df_monthly['ä»Šå€¼'] / 100.0
        yoy_returns = (1 + monthly_returns).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1

        final_df = (yoy_returns * 100).to_frame('ä»Šå€¼').reset_index()
        final_df.columns = ['æ—¥æœŸ', 'ä»Šå€¼']
        final_df['ä»Šå€¼'] = pd.to_numeric(final_df['ä»Šå€¼'], errors='coerce')
        final_df.dropna(subset=['ä»Šå€¼'], inplace=True)
        return final_df
    except Exception:
        return pd.DataFrame()


# =================================================================
# å®è§‚äº‹ä»¶å®šä¹‰ (æœ€ç»ˆç‰ˆ)
# =================================================================
EVENTS = {
    # --- åŸæœ‰å› å­ ---
    'pce': {'func': ak.macro_usa_core_pce_price},
    'nfp': {'func': ak.macro_usa_non_farm},
    'rtl': {'func': ak.macro_usa_retail_sales},
    'ism_non_mfg': {'func': ak.macro_usa_ism_non_pmi},  # å·²æ”¹åä»¥åŒºåˆ†
    'job': {'func': ak.macro_usa_initial_jobless},
    'epu': {'func': lambda: ak.article_epu_index(symbol="USA")},

    # --- æ‰€æœ‰å·²é€šè¿‡æµ‹è¯•çš„æ–°å› å­ ---
    'cpi_yoy': {'func': ak.macro_usa_cpi_yoy},
    'ppi_yoy': {'func': get_usa_ppi_yoy_from_monthly},
    'ism_mfg': {'func': ak.macro_usa_ism_pmi},
    'durable': {'func': ak.macro_usa_durable_goods_orders},
    'confidence': {'func': ak.macro_usa_cb_consumer_confidence},
    'unemployment': {'func': ak.macro_usa_unemployment_rate},
}


def fetch_event(name: str) -> pd.Series:
    """
    (V7.4) å®è§‚æ•°æ®è·å–å¼•æ“ã€‚
    """
    cfg = EVENTS[name]
    CACHE_DIR = "cache"
    CACHE_EXPIRATION = timedelta(hours=6)
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"macro_{name}.pkl")

    if os.path.exists(cache_file):
        file_mod_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
        if datetime.now() - file_mod_time < CACHE_EXPIRATION:
            try:
                logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½å®è§‚æ•°æ®: {name}")
                final_series = pd.read_pickle(cache_file)
                if not final_series.empty:
                    return final_series
            except Exception as e:
                logger.warning(f"åŠ è½½å®è§‚æ•°æ®ç¼“å­˜ {name} å¤±è´¥: {e}ï¼Œå°†é‡æ–°è·å–ã€‚")

    try:
        logger.info(f"ğŸ”„ ä»ç½‘ç»œè·å–å®è§‚æ•°æ®: {name}")
        raw = cfg['func']()

        if raw is None or not isinstance(raw, pd.DataFrame) or raw.empty:
            logger.warning(f"å®è§‚æ•°æ® {name} ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚")
            return pd.Series(dtype=float)

        raw.columns = [str(c).lower() for c in raw.columns]

        val_col_name = None
        possible_value_cols = ['ä»Šå€¼', 'ç°å€¼', 'æœ€æ–°å€¼', 'å€¼', 'index']
        for col in raw.columns:
            if any(k in col for k in possible_value_cols):
                val_col_name = col
                break
        if not val_col_name:
            logger.warning(f"åœ¨ {name} æ•°æ®ä¸­æ— æ³•è¯†åˆ«æ•°å€¼åˆ—: {raw.columns}")
            return pd.Series(dtype=float)

        date_series = None
        if 'year' in raw.columns and 'month' in raw.columns:
            try:
                date_series = pd.to_datetime(raw['year'].astype(str) + '-' + raw['month'].astype(str), errors='coerce')
            except Exception:
                pass

        if date_series is None or date_series.isna().all():
            date_col_name = None
            possible_date_cols = ['æ—¥æœŸ', 'date', 'æ—¶é—´', 'ç»Ÿè®¡æ—¶é—´']
            for col in raw.columns:
                if any(k in col for k in possible_date_cols):
                    date_col_name = col
                    break
            if date_col_name:
                date_series = pd.to_datetime(raw[date_col_name], errors='coerce')

        if date_series is None or date_series.isna().all():
            logger.error(f"âŒ åœ¨ {name} æ•°æ®ä¸­æ— æ³•æ„å»ºæœ‰æ•ˆçš„æ—¥æœŸç´¢å¼•ã€‚åˆ—: {raw.columns}")
            return pd.Series(dtype=float)

        final_series = pd.Series(data=pd.to_numeric(raw[val_col_name], errors='coerce').values, index=date_series)
        final_series.dropna(inplace=True)

        if final_series.empty:
            logger.warning(f"å› å­ {name} å¤„ç†åæ•°æ®ä¸ºç©ºã€‚")
            return pd.Series(dtype=float)

        final_series_sorted = final_series[~final_series.index.duplicated(keep='last')].sort_index()
        logger.info(f"âœ… è·å–å®è§‚æ•°æ® {name} æˆåŠŸ, æœ€æ–°å€¼: {final_series_sorted.iloc[-1]}")

        try:
            pd.to_pickle(final_series_sorted, cache_file)
            logger.info(f"ğŸ’¾ å®è§‚æ•°æ® {name} å·²ç¼“å­˜è‡³æœ¬åœ°: {cache_file}")
        except Exception as e:
            logger.error(f"ç¼“å­˜å®è§‚æ•°æ® {name} å¤±è´¥: {e}")

        return final_series_sorted

    except Exception as e:
        logger.error(f"âŒ è·å–å®è§‚æ•°æ® {name} å¤±è´¥: {e}\n{traceback.format_exc()}")
        return pd.Series(dtype=float)


class StrictMacro:
    def __init__(self, idx: pd.DatetimeIndex):
        self.idx = idx if isinstance(idx, pd.DatetimeIndex) else pd.to_datetime(idx)
        self.idx = self.idx.normalize().drop_duplicates().sort_values()
        self.logger = logger
        self.logger.info("ä¸¥æ ¼å®è§‚å› å­æ„å»ºå™¨ (StrictMacro) åˆå§‹åŒ–å®Œæˆã€‚")

    def factor(self, name: str) -> pd.Series:
        raw = fetch_event(name)
        if raw.empty:
            return pd.Series(0.0, index=self.idx)
        try:
            factor_series = raw.reindex(self.idx, method='ffill')
            factor_series.fillna(method='bfill', inplace=True)
            factor_series.fillna(0.0, inplace=True)
            self.logger.info(f"å®è§‚å› å­ {name} æ„å»ºå®Œæˆ, è¿”å›å…¶æ°´å¹³å€¼ã€‚æœ€æ–°å€¼: {factor_series.iloc[-1]:.4f}")
            return factor_series
        except Exception as e:
            self.logger.error(f"æ„å»ºå®è§‚å› å­ {name} å¤±è´¥: {e}\n{traceback.format_exc()}")
            return pd.Series(0.0, index=self.idx)

    def factors(self) -> Dict[str, pd.Series]:
        self.logger.info("å¼€å§‹æ„å»ºæ‰€æœ‰å®è§‚å› å­...")
        all_factors = {n: self.factor(n) for n in EVENTS}
        self.logger.info("âœ… æ‰€æœ‰å®è§‚å› å­æ„å»ºå®Œæˆã€‚")
        return all_factors
```
